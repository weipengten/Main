{
  "hash": "107c74a95409e9ecfbc10d54bd5b3e35",
  "result": {
    "markdown": "---\ntitle: \"Analyzing Bus Traffic Flows in Singapore\"\nauthor: \"Ten Wei Peng\"\ndate: \"July 10, 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n# **Setting the Scene**\n\nWhat are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\n\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\n\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\n\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner's ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data.\n\n# **Motivation and Objective**\n\nThis take-home exercise is motivated by two main reasons. Firstly, despite increasing amounts of open data available for public consumption, there has not been significant practice research carried out to show how these disparate data sources can be integrated, analysed, and modelled to support policy making decisions.\n\nSecondly, there is a general lack of practical research to show how geospatial data science and analysis (GDSA) can be used to support decision-making.\n\nHence, your task for this take-home exercise is to conduct a case study to demonstrate the potential value of GDSA to integrate publicly available data from multiple sources for building a spatial interaction models to determine factors affecting urban mobility patterns of public bus transit.\n\n# **The Data**\n\n## **Open Government Data**\n\nFor the purpose of this assignment, data from several open government sources will be used:\n\n-   *Passenger Volume by Origin Destination Bus Stops*, *Bus Stop Location*, *Train Station* and *Train Station Exit Point*, just to name a few of them, from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).\n\n-   *Master Plan 2019 Subzone Boundary*, *HDB Property Information*, *School Directory and Information* and other relevant data from [Data.gov.sg](https://beta.data.gov.sg/).\n\n## **Specially collected data**\n\n-   *Business*, *entertn*, *F&B*, *FinServ*, *Leisure&Recreation* and *Retails* are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\n\n-   HDB: This data set is the geocoded version of *HDB Property Information* data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest *HDB Property Information* provided on data.gov.sg, this [link](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/?panelset6=glimpse%28%29#geocoding-our-aspatial-data) provides a useful step-by-step guide.\n\n# **The Task**\n\nThe specific tasks of this take-home exercise are as follows:\n\n## **Geospatial Data Science**\n\n-   Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).\n\n-   With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level\n\n    | Peak hour period             | Bus tap on time |\n    |------------------------------|-----------------|\n    | Weekday morning peak         | 6am to 9am      |\n    | Weekday afternoon peak       | 5pm to 8pm      |\n    | Weekend/holiday morning peak | 11am to 2pm     |\n    | Weekend/holiday evening peak | 4pm to 7pm      |\n\n-   Display the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\n\n-   Describe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\n\n-   Assemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\n\n-   Compute a distance matrix by using the analytical hexagon data derived earlier.\n\n## **Spatial Interaction Modelling**\n\n-   Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\n\n-   Present the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\n\n-   With reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual).\n\n# Install R Package\n\nThese are packages here necessary for this project:\n\n-   sf: Handles geospatial data using the Simple Features format.\n\n-   sp: Core package for spatial data structures and operations.\n\n-   sfdep: Provides spatial econometrics tools, including spatial weights matrices.\n\n-   tidyverse: Collection of packages for tidy data manipulation and visualization.\n\n-   tmap: Creates thematic maps for effective visualization.\n\n-   viridis: Offers perceptually uniform color palettes.\n\n-   reshape2: Reshapes and aggregates data for better compatibility with plotting functions.\n\n-   performance: Evaluates the performance of algorithms or functions.\n\n-   stplanr: Provides spatial transport planning functions.\n\n-   httr: Makes HTTP requests, useful for API interactions.\n\n-   lwgeom: Extends sf functionality with additional geometric operations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, sp, sfdep, tidyverse, tmap, viridis, reshape2, performance,  stplanr, httr, lwgeom, DT, units)\n```\n:::\n\n\n# 1. Importing Data\n\nWe will import the data as a first step before proceeding with data cleaning, data wrangling and data exploration for the following:\n\n-   **Passenger Volume**, a csv file, data set downloaded from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)\n\n-   **MPSZ-2019**, a MULTIPOLYGON feature layer ESRI shapefile format providing the sub-zone boundary of URA Master Plan 2019\n\n-   **BusStop**, a POINT feature layer ESRI shapefile format\n\n-   **Train Station**, a POLYGON feature layer ESRI shapefile format\n\n-   **Train Station Exit Point**, a POLYGON feature layer ESRI shapefile format\n\n-   **Train Lines**, LINESTRING feature layer in KML format\n\n-   **Central Business District**, google-sourced location\n\n-   **Intergrated Transoort Hub**, a POINT feature layer in KML format\n\n-   **Parks**, a POINT feature layer in KML format\n\n-   **HDB**, a csv file, data set downloaded and prepared by Prof Kam and his students\n\n-   **School**, a csv file downloaded from LTA MALL\n\n-   **Specially Collected Data**, other vital data collected with POLYGON feature layer in ESRI shapefile format\n\n::: panel-tabset\n## Passenger Volume\n\n**Passenger Volume** is an aspatial data, we can import the data simply by using the read_csv function from tidyverse package and output it as a tibble dataframe called `odbus`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodbus <- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n```\n:::\n\n\n## MPSZ-2019\n\n**MPSZ-2019**: This data provides the sub-zone boundary of URA Master Plan 2019. Both data sets are in ESRI shapefile format. We save it as a sf data frame called `mpsz` using the **st_read** function of the sf package. The data is then geo-referenced to coordinates from the Singapore SVY21 coordinate system (EPSG: 3414).\n\n> Note that the geometry data provided in the datasets needs to be projected to local standards and should be same for all datasets. Therefore we will be have to make sure or transform all the datasets to projected CRS using Singapore SVY21 coordinate system as mentioned above.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz <- st_read(dsn =  \"data/geospatial\", layer = \"MPSZ-2019\") %>% \n  st_transform(crs = 3414)\n```\n:::\n\n\nNote st_read() function of sf package is used to import the shapefile into R as sf data frame. st_transform() function of sf package is used to transform the projection to crs 3414.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz <- st_make_valid(mpsz)\nlength(which(st_is_valid(mpsz) == FALSE))\n\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz <- read_rds( \"data/rds/mpsz.rds\")\n```\n:::\n\n\n## Bus Stop Location\n\n**Bus Stop** is a geospatial data in .shp file. We save it as a sf data frame called `busstop` using the **st_read** function of the sf package. The data is then geo-referenced to coordinates from the Singapore SVY21 coordinate system (EPSG: 3414)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbusstop <- st_read(dsn = \"data/geospatial\", \n                   layer = \"BusStop\") %>%\n  st_transform(crs=3414)\n```\n:::\n\n\n## CBD\n\nProximity to CBD is often a popular variable for a very good reason - nearer means more dense population. Even though the Singapore goverment is planning to mitigate this by constructing hubs in each town, it is unlikely that proximity to city does not affect travel decisions. Coordinates of CBD is dervied from [GeoHack](https://geohack.toolforge.org/geohack.php?pagename=Central_Area,_Singapore&params=1_17_30_N_103_51_00_E_type:city(60520)_region:SG)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlat <- 1.291667\nlng <- 103.85\n\ncbd_sf <- data.frame(lat, lng) %>%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs=4326) %>%\n  st_transform(crs=3414)\n```\n:::\n\n\n## Train Station\n\n**Train Station** is a geospatial data in .shp file. We save it as a sf data frame called `trainstation` using the **st_read** function of the sf package. The data is then geo-referenced to coordinates from the Singapore SVY21 coordinate system (EPSG: 3414)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrainstation <- st_read(dsn = \"data/geospatial\", \n                   layer = \"RapidTransitSystemStation\") %>%\n  st_transform(crs=3414)\n```\n:::\n\n\n## Train Station Exit Point\n\n**Train Station Exit Point** is a geospatial data in .shp file. We save it as a sf data frame called `trainstationEP` using the **st_read** function of the sf package. The data is then geo-referenced to coordinates from the Singapore SVY21 coordinate system (EPSG: 3414)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrainstationEP <- st_read(dsn = \"data/geospatial\", \n                   layer = \"RapidTransitSystemStation\") %>%\n  st_transform(crs=3414)\n\nwrite_rds(trainstationEP, \"data/rds/trainstationEP.rds\")\n```\n:::\n\n\n## Train Lines\n\nDue to the issue with geometry in the trainstation dataset, it is unlikely we will be able to make good use of it. One other possible substitute could be the train line dataset from data.gov.sg. With it, we can calculate proximity to train lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmrt = st_read(\"data/geospatial/MasterPlan2003MRTLine.kml\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmrt <- st_zm(mrt)\nmrt <- st_transform(mrt, crs = 3414)\n\nwrite_rds(mrt, \"data/rds/mrt.rds\")\n```\n:::\n\n\nBelow is a visualisation of how our data looks like, exactly as described for a LINESTRING format, and exactly as expected. We expect to be able to calculate proximity to train lines since the train station datasets seems to be having an issue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmrt <- read_rds( \"data/rds/mrt.rds\")\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.5) +\n  tm_borders(lwd = 1, alpha = 0.5) +\n  tm_layout(frame = FALSE)+\n  tmap_style(\"gray\")+\ntm_shape(mrt) +\n  tm_lines(col = \"purple\", size = 0.1) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Intergrated Transoort Hub (ITH)\n\nFor a city-state like Singapore, the importance of distance to city for us might not be as big as compared to other countries like China and Singapore. Furthermore, the government already planned for the scenario of traffic congestion by constructing Intergrated Transport Hubs all over Singapore. This is to promote traffic and activities within each SUBZONE instead of overcrowding the central district. Hence, it would be wise to include that in our analysis and be dearly wrong so if we do not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nith = st_read(\"data/geospatial/MasterPlan2019SDCPIntegratedTransportHublayerKML.kml\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nith <- st_zm(ith)\nith <- st_transform(ith, crs = 3414)\n\nwrite_rds(ith, \"data/rds/ith.rds\")\n```\n:::\n\n\nAs expected of ITHs, the locations are spread out across the island and do not cluster\n\n\n::: {.cell}\n\n```{.r .cell-code}\nith <- read_rds( \"data/rds/ith.rds\")\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.5) +\n  tm_borders(lwd = 1, alpha = 0.5) +\n  tm_layout(frame = FALSE)+\n  tmap_style(\"gray\")+\ntm_shape(ith) +\n  tm_dots(col = \"purple\", size = 0.1) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Parks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparks = st_read(\"data/geospatial/Parks.kml\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparks <- st_zm(parks)\nparks <- st_transform(parks, crs = 3414)\n\nwrite_rds(parks, \"data/rds/parks.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparks <- read_rds( \"data/rds/parks.rds\")\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.5) +\n  tm_borders(lwd = 1, alpha = 0.5) +\n  tm_layout(frame = FALSE)+\n  tmap_style(\"gray\")+\ntm_shape(parks) +\n  tm_dots(col = \"purple\", size = 0.1) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## HDB\n\n**HDB**: Using geocoded version of HDB Property Information data from data.gov\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhdb <- read_csv(\"data/aspatial/hdb.csv\")\nhdb_sf <- st_as_sf(hdb, coords = c(\"lng\", \"lat\"), crs = 4326) %>%\n  st_transform(crs = 3414)\n\nwrite_rds(hdb_sf, \"data/rds/hdb_sf.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhdb_sf <- read_rds( \"data/rds/hdb_sf.rds\")\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.5) +\n  tm_borders(lwd = 1, alpha = 0.5) +\n  tm_layout(frame = FALSE)+\n  tmap_style(\"gray\")+\ntm_shape(hdb_sf) +\n  tm_dots(col = \"purple\", size = 0.1) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n## School Directory\n\n**School Directory**: Using geocoded version of HDB Property Information data from data.gov\n\nThe provided code chunks perform geocoding using the SLA [OneMap API](https://www.onemap.gov.sg/apidocs/) in R. The process involves reading input data in CSV format into the R Studio environment using the **`read_csv`** function from the **`readr`** package. The geocoding is then executed using a series of HTTP calls facilitated by functions from the **`httr`** package, sending individual records to the OneMap geocoding server.\n\nThe results are organized into two tibble data.frames: **`found`** and **`not_found`**. The **`found`** data.frame contains records that were successfully geocoded, while **`not_found`** includes postal codes that failed the geocoding process.\n\nIn the final step, the **`found`** data table is joined with the initial CSV data table using a unique identifier (**`POSTAL`**) shared between the two data tables. The resulting data table is saved as a new CSV file named \"found.\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool <- read_csv(\"data/aspatial/Generalinformationofschools.csv\")\n```\n:::\n\n\n### Geocoding using SLA API\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl<-\"https://www.onemap.gov.sg/api/common/elastic/search\"\n\ncsv<-read_csv(\"data/aspatial/Generalinformationofschools.csv\")\npostcodes<-csv$`postal_code`\n\nfound<-data.frame()\nnot_found<-data.frame()\n\nfor(postcode in postcodes){\n  query<-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')\n  res<- GET(url,query=query)\n  \n  if((content(res)$found)!=0){\n    found<-rbind(found,data.frame(content(res))[4:13])\n  } else{\n    not_found = data.frame(postcode)\n  }\n}\n```\n:::\n\n\nNext, the code chunk below will be used to combine both *found* and *not_found* data.frames into a single tibble data.frame called *merged*. At the same time, we will write *merged* and *not_found* tibble data.frames into two separate csv files called *schools* and *not_found* respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmerged = merge(csv,found, by.x='postal_code', by.y='results.POSTAL',all=TRUE)\n               write.csv(merged, file=\"data/aspatial/schools.csv\")\n               write.csv(not_found, file=\"data/aspatial/not_found.csv\")\n```\n:::\n\n\n### Tidying schools data.frame\n\nIn this sub-section, we will import schools.csv into R environment and at the same time tidying the data by selecting only the necessary fields as well as rename some field:\n\n-   import schools.csv in R environment as an tibble data.frame called schools,\n\n-   rename results.LATITUDE and results.LONGITUDE to latitude and longitude respectively,\n\n-   retain only postal_code, school_name, latitude and longitude in schools tibble data.frame\n\n-   With the help of Google Map, we derived the location information of the ungeocoded school by using it's postcode for \"ZHENGHUA SECONDARY SCHOOL\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools <- read_csv(\"data/aspatial/schools.csv\") %>%\n  rename(latitude = \"results.LATITUDE\",\n         longitude = \"results.LONGITUDE\") %>%\n  bind_rows(tibble(\n    postal_code = \"679962\",\n    school_name = \"ZHENGHUA SECONDARY SCHOOL\",\n    latitude = 1.3887,\n    longitude = 103.7652\n  )) %>%\n  drop_na() %>%\n  select(postal_code, school_name, latitude, longitude)\n```\n:::\n\n\n### Converting an aspatial data into sf tibble data.frame\n\nNext, we converted schools tibble data.frame data into a simple feature tibble data.frame called **schools_sf** by using values in latitude and longitude field using **st_as_sf** function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools_sf <- st_as_sf(schools,\n                       coords =c(\"longitude\",\"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs=3414)\n\nwrite_rds(schools_sf, \"data/rds/schools_sf.rds\")\n```\n:::\n\n\n### Plotting a point simple feature layer of schools onto mpsz\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools_sf <- read_rds( \"data/rds/schools_sf.rds\")\n\ntmap_mode(\"plot\")\n\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.5) +\n  tm_borders(lwd = 1, alpha = 0.5) +\n  tm_layout(frame = FALSE)+\n  tmap_style(\"gray\")+\ntm_shape(schools_sf) +\n  tm_dots(col = \"purple\", size = 0.1) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## Specially Collected Data\n\nNow, let's import the rest of the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBusiness <- st_read(dsn = \"data/geospatial\", \n                   layer = \"Business\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `Business' from data source \n  `C:\\weipengten\\Main\\Geospatial_Analytics\\Analyzing_Bus_Traffic_Flows\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6550 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3669.148 ymin: 25408.41 xmax: 47034.83 ymax: 50148.54\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nFinServ <- st_read(dsn = \"data/geospatial\", \n                   layer = \"FinServ\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `FinServ' from data source \n  `C:\\weipengten\\Main\\Geospatial_Analytics\\Analyzing_Bus_Traffic_Flows\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3320 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4881.527 ymin: 25171.88 xmax: 46526.16 ymax: 49338.02\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nFnB <- st_read(dsn = \"data/geospatial\", \n                   layer = \"F&B\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `F&B' from data source \n  `C:\\weipengten\\Main\\Geospatial_Analytics\\Analyzing_Bus_Traffic_Flows\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1919 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6010.495 ymin: 25343.27 xmax: 45462.43 ymax: 48796.21\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRetails <- st_read(dsn = \"data/geospatial\", \n                   layer = \"Retails\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `Retails' from data source \n  `C:\\weipengten\\Main\\Geospatial_Analytics\\Analyzing_Bus_Traffic_Flows\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 37635 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4737.982 ymin: 25171.88 xmax: 48265.04 ymax: 50135.28\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nEntertn <- st_read(dsn = \"data/geospatial\", \n                   layer = \"entertn\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `entertn' from data source \n  `C:\\weipengten\\Main\\Geospatial_Analytics\\Analyzing_Bus_Traffic_Flows\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 114 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 10809.34 ymin: 26528.63 xmax: 41600.62 ymax: 46375.77\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nLeisure <- st_read(dsn = \"data/geospatial\", \n                   layer = \"Liesure&Recreation\") %>%\n  st_transform(crs=3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `Liesure&Recreation' from data source \n  `C:\\weipengten\\Main\\Geospatial_Analytics\\Analyzing_Bus_Traffic_Flows\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1217 features and 30 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6010.495 ymin: 25134.28 xmax: 48439.77 ymax: 50078.88\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n:::\n\n# 2. Data Cleaning\n\n::: panel-tabset\n## Passenger Volume\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(odbus)\n```\n:::\n\n\nAs we intend to utilize Bus-stop codes as our unique identifiers when joining with our other datasets, it is not advisable to have it remain as a chr datatype. In fact, we should change it to a factor datatype.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- odbus %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\n### Checking for Missing Data\n\nThere is no missing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(odbus)\n```\n:::\n\n\n### Classifying Peak Hours\n\nWith reference to the time intervals provided in the requirements, we computed the passenger trips generated by origin. The passenger trips by origin are saved as - weekday_morning_peak\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweekday_morning_peak <- odbus %>%\n  filter(DAY_TYPE == \"WEEKDAY\") %>%\n  filter(TIME_PER_HOUR >= 6 &\n           TIME_PER_HOUR <= 9) %>%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %>%\n\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nwrite_rds(weekday_morning_peak, \"data/rds/weekday_morning_peak.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nweekday_morning_peak <- read_rds(\"data/rds/weekday_morning_peak.rds\")\n```\n:::\n\n\nIn the code above, we have did a summation of Origin trips , grouped by the origin bus stop number for the weekday_morning_peak through filtering for weekdays for the time range 6am to 9am.\n\nWe save our processed data into .rds data format files using the `write_rds()` of **readr** package. The output file is saved in *rds* sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\n## Bus Stop Location\n\n### Checking for Duplicates\n\nPassed initial checks for whole duplicate rows, however...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- busstop %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nduplicate bus stops found, removing duplicates directly...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- busstop[duplicated(busstop$BUS_STOP_N), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n  # Remove duplicates from the original dataframe\n  busstop <- busstop[!duplicated(busstop$BUS_STOP_N), ]\n  cat(\"Duplicates removed from the BUS_STOP_N column.\\n\")\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n```\n:::\n\n\nChecked duplicates removed successfully\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- busstop[duplicated(busstop$BUS_STOP_N), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n```\n:::\n\n\n### Checking for Missing Data\n\nNo missing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(busstop)\n```\n:::\n\n\nFinally, we will save the cleaned busstop dataset into .rds data format using the `write_rds()` of **readr** package. The output file is saved in *rds* sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(busstop, \"data/rds/busstop.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbusstop <- read_rds(\"data/rds/busstop.rds\")\n```\n:::\n\n\n## Hexagonal Dataset\n\n### Create Hexagon Dataset from busstop\n\nNext we proceed to fulfill our requirement of preparing a hexagon dataset with specified cell dimensions of 375 by 375 units called **hexagon** using the **st_make_grid** function from the sf package.\n\nWe convert it into a sf dataframe called **hexagon_sf** using the **st_sf** function of sf package.\n\nThe code also adds a new variable/column called \"grid_id\" to the sf object. The \"grid_id\" values are assigned incrementally, starting from 1 and corresponding to the order of the hexagons in the grid. This step essentially assigns a unique identifier to each hexagon in the grid, facilitating further spatial analysis or mapping.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon = st_make_grid(busstop, c(375, 375), what = \"polygons\", square = FALSE)\n# To sf and add grid ID\nhexagon_sf = st_sf(hexagon) %>%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(hexagon))) %>%\n  st_transform(crs = 3414)\n```\n:::\n\n\nlet's change grid_id factor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$grid_id <- as.factor(hexagon_sf$grid_id)\n```\n:::\n\n\n### Checking for Duplicates\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- hexagon_sf[duplicated(hexagon_sf$grid_id), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the grid_id column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the grid_id column.\\n\")\n}\n```\n:::\n\n\n### Checking for Missing Data\n\nA brief overplot shows that there are 9918 grids in total and 7744 are without bus stops. We have a max of 10 bus stops per grid_id\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$n_colli = lengths(st_intersects(hexagon_sf, busstop))\n\ncount_all_grid_ids <- n_distinct(hexagon_sf$grid_id)\n\ncount_zero_bus_stops <- hexagon_sf %>%\n\n  filter(n_colli == 0) %>%\n\n  summarize(count = n_distinct(grid_id)) %>%\n\n  pull(count)\n\nprint(count_all_grid_ids)\n\nprint(count_zero_bus_stops)\n\nsummary(hexagon_sf$n_colli)\n```\n:::\n\n\nFilter for only hexagon data with non-zero counts of bus stops\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf = filter(hexagon_sf, n_colli > 0)\nwrite_rds(hexagon_sf, \"data/rds/hexagon_sf.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf <- read_rds(\"data/rds/hexagon_sf.rds\")\n```\n:::\n\n\n### VIsualising the dataset\n\nWe can also do a visualisation to analyze the distribution of busstops. We specify break points at 0,1,2,3,4 and 5\n\nFrom the map below, it is obvious that most hexagons have 1 or 2 bus stops in their grid with some having 4 or 5 bus stops. There is approximately one 'cluster' that are close to each other and having 4 or 5 bus stops in each region in North, East, South, West.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| code-fold: true\n\n#| code-summary: \"Show the code\"\n\ntmap_mode(\"plot\")\n\nmap_busstopcounts = tm_shape(hexagon_sf) +\n\n  tm_fill(\n\n    col = \"n_colli\",\n\n    palette = c(\"grey\",rev(viridis(5))),\n\n    breaks = c(0, 1, 2, 3, 4, 5,6,7,8,9,10),\n\n    title = \"Number of Busstops\",\n\n    id = \"grid_id\",\n\n    showNA = FALSE,\n\n    alpha = 0.6,\n\n    popup.vars = c(\n\n      \"Number of collisions: \" = \"n_colli\"\n\n    ),\n\n    popup.format = list(\n\n      n_colli = list(format = \"f\", digits = 0)\n\n    )\n\n  ) +\n\n  tm_borders(col = \"grey40\", lwd = 0.7) +\n  tm_view(set.zoom.limits = c(11, 14))\n\nmap_busstopcounts\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\nA few notable findings were:\n\n-   In the North-West, bus stops are scarce around the cemetery in Choa Chu Kang, the nearest bus stops in that area are those along Lim Chu Kang road. Tengah Airbase is also located in that area.\n\n-   At the far East, bus stops are scarce around Changi Airport\n\n    *- \"grid_id\" = 9888 is an extreme outlier, we will need to drop it*\n\n    *- \"grid_id\" for 9182, 9348, 9431 are potential outliers as well*\n\n-   Towards the middle, we have Paya Lebar Airbase\n\n-   In the middle, we have the Central Water Catchment\n\n-   A standalone bus stop in Sentosa Island\n\n    *- \"grid_id\" = 5105 is a potential outlier and should be considered for exclusion*\n\n-   A few bus stops in Johor are surprisingly in our dataset too and in\n\n    *- \"grid_id\" = 3154 is an extreme outlier, we will need to drop it.*\n\n    *- \"grid_id\" for 3646, 3729, 3812 are potential outliers as well*\n\n-   Other than those mentioned above, the positioning of the rest of the bus stops seem to be acceptable and will not skew our dataset too much.\n\n### Removing Outliers\n\nHence, let's proceed straight to dropping these data that will likely cause problems for our analysis. After deeper consideration, we decided that we should drop three extreme outliers, which are grid_ids for 9888, 5105, 3154\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nhexagon_sf <- hexagon_sf %>%\n\n  filter(!grid_id %in% c(9888, 5105, 3154))\n```\n:::\n\n\n## Train Station\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(trainstation)\n```\n:::\n\n\nColumns STN_NAM_DE, TYP_CD_DES and geometry looks like they will be useful for our analysis. Let's rename them to be more intuitive. It's also necessary to check that we do not have duplicate STN_NAM_DE too as it is out identifier for the trainstation dataset. STN_NAM_DE is now renamed to **STATION_NAME** and TYP_CD_DES to **STATION_TYPE**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrainstation <- trainstation %>%\n  rename(STATION_NAME = STN_NAM_DE, STATION_TYPE = TYP_CD_DES)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows, let's proceed to checking for duplicates in the ST_NAM_DE column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- trainstation %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nThere are are 25 in the ST_NAM_DE column. HOWEVER, it is important to note that we can definitely have more than one station sharing the same name. For example, Tampines MRT station for east-west line can be a distance away from Tampines MRT station downtown line. Hence it is unwise to drop any duplicate stations we found here and keep it as it is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- trainstation[duplicated(trainstation$STATION_NAME), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the ST_NAM_DE column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the ST_NAM_DE column.\\n\")\n}\n```\n:::\n\n\n### Checking for Missing Data\n\nNo Missing Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(trainstation)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(trainstation, \"data/rds/trainstation.rds\")\ntrainstation <- read_rds(\"data/rds/trainstation.rds\")\n```\n:::\n\n\n## Train Station Exit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(trainstationEP)\n```\n:::\n\n\nSimilarly, let's rename the columns in trainstationEP. STN_NAM_DE is now renamed to **STATION_NAME** and TYP_CD_DES to **STATION_TYPE**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrainstationEP <- trainstationEP %>%\n  rename(STATION_NAME = STN_NAM_DE, STATION_TYPE = TYP_CD_DES)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- trainstationEP %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nChecking for duplicate records with same geometry, the results below demonstrate that each geometry value is unqiue\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- trainstationEP[duplicated(trainstationEP$geometry), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the geometry column.\\n\")\n  print(duplicates)\n  # Remove duplicates from the original dataframe\n  trainstationEP <- trainstationEP[!duplicated(trainstationEP$geometry), ]\n  cat(\"Duplicates removed from the geometry column.\\n\")\n} else {\n  cat(\"No duplicate values found in the geometry column.\\n\")\n}\n```\n:::\n\n\n### Feature Engineering\n\nIt is wise to note that STATION_NAME here represents the station exit.\n\nEach duplicate in the STATION_NAME likely represents a unique STATION EXIT and we perform the below. We also have tested that none of the STATION_NAME has similar geometry indirectly.\n\n> Note: (however, a more rigorous check would also involve calculating distances between said Exits of same station to check if they are of the acceptable range. We did not do that due to time limitations and having 8 datasets to investigate)\n\nThe data transformation involves grouping the dataset by the variable STATION_NAME. Within each group, a new column named **\"Exit\"** is created, representing the row number within that group.\n\nThe dataset is then ungrouped, and a composite column named **STATION_EXIT** is generated by combining the values of STATION_NAME and \"Exit\" with an underscore. This process results in a unique identifier (STATION_EXIT) for each entry, capturing the occurrence sequence within each station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrainstationEP <- trainstationEP %>%\n  group_by(STATION_NAME) %>%\n  mutate(Exit = row_number()) %>%\n  ungroup() %>%\n  mutate(STATION_EXIT = paste(STATION_NAME, Exit, sep = \"_\"))\n```\n:::\n\n\n### Checking for Missing Data\n\nThere is no missing data in our required columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(trainstationEP)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(trainstationEP, \"data/rds/trainstationEP.rds\")\ntrainstationEP <- read_rds(\"data/rds/trainstationEP.rds\")\n```\n:::\n\n\n## Train Lines\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(mrt)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows, let's proceed to checking for duplicates in the ST_NAM_DE column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- mrt %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\n### Checking for Missing Data\n\nNo Missing Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mrt)\n```\n:::\n\n\n## Intergrated Transport Hub\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(ith)\n```\n:::\n\n\n### Checking for Duplicates\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- ith %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nduplicates <- ith[duplicated(ith$geometry), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the geometry column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the geometry column.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo duplicate values found in the geometry column.\n```\n:::\n:::\n\n\n### Checking for Missing Data\n\nNo Missing Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ith)\n```\n:::\n\n\n## MPSZ-2019\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(mpsz)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows, let's proceed to checking for duplicates in the SUBZONE_C column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- mpsz %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nThere are no duplicates in the SUBZONE_C column too\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- mpsz[duplicated(mpsz$SUBZONE_C), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the SUBZONE_C column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the SUBZONE_C column.\\n\")\n}\n```\n:::\n\n\n### Checking for Missing Data\n\nNo Missing Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mpsz)\n```\n:::\n\n\n## Parks\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(parks)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows, let's proceed to checking for duplicates in the geometry column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- parks %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nThere are no duplicates in the geometry column too\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- parks[duplicated(parks$geometry), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the geometry column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the geometry column.\\n\")\n}\n```\n:::\n\n\n### Checking for Missing Data\n\nNo Missing Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(parks)\n```\n:::\n\n\n## HDB\n\n### Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(hdb_sf)\n```\n:::\n\n\n### Checking for Duplicates\n\nPassed initial checks in code chunk below for whole duplicate rows, let's proceed to checking for duplicates in the addr column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- hdb_sf %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nIt seems some duplicates came up when checking the \\*\\*addr\\* column,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- hdb_sf[duplicated(hdb_sf$addr), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the addr column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the addr column.\\n\")\n}\n```\n:::\n\n\nSpecifying the unique identifier as a combination of **addr**, **blk_no** and **street**, it was shown that data is unqiue and there were no duplicates after all.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhdb_sf <- hdb_sf %>%\n  mutate(unique_identifier = paste(addr, blk_no, street, sep = \"_\"))\n\nduplicates <- hdb_sf[duplicated(hdb_sf$unique_identifier), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the unique_identifier column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the unique_identifier column.\\n\")\n}\n```\n:::\n\n\n### Checking for Missing Data\n\nFrom the results, it seems there is at least one row that has missing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(hdb_sf)\n```\n:::\n\n\nWe will drop the record indexed 8981 in ADMIRALTY since it is missing important SUBZONE data and it might affect out analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhdb_sf <- hdb_sf %>%\n   filter(!...1 %in% c(8981))\n```\n:::\n\n\n## School Directory\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(schools_sf)\n```\n:::\n\n\n### Checking for Duplicates\n\nFailed initial checks in code chunk below for whole duplicate rows,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- schools_sf %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools_sf <- unique(schools_sf)\n```\n:::\n\n\nThere is no duplicates in the \\*\\*school_name\\* column now,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- schools_sf[duplicated(schools_sf$school_name), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the school_name column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the school_name column.\\n\")\n}\n```\n:::\n\n\nIt seems some duplicates came up when checking the \\*\\*postal_code\\* column,\n\nWe decided to treat these special schools as 4 unique records as they provide different services and purposes (eg. primary and secondary) and should not be treated as the same. Hence, no additional cleaning is required.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicates <- schools_sf[duplicated(schools_sf$postal_code), ]\nduplicates_records <- schools_sf[schools_sf$postal_code %in% duplicates$postal_code, ]\n\n# Print the new dataframe\nprint(duplicates_records)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(schools_sf, \"data/rds/schools_sf_cleaned.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nschools_sf_cleaned <- read_rds( \"data/rds/schools_sf_cleaned.rds\")\n```\n:::\n\n\n## Specially Collected Data\n\nWe briefly checked for duplicates in the specially prepared datasets by Prof Kam and found out that 4 of them have few duplicates whereas FinServ and Retails have multiple duplicates which we do not have time to clean and to make sense of.\n\nFor *Business* and *Entertn* dataset, we found one duplicate each, we then handled then by only using the unique records\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- Business %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n\nBusiness <- unique(Business)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- Entertn %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n\nEntertn <- unique(Entertn)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- FnB %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- Retails %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- FinServ %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- Leisure %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n:::\n\n# 3. Combining the Datasets\n\n::: panel-tabset\n## Busstop and Hexagon\n\n### Joining the data\n\nWe needed to perform aggregation of passenger trips by Hexagon instead of Origin Bus Stop, hence we need to first integrate bus stop data and the hexagon dataset using the **st_intersection** function from the sf package. The intersection operation retains only the spatial elements (points) that overlap between the original bus stop locations and the hexagonal grid.The resulting busstop_hexagon dataset contains information about which hexagon grid each bus stop is located in.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Combine Busstop and Hexagon\nbusstop_hexagon <- st_intersection(busstop, hexagon_sf) %>%\n  select(BUS_STOP_N, grid_id) %>%\n  st_drop_geometry\n```\n:::\n\n\n### Post-join Checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- busstop_hexagon %>% \n  group_by_all() %>% \n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nThere is one duplicate in the BUS_STOP_N column, we will proceed to dropping it\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nduplicates <- busstop_hexagon[duplicated(busstop_hexagon$BUS_STOP_N), ]\n# Check if there are any duplicates\nif (nrow(duplicates) > 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n  # Remove duplicates from the original dataframe\n  busstop_hexagon <- busstop_hexagon[!duplicated(busstop_hexagon$BUS_STOP_N), ]\n  cat(\"Duplicates removed from the BUS_STOP_N column.\\n\")\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDuplicate values found in the BUS_STOP_N column.\n       BUS_STOP_N grid_id\n3269.1      25059      86\nDuplicates removed from the BUS_STOP_N column.\n```\n:::\n:::\n\n\nDuplicate successfully removed\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nduplicates <- busstop_hexagon[duplicated(busstop_hexagon$BUS_STOP_N), ]\nprint(duplicates)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] BUS_STOP_N grid_id   \n<0 rows> (or 0-length row.names)\n```\n:::\n:::\n\n\nNo missing data post-join\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(busstop_hexagon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  BUS_STOP_N           grid_id    \n Length:5141        3632   :  10  \n Class :character   6912   :   8  \n Mode  :character   7649   :   8  \n                    2669   :   7  \n                    4638   :   7  \n                    5775   :   7  \n                    (Other):5094  \n```\n:::\n:::\n\n\n## Busstop and Hexagon and Origin Data\n\n### Joining the data\n\nNext, we are going to append the planning subzone code from busstop_hexagon data frame onto weekday_morning_peak data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nod_data  <- left_join(weekday_morning_peak,\n                         busstop_hexagon,\n                         by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %>% \n  rename(ORIGIN_BS = ORIGIN_PT_CODE,  \n         ORIGIN_GRID = grid_id, \n         DESTIN_BS = DESTINATION_PT_CODE)\n```\n:::\n\n\n### Post-join Checks\n\nNo duplicates found\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- od_data  %>% \n  group_by_all() %>% \n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nWe found some missing data in the ORIGIN_GRID column but that is expected since we did dropped some outliers when investigating the hexagon dataset\n\nLet's proceed to dropping them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(od_data)\n```\n:::\n\n\nRemoved NA values for ORIGIN_GRID column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nod_data <- od_data %>%\n  filter(!is.na(ORIGIN_GRID))\nsummary(od_data)\n```\n:::\n\n\nNext, we will update od_data data frame with the hexagon grids.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nod_data <- left_join(od_data , busstop_hexagon, by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"))\n\nduplicate <- od_data %>% \n  group_by_all() %>%\n  filter(n()>1) %>% \n  ungroup()\nduplicate\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nod_data <- unique(od_data)\n\nod_data <- od_data %>% \nrename(DESTIN_GRID = grid_id) %>% \n  drop_na() %>% \n  group_by(ORIGIN_GRID, DESTIN_GRID) %>% \n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\nwrite_rds(od_data,  \"data/rds/od_data.rds\")\n```\n:::\n\n:::\n\n# 4. Visualising Spatial Interaction\n\nIn this section, you will learn how to prepare a desire line by using **stplanr** package.\n\n::: panel-tabset\n## Creating Flow Data\n\n### Removing intra-zonal flows\n\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nod_data <- read_rds( \"data/rds/od_data.rds\")\nod_data1 <- od_data[od_data$ORIGIN_GRID!=od_data$DESTIN_GRID,]\n```\n:::\n\n\n### Creating desire lines\n\nIn this code chunk below, `od2line()` of **stplanr** package is used to create the desire lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflowLine <- od2line(flow = od_data1,\n                    zones = hexagon_sf,\n                    zone_code = \"grid_id\")\n```\n:::\n\n\n## Visualising the desire lines for (MORNING_PEAK \\>= 5000)\n\nTo visualise the resulting desire lines, the code chunk below is used.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ntm_shape(hexagon_sf) +\n  tm_polygons() +\nflowLine %>%  \n  filter(MORNING_PEAK >= 5000) %>%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-113-1.png){width=672}\n:::\n:::\n\n\nThe filtered results above likely describe a few key patterns to daily commuting for the morning peak weekdays:\n\n-   Those multiple and short lines are likely bus routes people take to the interchange for weekdays peak morning 6am to 9am for their daily commute.\n\n-   Those long bus routes across the country are probably the more efficient routes people can take via bus that's actually more convenient than by taking the train.\n\nOverall, these patterns suggest that the majority of people commute via bus to interchange or an mrt station before continuing the long journey ahead. However is this the most efficient commuting pattern or is it because we do not have other choices? After all, the huge volume of commute to the interchange likely means that busses stop at busstops for longer periods of time to alight passengers. Is it wise to build more busstops and install more traffic lights in between these routes?\n\n## Visualising the desire lines for (MORNING_PEAK \\>= 10000)\n\nTo visualise the resulting desire lines, the code chunk below is used.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ntm_shape(hexagon_sf) +\n  tm_polygons() +\nflowLine %>%  \n  filter(MORNING_PEAK >= 10000) %>%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-114-1.png){width=672}\n:::\n:::\n\n\nFor MORNING_PEAK \\>= 10000, it reveals a few possible interesting observations:\n\n-   One long-distance route stands out which is the Changi-Woodlands route. Also notably, the bus routes in Woodlands are quite prominent here, showing us just how many people are frequenting these routes.\n\n-   Apparently some hubs and interchanges have more frequented bus routes as compared to the others. One thing we can conclude is that the bus routes are frequented there more than others. However, there could be many reasons for such a phenomena. Jurong west and Jurong East seem to have higher flows as compared to other areas (other than Woodlands), however maybe it could be that the area is very inaccessible and inconvenient that it is necessary to commute via the train whereas other regions might have more accessible bus routes and train stations?\n\n## Visualising the desire lines for (MORNING_PEAK \\>= 20000)\n\nTo visualise the resulting desire lines, the code chunk below is used.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\ntm_shape(hexagon_sf) +\n  tm_polygons() +\nflowLine %>%  \n  filter(MORNING_PEAK >= 20000) %>%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-115-1.png){width=672}\n:::\n:::\n\n\nDefinitely an exceedingly high amount of traffic can be observed here, government should plan for new mrt locations with this in mind such that we can disperse the bulk of the traffic flow in these regions. Later on, we will proceed further to find out what are the factors that could have result in such phenomena.\n:::\n\n# 5. Preparing Distance Matrix and Flow Data\n\n::: panel-tabset\n## Preparing Distance Matrix\n\n### Converting hexagon_sf to SpatialPolygonsDataFrame\n\nWe are required to compute a distance matrix by using the analytical hexagon data derived earlier.\n\nResearch have shown that computing distance matrix by using sp method is more efficient for large datasets. In view of this, sp method is used in the code chunks below.\n\nFirst [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) will be used to convert *hexagon_sf* from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sp <- as(hexagon_sf, \"Spatial\")\nhexagon_sp\n```\n:::\n\n\n### Computing the distance matrix\n\nNext, [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones. The result is stored in the **dist** object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist <- spDists(hexagon_sp, \n                longlat = FALSE)\nhead(dist, n=c(10, 10))\n```\n:::\n\n\n### Labelling column and row heanders of a distance matrix\n\nFirst, we will create a list sorted according to the the distance matrix by planning grid_id.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_names <- hexagon_sf$grid_id\n```\n:::\n\n\nNext we will attach grid_id to row and column for distance matrix matching ahead\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(dist) <- paste0(grid_names)\nrownames(dist) <- paste0(grid_names)\n```\n:::\n\n\n### Pivoting distance value by grid_id\n\nThis code chunk melts the distance matrix into a long format, creating a dataframe **distPair** with columns for origin (Var1), destination (Var2), and distance (dist).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistPair <- melt(dist) %>%\n  rename(dist = value)\nhead(distPair, 10)\n```\n:::\n\n\n### Updating intra-zonal distances\n\nIn this section, we are going to append a constant value to replace the intra-zonal distance of 0.\n\nThe code snippet below first filters the distPair dataframe to exclude intra-zonal distances (dist \\> 0) and then displays a summary of the remaining distances. This information is used to decide on a constant distance value to replace intra-zonal distances.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistPair %>%\n  filter(dist > 0) %>%\n  summary()\n```\n:::\n\n\nWe found out that the minimum distance is 375m, let's treat the intra-zone distance benchmark asan approximately 150m (or \\~ 375/2).\n\nThis code chunk updates the intra-zonal distances in the distPair dataframe, replacing distances of 0 with a constant value of 150.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistPair$dist <- ifelse(distPair$dist == 0,\n                        150, distPair$dist)\n```\n:::\n\n\nThis code chunk renames the columns Var1 and Var2 to orig and dest, respectively, for clarity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistPair <- distPair %>%\n  rename(orig = Var1,\n         dest = Var2)\n```\n:::\n\n\nThis code snippet converts the orig and dest columns in the distPair dataframe to factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistPair$orig <- as.factor(distPair$orig)\ndistPair$dest <- as.factor(distPair$dest)\n```\n:::\n\n\nLastly, the code chunk below is used to save the dataframe for future use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(distPair, \"data/rds/distPair.rds\") \n```\n:::\n\n\nFinally, we have prepared our distance matrix\n\n## Preparing Flow Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data <- od_data %>%\n  group_by(ORIGIN_GRID, DESTIN_GRID) %>% \n  summarize(TRIPS = sum(MORNING_PEAK)) \nhead(flow_data, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n# Groups:   ORIGIN_GRID [2]\n   ORIGIN_GRID DESTIN_GRID TRIPS\n   <fct>       <fct>       <dbl>\n 1 3           255             1\n 2 3           296             1\n 3 3           377             4\n 4 3           552             3\n 5 3           594            93\n 6 3           597             1\n 7 170         3               1\n 8 170         211             1\n 9 170         255             2\n10 170         298             2\n```\n:::\n:::\n\n\n### Separating intra-flow from passenger volume df\n\nIn this code snippet, two new fields, **FlowNoIntra** and **offset**, are added to the flow_data dataframe based on the condition of equality between ORIGIN_GRID and DESTIN_GRID.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data$FlowNoIntra <- ifelse(\n  flow_data$ORIGIN_GRID == flow_data$DESTIN_GRID, \n  0, flow_data$TRIPS)\nflow_data$offset <- ifelse(\n  flow_data$ORIGIN_GRID == flow_data$DESTIN_GRID, \n  0.000001, 1)\n```\n:::\n\n\nThe code chunk filters the flow_data dataframe to include only inter-zonal flows (FlowNoIntra \\> 0), creating a new dataframe named inter_zonal_flow\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninter_zonal_flow <- flow_data %>%\n  filter(FlowNoIntra > 0)\n```\n:::\n\n\n### Combining passenger volume data with distance value\n\nThe code chunk below performs a left_join() of dplyr between the inter_zonal_flow dataframe and the distPair dataframe based on the matching conditions of ORIGIN_GRID and DESTIN_GRID, creating a new dataframe named **flow_data1**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndistPair <- read_rds(\"data/rds/distPair.rds\")\n\nflow_data1 <- inter_zonal_flow %>%\n  left_join (distPair,\n             by = c(\"ORIGIN_GRID\" = \"orig\",\n                    \"DESTIN_GRID\" = \"dest\"))\n```\n:::\n\n:::\n\n# 6. Preparing Attractiveness and Propulsiveness Attributes\n\n::: panel-tabset\n## School Proximity\n\nSchools can be a huge attributing factor to the total number of trips in an area. We can analyze this in two possible ways: school count and distance to nearest school.\n\n### School Count\n\nSchools can significantly influence morning weekday peak origin flows, as traffic may be impacted by students commuting to school. To capture this effect, we calculate the number of schools within each hexagon using the **st_intersects()** function, and the results are stored in a new variable named **SCHOOL_COUNT** in the **hexagon_sf** dataframe. The summary provides an overview of school counts across hexagons\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$`SCHOOL_COUNT`<- lengths(\n  st_intersects(\n    hexagon_sf, schools_sf_cleaned))\nsummary(hexagon_sf$SCHOOL_COUNT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.1325  0.0000  3.0000 \n```\n:::\n:::\n\n\n### Distance to nearest School\n\nUnderstanding the distance from each hexagon to the nearest school is crucial for assessing school proximity. We use the **st_nearest_feature()** function to find the nearest schools for each hexagon, calculate the distances using **st_distance()**, and extract the minimum distance as a new variable, **MIN_DISTANCE_TO_SCHOO**L. The summary provides insights into the distribution of minimum distances.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use st_nearest_feature to get the indices of the nearest schools for each hexagon\nnearest_indices <- st_nearest_feature(hexagon_sf, schools_sf_cleaned)\n\n# Subset the schools_sf using the indices\nnearest_schools <- schools_sf_cleaned[nearest_indices, ]\n\n# Calculate the distances\nmin_distances <- st_distance(hexagon_sf, nearest_schools)\n\n# Extract the minimum distance from each row\nhexagon_sf$MIN_DISTANCE_TO_SCHOOL <- apply(min_distances, 1, min)\n\n# Display summary of the minimum distances\nsummary(hexagon_sf$MIN_DISTANCE_TO_SCHOOL)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   108.0   351.0   906.4   923.7 10766.1 \n```\n:::\n:::\n\n\n## CBD Proximity\n\nUnderstanding the proximity of hexagons to the **Central Business District** (CBD) is essential for assessing traffic patterns. We calculate the distances from each hexagon to the CBD using **st_distance()**, and the results are stored in a new variable named **DISTANCE_TO_CBD**. The summary provides an overview of the minimum distances to the CBD across hexagons.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the distances\nDISTANCE_TO_CBD <- st_distance(hexagon_sf, cbd_sf)\nhexagon_sf$DISTANCE_TO_CBD <- apply(DISTANCE_TO_CBD, 1, min)\n\n# Display summary of the minimum distances\nsummary(hexagon_sf$DISTANCE_TO_CBD)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0    7038   11833   11637   15990   25983 \n```\n:::\n:::\n\n\n## Train Line Proximity\n\nProximity to train lines is a key factor influencing traffic patterns. We use the **st_nearest_feature()** function to find the nearest train lines for each hexagon, calculate the distances using **st_distance()**, and extract the minimum distance as a new variable, **MIN_DISTANCE_TO_MRTLINE**. The summary provides insights into the distribution of minimum distances to train lines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use st_nearest_feature to get the indices of the nearest schools for each hexagon\nnearest_indices <- st_nearest_feature(hexagon_sf, mrt)\n\n# Subset the schools_sf using the indices\nnearest_line <- mrt[nearest_indices, ]\n\n# Calculate the distances\nmin_distances <- st_distance(hexagon_sf, nearest_line)\n\n# Extract the minimum distance from each row\nhexagon_sf$MIN_DISTANCE_TO_MRTLINE <- apply(min_distances, 1, min)\n\n# Display summary of the minimum distances\nsummary(hexagon_sf$MIN_DISTANCE_TO_MRTLINE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   154.1   691.5  1265.9  1529.0 11982.0 \n```\n:::\n:::\n\n\n## ITH Proximity\n\nProximity to **Integrated Transport Hubs** (ITH) can impact traffic patterns. We use the **st_nearest_feature()** function to find the nearest ITH for each hexagon, calculate the distances using **st_distance()**, and extract the minimum distance as a new variable, **MIN_DISTANCE_TO_ITH**. The summary provides insights into the distribution of minimum distances to ITH.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use st_nearest_feature to get the indices of the nearest schools for each hexagon\nnearest_indices <- st_nearest_feature(hexagon_sf, ith)\n\n# Subset the schools_sf using the indices\nnearest_ith<- ith[nearest_indices, ]\n\n# Calculate the distances\nmin_distances <- st_distance(hexagon_sf, nearest_ith)\n\n# Extract the minimum distance from each row\nhexagon_sf$MIN_DISTANCE_TO_ITH <- apply(min_distances, 1, min)\n\n# Display summary of the minimum distances\nsummary(hexagon_sf$MIN_DISTANCE_TO_ITH)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   779.2  1372.6  1685.6  2271.8  9100.4 \n```\n:::\n:::\n\n\n## Park Proximity\n\nProximity to parks can influence the attractiveness of an area. We use the **st_nearest_feature()** function to find the nearest parks for each hexagon, calculate the distances using **st_distance()**, and extract the minimum distance as a new variable, **MIN_DISTANCE_TO_PARK**. The summary provides insights into the distribution of minimum distances to parks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use st_nearest_feature to get the indices of the nearest schools for each hexagon\nnearest_indices <- st_nearest_feature(hexagon_sf, parks)\n\n# Subset the schools_sf using the indices\nnearest_park<- parks[nearest_indices, ]\n\n# Calculate the distances\nmin_distances <- st_distance(hexagon_sf, nearest_park)\n\n# Extract the minimum distance from each row\nhexagon_sf$MIN_DISTANCE_TO_PARK <- apply(min_distances, 1, min)\n\n# Display summary of the minimum distances\nsummary(hexagon_sf$MIN_DISTANCE_TO_PARK)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   164.2   454.3   931.5   963.4 11137.5 \n```\n:::\n:::\n\n\n## HDB Density / Hawker/ Commericial / Avg Age\n\nUnderstanding the density of residential use is valuable for assessing morning peak flows. The code chunk below intersects hexagon_sf with hdb_sf to obtain HDB data within each hexagon. We then calculate various attributes such as the total number of dwelling units (**UNITS**), the presence of markets and hawker centers (**MARKET_HAWKER_Y**), the presence of commercial areas (**COMMERCIAL_Y**), and the average age of HDB units (**AVG_AGE**). The summary provides an overview of these attributes across hexagons.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Combine Busstop and Hexagon\nhdb_hexagon <- st_intersection(hexagon_sf, hdb_sf)%>%\n  st_drop_geometry\n```\n:::\n\n\nNo duplicates were found for whole rows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- hdb_hexagon %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\nThere is no missing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(hdb_hexagon)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhdb_hexagon_counts <- hdb_hexagon %>%\n  group_by(grid_id) %>%\n  summarise(\n    UNITS = sum(total_dwelling_units),\n    MARKET_HAWKER_Y = sum(market_hawker == \"Y\"),\n    COMMERCIAL_Y = sum(commercial == \"Y\"),\n    AVG_AGE = mean(as.numeric(format(Sys.Date(), \"%Y\")) - year_completed, na.rm = TRUE)\n  )\n\nsummary(hdb_hexagon_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    grid_id        UNITS        MARKET_HAWKER_Y   COMMERCIAL_Y   \n 1758   :  1   Min.   :   0.0   Min.   :0.0000   Min.   : 0.000  \n 1799   :  1   1st Qu.: 481.0   1st Qu.:0.0000   1st Qu.: 1.000  \n 1800   :  1   Median : 986.5   Median :0.0000   Median : 2.000  \n 1841   :  1   Mean   :1014.1   Mean   :0.1029   Mean   : 2.389  \n 1883   :  1   3rd Qu.:1496.2   3rd Qu.:0.0000   3rd Qu.: 3.000  \n 1923   :  1   Max.   :3315.0   Max.   :3.0000   Max.   :20.000  \n (Other):956                                                     \n    AVG_AGE     \n Min.   : 3.00  \n 1st Qu.:24.89  \n Median :34.04  \n Mean   :31.98  \n 3rd Qu.:40.11  \n Max.   :73.86  \n                \n```\n:::\n:::\n\n\n## Business, FnB, Entertn, Leisure Count\n\nFor the section below, we do a count of the these entities in each grid and derived the variables **Business_COUNT**, **Entertn_COUNT**, **FnB_COUNT**, **Leisure_COUNT** in hexagon_sf dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$`Business_COUNT`<- lengths(\n  st_intersects(\n    hexagon_sf, Business))\nsummary(hexagon_sf$Business_COUNT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   2.078   2.000  44.000 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$`Entertn_COUNT`<- lengths(\n  st_intersects(\n    hexagon_sf, Entertn))\nsummary(hexagon_sf$Entertn_COUNT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.00000 0.00000 0.04556 0.00000 7.00000 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$`FnB_COUNT`<- lengths(\n  st_intersects(\n    hexagon_sf, FnB))\nsummary(hexagon_sf$FnB_COUNT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.8058  0.0000 81.0000 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhexagon_sf$`Leisure_COUNT`<- lengths(\n  st_intersects(\n    hexagon_sf, Leisure))\nsummary(hexagon_sf$Leisure_COUNT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   0.399   0.000  23.000 \n```\n:::\n:::\n\n:::\n\n# 7. Calibrating Spatial Interaction Models\n\n::: panel-tabset\n## Final Data Integration\n\n### Data Integration hdb_hexagon_counts and flow_data1\n\nFinally, we will append SCHOOL_COUNT and MIN_DISTANCE_TO_SCHOOL fields from hexagon_sf data.frame into flow_data sf tibble data.frame by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data1 <- flow_data1 %>%\n  left_join(hdb_hexagon_counts,\n            by = c(\"DESTIN_GRID\" = \"grid_id\")) \n```\n:::\n\n\n### Data Integration hexagon_sf with flow_data1\n\nFinally, we will append SCHOOL_COUNT and MIN_DISTANCE_TO_SCHOOL fields from hexagon_sf data.frame into flow_data sf tibble data.frame by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data1 <- flow_data1 %>%\n  left_join(hexagon_sf,\n            by = c(\"DESTIN_GRID\" = \"grid_id\")) %>%\n  rename(DIST = dist)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nduplicate <- flow_data1 %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  ungroup()\nduplicate\n```\n:::\n\n\n## Checking for variables with zero values\n\nThe purpose of the code chunks below is to handle and adjust values in the **flow_data1** dataframe, specifically replacing zero values with 0.99 in certain columns. This approach is taken to avoid issues associated with zero values in subsequent calculations or analyses, as zero can sometimes lead to undefined or problematic results.\n\nZero values in certain columns may pose issues in computations or statistical analyses. Replacing these zeros with 0.99 avoids potential division-by-zero errors or other problems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data1$Business_COUNT <- ifelse(\n  flow_data1$Business_COUNT == 0,\n  0.99, flow_data1$Business_COUNT)\n\nflow_data1$AVG_AGE <- ifelse(\n  flow_data1$AVG_AGE == 0,\n  0.99, flow_data1$AVG_AGE)\n\nflow_data1$UNITS <- ifelse(\n  flow_data1$UNITS == 0,\n  0.99, flow_data1$UNITS)\n\nflow_data1$MIN_DISTANCE_TO_SCHOOL <- ifelse(\n  flow_data1$MIN_DISTANCE_TO_SCHOOL == 0,\n  0.99, flow_data1$MIN_DISTANCE_TO_SCHOOL)\n\nflow_data1$DISTANCE_TO_CBD <- ifelse(\n  flow_data1$DISTANCE_TO_CBD == 0,\n  0.99, flow_data1$DISTANCE_TO_CBD)\n\nflow_data1$MIN_DISTANCE_TO_MRTLINE <- ifelse(\n  flow_data1$MIN_DISTANCE_TO_MRTLINE == 0,\n  0.99, flow_data1$MIN_DISTANCE_TO_MRTLINE)\n\nflow_data1$MIN_DISTANCE_TO_ITH <- ifelse(\n  flow_data1$MIN_DISTANCE_TO_ITH == 0,\n  0.99, flow_data1$MIN_DISTANCE_TO_ITH)\n\nflow_data1$MIN_DISTANCE_TO_PARK <- ifelse(\n  flow_data1$MIN_DISTANCE_TO_PARK == 0,\n  0.99, flow_data1$MIN_DISTANCE_TO_PARK)\n\nflow_data1$FnB_COUNT <- ifelse(\n  flow_data1$FnB_COUNT == 0,\n  0.99, flow_data1$FnB_COUNT)\n\nflow_data1$Entertn_COUNT <- ifelse(\n  flow_data1$Entertn_COUNT == 0,\n  0.99, flow_data1$Entertn_COUNT)\n\nflow_data1$Leisure_COUNT <- ifelse(\n  flow_data1$Leisure_COUNT == 0,\n  0.99, flow_data1$Leisure_COUNT)\n\nflow_data1$COMMERCIAL_Y <- ifelse(\n  flow_data1$COMMERCIAL_Y == 0,\n  0.99, flow_data1$COMMERCIAL_Y)\n```\n:::\n\n\nAdding 1 to SCHOOL_COUNT and MARKET_HAWKER_Y ensures that these counts, which likely have zero values in certain cases, become nonzero. This is similar to the method above but different in the sense that since these two variables have a very short range of values with SCHOOL_COUNT ranging from 0 to 3, assigning 0.99 to 0 can totally change the meaning of it. We avoid that to a certain extent by adding 1 across all values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data1$SCHOOL_COUNT <- flow_data1$SCHOOL_COUNT + 1\nflow_data1$MARKET_HAWKER_Y <- flow_data1$MARKET_HAWKER_Y + 1\n```\n:::\n\n\nIn summary, these adjustments are made to ensure the robustness and accuracy of subsequent analyses by addressing potential issues associated with zero values in critical columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(flow_data1)\n```\n:::\n\n\n## Visualising the dependent variable\n\nFirstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = flow_data1,\n       aes(x = TRIPS)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-150-1.png){width=672}\n:::\n:::\n\n\nNotice that the distribution is highly skewed and does not resemble bell shape or also known as normal distribution.\n\nNext, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance. A scatter plot is plotted with a linear regression trend line using (**geom_smooth**(method = lm)) to visualize the overall trend in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = flow_data1,\n       aes(x = DIST,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-151-1.png){width=672}\n:::\n:::\n\n\nThe code chunk below utilizes the **ggplot2** package to generate a scatter plot with a logarithmic transformation of both the x and y axes. With (log(DIST)) mapped to the x-axis, and (log(TRIPS)) mapped to the y-axis, a scatter plot is plotted with a linear regression trend line. We can see that their relationship more resembles a linear relationship more by using the log transformed version of both variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = flow_data1,\n       aes(x = log(DIST),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm)\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-152-1.png){width=672}\n:::\n:::\n\n\n## Multi-collinearity\n\nWe check the presence of multi-collinearity with the following using a correlation plot:\n\n-   Exclude the dependent variable\n\n-   Removed variables with zero variance\n\n-   Removed numeric variables with non-zero variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumeric_independent_vars <- flow_data1 %>%\n  ungroup() %>%\n  select_if(is.numeric) %>%\n  select(-TRIPS)  # Exclude the dependent variable if needed\n\n# Remove variables with zero variance\nnumeric_independent_vars <- numeric_independent_vars %>%\n  select_if(function(x) var(x) != 0)\n\n# Check if there are still variables left\nif (ncol(numeric_independent_vars) > 1) {\n  # Calculating the correlation matrix\n  cor_matrix <- cor(numeric_independent_vars)\n\n  # Creating a correlation plot\n  corrplot::corrplot(cor_matrix, method = \"color\", type = \"upper\", order = \"hclust\")\n} else {\n  cat(\"No numeric variables with non-zero variance.\\n\")\n}\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-153-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncorrplot::corrplot(cor_matrix, method = \"color\", type = \"upper\", order = \"hclust\", \n                   addrect = 8, # Add rectangles to represent the correlation matrix structure\n                   tl.srt = 45, # Rotate variable names for better visibility\n                   tl.col = \"black\", # Set text color\n                   tl.cex = 0.8, # Reduce text size for better readability\n                   col = colorRampPalette(c(\"navy\", \"white\", \"firebrick3\"))(100))\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-153-2.png){width=672}\n:::\n:::\n\n\nFrom the results shown above, there seems to be no issue with multicollinearity for our resulting independent variables. We can confirm our interpretation with a VIF analysis.\n\n### VIF\n\nVIF results are also less than 10 for our resulting variables, showing that there is no issue of high multicollinearity, we can safely proceed to our spatial modelling\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate VIF\nvif_results <- car::vif(lm(as.formula(paste(\"TRIPS ~\", paste(names(numeric_independent_vars), collapse = \"+\"))), data = flow_data1))\n\n# Print VIF results\nprint(vif_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            FlowNoIntra                    DIST                 n_colli \n               1.022793                1.047879                1.035589 \n           SCHOOL_COUNT  MIN_DISTANCE_TO_SCHOOL         DISTANCE_TO_CBD \n               1.112150                3.271770                1.605823 \nMIN_DISTANCE_TO_MRTLINE     MIN_DISTANCE_TO_ITH    MIN_DISTANCE_TO_PARK \n               2.215530                1.910249                2.252305 \n         Business_COUNT           Entertn_COUNT               FnB_COUNT \n               1.327628                1.082635                1.879902 \n          Leisure_COUNT \n               1.896698 \n```\n:::\n:::\n\n\nAfter the analysis above, it seems we are likely required to remove **MARKET_HAWKER**, **COMMERCIAL_Y** and **AVG_AGE** as they see mto be exhibiting 0 variance and we are unable to verify whether it's safe to include them in the analysis. We will also remove offset and n_colli.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data1 <- flow_data1 %>%\n  select(-c(MARKET_HAWKER_Y, AVG_AGE, COMMERCIAL_Y, offset, n_colli, UNITS))\n\nwrite_rds(flow_data1,\n          \"data/rds/flow_data_tidy.rds\")\n```\n:::\n\n:::\n\n# 8.Spatial Modelling\n\nFor analysis focused on understanding origin flows, an **Origin-constrained** spatial model is more appropriate than **Destination constrained** spatial models. Similarly **Unconstrained** and **Doubly constrained** models are also appropriate\n\n-   Importing the modelling data\n\nWe first import the tidied spatial modelling data (**flow_data_tidy_SIM**) from an RDS file. This dataset is prepared for spatial interaction modeling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_data_tidy_SIM <- read_rds(\"data/rds/flow_data_tidy.rds\")\n```\n:::\n\n\n::: panel-tabset\n## Origin-Constrained Spatial Interaction Model\n\nThis chunk fits a Poisson regression model **glm** representing an Origin-Constrained Spatial Interaction Model. The formula includes various factors such as the origin grid, log-transformed counts of schools, businesses, distance to CBD, etc. The model is used to estimate the expected number of trips **TRIPS**. The summary provides coefficients and statistical information about the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# orcSIM_Poisson<- glm(formula = TRIPS ~\n#                         ORIGIN_GRID +\n#                         log(SCHOOL_COUNT) +\n#                         log(Business_COUNT) +\n#                         log(DISTANCE_TO_CBD) +\n#                         log(MIN_DISTANCE_TO_MRTLINE) +\n#                         log(MIN_DISTANCE_TO_ITH) +\n#                         log(MIN_DISTANCE_TO_PARK) +\n#                         log(FnB_COUNT)+\n#                         log(Entertn_COUNT) +\n#                         log(Leisure_COUNT) +\n#                         log(DIST),\n#                       family = poisson(link = \"log\"),\n#                       data = flow_data_tidy_SIM,\n#                       na.action = na.exclude)\n# summary(orcSIM_Poisson)\n```\n:::\n\n\nFunction **CalcRSquared** to calculate the R-squared value, a measure of how well the model fits the data. This function takes observed and estimated values as inputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCalcRSquared <- function(observed,estimated){\n  r <- cor(observed,estimated)\n  R2 <- r^2\n  R2\n}\n```\n:::\n\n\nHere, the previously defined R-squared function is applied to calculate the R-squared value for the Origin-Constrained model. The R-squared value helps assess the goodness of fit of the model to the observed data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCalcRSquared(orcSIM_Poisson$data$TRIPS, orcSIM_Poisson$fitted.values)\n```\n:::\n\n\n## Unconstrained Spatial Interaction Model\n\nThis chunk fits a Poisson regression model representing an Unconstrained Spatial Interaction Model. The formula includes various log-transformed variables. The summary provides coefficients and statistical information about the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuncSIM_Poisson <- glm(formula = TRIPS ~ \n                log(SCHOOL_COUNT) +\n                log(Business_COUNT) +\n                log(MIN_DISTANCE_TO_SCHOOL) +\n                log(DISTANCE_TO_CBD) +\n                log(MIN_DISTANCE_TO_MRTLINE) +\n                log(MIN_DISTANCE_TO_ITH) +\n                log(MIN_DISTANCE_TO_PARK) +\n                log(FnB_COUNT)+\n                log(Entertn_COUNT) +\n                log(Leisure_COUNT) + \n                log(DIST) - 1,\n              family = poisson(link = \"log\"),\n              data = flow_data_tidy_SIM,\n              na.action = na.exclude)\nsummary(uncSIM_Poisson)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = TRIPS ~ log(SCHOOL_COUNT) + log(Business_COUNT) + \n    log(MIN_DISTANCE_TO_SCHOOL) + log(DISTANCE_TO_CBD) + log(MIN_DISTANCE_TO_MRTLINE) + \n    log(MIN_DISTANCE_TO_ITH) + log(MIN_DISTANCE_TO_PARK) + log(FnB_COUNT) + \n    log(Entertn_COUNT) + log(Leisure_COUNT) + log(DIST) - 1, \n    family = poisson(link = \"log\"), data = flow_data_tidy_SIM, \n    na.action = na.exclude)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(>|z|)    \nlog(SCHOOL_COUNT)             2.013e-01  1.431e-03   140.7   <2e-16 ***\nlog(Business_COUNT)           1.951e-01  2.774e-04   703.2   <2e-16 ***\nlog(MIN_DISTANCE_TO_SCHOOL)   3.491e-02  1.726e-04   202.3   <2e-16 ***\nlog(DISTANCE_TO_CBD)          1.211e+00  1.883e-04  6431.6   <2e-16 ***\nlog(MIN_DISTANCE_TO_MRTLINE) -1.058e-01  7.256e-05 -1458.4   <2e-16 ***\nlog(MIN_DISTANCE_TO_ITH)     -1.099e-01  9.189e-05 -1196.0   <2e-16 ***\nlog(MIN_DISTANCE_TO_PARK)     1.339e-02  1.090e-04   122.8   <2e-16 ***\nlog(FnB_COUNT)                4.617e-01  3.727e-04  1238.8   <2e-16 ***\nlog(Entertn_COUNT)            3.212e-01  1.686e-03   190.5   <2e-16 ***\nlog(Leisure_COUNT)            1.078e-01  5.895e-04   182.9   <2e-16 ***\nlog(DIST)                    -6.748e-01  1.841e-04 -3664.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 304579907  on 172268  degrees of freedom\nResidual deviance:  80324515  on 172257  degrees of freedom\nAIC: 81153921\n\nNumber of Fisher Scoring iterations: 7\n```\n:::\n:::\n\n\nThe results from the Poisson regression model provide estimates for the coefficients of the log-transformed predictor variables. Interpretation-wise for example for log(SCHOOL_COUNT),\n\n**log(SCHOOL_COUNT):**\n\n-   Estimate: 0.2013\n\n-   Interpretation: A one-unit increase in the log-transformed school count is associated with an approximately 20.13% increase in the expected number of trips.\n\nRanking the factors based on their importance in predicting positive and negative impacts on the expected number of trips, we have:\n\n### Positive Factors (Higher values increase trips):\n\n1.  log(DISTANCE_TO_CBD): Approximately 121.1% increase in trips.\n2.  log(FnB_COUNT): Approximately 46.17% increase in trips.\n3.  log(Entertn_COUNT): Approximately 32.12% increase in trips.\n4.  log(Business_COUNT): Approximately 19.51% increase in trips.\n5.  log(SCHOOL_COUNT): Approximately 20.13% increase in trips.\n6.  log(MIN_DISTANCE_TO_PARK): Approximately 1.34% increase in trips.\n7.  log(MIN_DISTANCE_TO_SCHOOL): Approximately 3.49% increase in trips.\n8.  log(Leisure_COUNT): Approximately 10.78% increase in trips.\n\n### Negative Factors (Higher values decrease trips):\n\n1.  log(MIN_DISTANCE_TO_ITH): Approximately 10.99% decrease in trips.\n2.  log(MIN_DISTANCE_TO_MRTLINE): Approximately 10.58% decrease in trips.\n3.  log(DIST): Approximately 67.48% decrease in trips.\n\nThis ranking is based on the magnitude of the estimated coefficients in the Poisson regression model. Positive factors with higher coefficients have a greater impact in increasing the expected number of trips, while negative factors with higher magnitude coefficients have a greater impact in decreasing the expected number of trips.\n\nThe significance codes suggest that all coefficients are highly significant (p-value \\< 0.001), indicating that each variable has a substantial impact on the predicted number of trips in the Poisson regression model.\n\nSimilar to the Origin-Constrained model, this chunk calculates the R-squared value for the Unconstrained Spatial Interaction Model, providing an indication of how well the model fits the data. Apparently, it doesn't seem to be doing very well, with only a R-square value of 0.05594632.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCalcRSquared(uncSIM_Poisson$data$TRIPS, uncSIM_Poisson$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05594632\n```\n:::\n:::\n\n\n## Doubly Constrained Spatial Interaction Model\n\nThe code for the Doubly Constrained Spatial Interaction Model is commented out and not being used at the moment. The Doubly Constrained model considers constraints on both origins and destinations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dbcSIM_Poisson <- glm(formula = TRIPS ~\n#                         ORIGIN_GRID +\n#                         DESTIN_GRID +\n#                         log(SCHOOL_COUNT) +\n#                         log(Business_COUNT) +\n#                         log(UNITS) +\n#                         log(MIN_DISTANCE_TO_SCHOOL) +\n#                         log(DISTANCE_TO_CBD) +\n#                         log(MIN_DISTANCE_TO_MRTLINE) +\n#                         log(MIN_DISTANCE_TO_ITH) +\n#                         log(MIN_DISTANCE_TO_PARK) +\n#                         log(FnB_COUNT)+\n#                         log(Entertn_COUNT) +\n#                         log(Leisure_COUNT) +\n#                         log(DIST),\n#                       family = poisson(link = \"log\"),\n#                       data = flow_data_tidy_SIM,\n#                       na.action = na.exclude)\n# summary(dbcSIM_Poisson)\n```\n:::\n\n\nSimilar to the previous R-squared calculations, this chunk would calculate the R-squared value for the Doubly Constrained Spatial Interaction Model if the model were active.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCalcRSquared(dbcSIM_Poisson$data$TRIPS, dbcSIM_Poisson$fitted.values)\n```\n:::\n\n\n## Model comparison\n\nAnother useful model performance measure for continuous dependent variable is Root Mean Squared Error. First of all, let us create a list called model_list by using the code chunk below. Next, we will compute the RMSE of all the models in model_list file by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_list <- list(unconstrained=uncSIM_Poisson,\n                   originConstrained=orcSIM_Poisson,\n                   doublyConstrained=dbcSIM_Poisson)\n\ncompare_performance(model_list,\n                    metrics = \"RMSE\")\n```\n:::\n\n\nRMSE is calculated by taking the square root of the mean of the squared residuals. It provides a measure of the average magnitude of the residuals, giving you an idea of how well the model performs. The RMSE derived here 805.7545 is actually quite small, unfortunately we are unable to compare with other models due to some issues..\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresiduals <- uncSIM_Poisson$data$TRIPS - uncSIM_Poisson$fitted.values\n\n# Calculate RMSE\nrmse <- sqrt(mean(residuals^2))\nrmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.7545\n```\n:::\n:::\n\n\n## Visualising fitted\n\nIn this section, we visualise the observed values and the fitted values.\n\nFirstly we will extract the fitted values from each model by using the code chunk below. and will join the values to flow_data_tidy_SIM data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- as.data.frame(uncSIM_Poisson$fitted.values) %>%\n  round(digits = 0)\n\nflow_data_tidy_SIM <- flow_data_tidy_SIM %>%\n  cbind(df) %>%\n  rename(uncTRIPS = \"uncSIM_Poisson$fitted.values\")\n```\n:::\n\n\nScatterplots will be created by using **geom_point()** and other appropriate functions of ggplot2 package. This code creates a scatterplot using ggplot2, where the x-axis represents the fitted values (uncTRIPS) and the y-axis represents the observed values (TRIPS).A strong, positive correlation suggests that the model is capturing the underlying patterns in the data well.\n\nThe observations show that there could be possibly be a linear relationship and the model captures the actual results to a certain extent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = flow_data_tidy_SIM,\n                aes(x = uncTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n```\n\n::: {.cell-output-display}\n![](Analyzing_Bus_Traffic_Flows_files/figure-html/unnamed-chunk-167-1.png){width=672}\n:::\n:::\n\n\nWe can do a visualisation for all if they were actually working in the code chunk below\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# df <- as.data.frame(uncSIM_Poisson$fitted.values) %>%\n#   round(digits = 0)\n# df <- as.data.frame(orcSIM_Poisson$fitted.values) %>%\n#   round(digits = 0)\n# df <- as.data.frame(decSIM_Poisson$fitted.values) %>%\n#   round(digits = 0)\n# df <- as.data.frame(dbcSIM_Poisson$fitted.values) %>%\n#   round(digits = 0)\n# \n# SIM_data <- SIM_data %>%\n#   cbind(df) %>%\n#   rename(orcTRIPS = \"uncSIM_Poisson$fitted.values\") %>%\n#   rename(uncTRIPS = \"orcSIM_Poisson$fitted.values\") %>%\n#   rename(uncTRIPS = \"decSIM_Poisson$fitted.values\") %>%\n#   rename(uncTRIPS = \"dbcSIM_Poisson$fitted.values\")\n# \n# \n# unc_p <- ggplot(data = SIM_data,\n#                 aes(x = uncTRIPS,\n#                     y = TRIPS)) +\n#   geom_point() +\n#   geom_smooth(method = lm)\n# \n# orc_p <- ggplot(data = SIM_data,\n#                 aes(x = orcTRIPS,\n#                     y = TRIPS)) +\n#   geom_point() +\n#   geom_smooth(method = lm)\n# \n# dec_p <- ggplot(data = SIM_data,\n#                 aes(x = decTRIPS,\n#                     y = TRIPS)) +\n#   geom_point() +\n#   geom_smooth(method = lm)\n# \n# dbc_p <- ggplot(data = SIM_data,\n#                 aes(x = dbcTRIPS,\n#                     y = TRIPS)) +\n#   geom_point() +\n#   geom_smooth(method = lm)\n# \n# ggarrange(unc_p, orc_p, dec_p, dbc_p,\n#           ncol = 2,\n#           nrow = 2)\n```\n:::\n\n:::\n",
    "supporting": [
      "Analyzing_Bus_Traffic_Flows_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}