{
  "hash": "27ce72c624357de8927299c8da9d9c1f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"AutoML for Binary Classification\"\nauthor: \"Ten Wei Peng\"\ndescription: \"Automated Machine Learning (In-Progress)\"\nexecute: \n  eval: false\n  echo: true\n  warning: false\n  freeze: true\ndate: \"2024-08-02\"\nformat: html\n---\n\n![](images/Capture.PNG)\n\n## Introduction\n\nHello everyone, this post is intended to demonstrate how it is possible to automate some of the evaluation process in deciding which machine learning models to utilise to achieve the best desired results, which is commonly know as **AutoML.**\n\nI was intrigued that such a thing was possible and couldn't wait to try it, here's an attempt on my version of AutoML for a Binary Classification Model. Going forward, i would like to automate more of the common decision-making processes like pre-processing that is in tune with domain knowledge.\n\n`Changelog:`\n\n`\\[1.0.0\\] - 2024-08-02 - Initial deployment after consolidating course work and initial refinement.`\n\n`\\[1.0.1\\] - 2024-09-01 - Added Context, Logistic Regression, Fine-tuned Models, Evaluation of Results for Multivariate Analysis, Logistic Regression, Machine Learning Predictions`\n\n## Context\n\nPredicting surgical risks is crucial for enhancing patient safety and improving surgical outcomes by allowing healthcare providers to tailor care and allocate resources effectively.\n\nBy using real-world EMR data from obtained from Chan et al. (2018) \\[1\\], we aim to evaluate risk factors and develop predictive models to predict the following outcome: **mortality within 30 days**.\n\n## Methodology\n\nThe obtained data set includes 90,785 surgery patients (excluded Cardiac and Neurosurgery patients) from Singapore General Hospital from 1 January 2012 to 31 October 2016. For each outcome, we developed a respective multivariate model to evaluate the risks factors and test out different machine learning model to see which model best predict the outcome.\n\n## Importing Libraries\n\n::: {#a50b9300 .cell execution_count=1}\n``` {.python .cell-code}\n#Importing necessary libraries\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pandas.api.types import CategoricalDtype\n\nfrom sklearn.impute import SimpleImputerfrom scipy.stats import skew\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer,StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split,RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import label_binarize,LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, roc_curve, auc\n\n\nfrom imblearn.combine import SMOTEENN\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\n \n \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nwarnings.filterwarnings('ignore')\n```\n:::\n\n\n## Load the Dataset\n\nThis Python code snippet uses TensorFlow's Keras API to load the CIFAR-10 dataset and print the shapes of the training and test sets. The CIFAR-10 dataset is a popular dataset used for training machine learning and computer vision algorithms. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class.\n\n::: {#3c2574a1 .cell execution_count=2}\n``` {.python .cell-code}\nurl = 'https://raw.githubusercontent.com/weipengten/ISSS623GroupProject--Applied-Healthcare-/main/CARES_data.xlsx'\ndf = pd.read_excel(url, index_col='Indexno')\n\nprint(df.shape)\ndf.head()\n```\n:::\n\n\n![](images/clipboard-2209450693.png)\n\n## Data Cleaning\n\n-   Replace missing values with nan\n\n-   String formatting\n\n-   Create bins for age as age_bins\n\n-   Create bins for rcri_score as rcri_bin\n\n-   Drop rows with at least 10 missing values in columns, 93.73% remaining\n\n-   NUMERICAL columns (remaining columns except for DaysbetweenDeathandoperation): impute missing values with median\n\n-   CATEGORICAL columns (remaining columns except for \\['mortality', 'daysbetweendeathandoperation', '\\@30daymortality', 'thirtydaymortality'\\]): impute missing values with 'None'\n\n### Replace missing values with nan\n\n::: {#223c521c .cell execution_count=3}\n``` {.python .cell-code}\n# replace missing values with pd.NA\nnull_values = ['#NULL!', 'BLANK', 'none', 'NA', '<NA>', 'None']\ndf.replace(null_values, np.nan, inplace=True)\n\n# check data types and missing values\ndf.info()\n```\n:::\n\n\n![](images/clipboard-4006132243.png)\n\n::: {#c95b81c9 .cell execution_count=4}\n``` {.python .cell-code}\n# statistical summary\ndf.describe()\n```\n:::\n\n\n![](images/clipboard-62069690.png)\n\n### String formatting\n\n::: {#c00a64a9 .cell execution_count=5}\n``` {.python .cell-code}\n# replace all column names with lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# replace all string values in the DataFrame with lowercase\ndf = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n```\n:::\n\n\n### Replace missing values with nan\n\n::: {#e73a9667 .cell execution_count=6}\n``` {.python .cell-code}\n# replace missing values with pd.NA\nnull_values = ['#NULL!', 'BLANK', 'none', 'NA', '<NA>', 'None']\ndf.replace(null_values, np.nan, inplace=True)\n\n# check data types and missing values\ndf.info()\n```\n:::\n\n\n![](images/clipboard-263768488.png)\n\n#### Check potential duplicates\n\n::: {#dc9a26e9 .cell execution_count=7}\n``` {.python .cell-code}\n# check potential duplicates\ndf.duplicated().sum()\n\n# We will not drop duplicates in this dataste as\n# they represent multiple legitimate entries representing different patients\n```\n:::\n\n\n![](images/clipboard-1862191658.png)\n\n#### Find all unique values for categorical features\n\n::: {#3a69aef7 .cell execution_count=8}\n``` {.python .cell-code}\n# find all unique values for categorical features\nfor column in df.select_dtypes(include=['object']).columns:\n    print(f'{column}: {df[column].unique()}')\n```\n:::\n\n\n![](images/clipboard-3989827221.png)\n\n### Create bins for age as age_bins\n\n::: {#0a218e75 .cell execution_count=9}\n``` {.python .cell-code}\n# bin the age column\nage_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\nage_labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-110']\ndf['age_binned'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n\n# summarize frequency counts\nage_binned_counts = df['age_binned'].value_counts().sort_index()\nprint(age_binned_counts, '\\n')\nplt.figure(figsize=(6, 4))\nsns.barplot(x=age_binned_counts.index, y=age_binned_counts.values, palette=\"viridis\")\nplt.xlabel('Age Bins')\nplt.ylabel('Frequency')\nplt.title('Frequency of Age Bins')\nplt.xticks(rotation=45)\nplt.show()\n\ndf['age_binned'] = df['age_binned'].astype('object')\n```\n:::\n\n\n![](images/clipboard-3669474606.png)\n\n### Create bins for rcri_score as rcri_bin\n\n::: {#eef910bf .cell execution_count=10}\n``` {.python .cell-code}\n# Function to bin 'RCRI score' into distinct ordinal categories\ndef smart_binning(df, column_name, bin_column_name):\n    # Create a copy of the DataFrame to avoid modifying the original\n    _df = df.copy()\n\n    # Create a mapping for the bins, including a label for NaN values\n    bin_labels = {1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6'}\n    _df[bin_column_name] = _df[column_name].map(bin_labels)\n\n    # Handle NaN values by assigning them to a specific category\n    _df[bin_column_name] = _df[bin_column_name].fillna('No_RCRI')\n\n    # Define the categorical type with ordered categories, including 'No_RCRI'\n    cat_type = CategoricalDtype(categories=['1', '2', '3', '4', '5', '6', 'No_RCRI'], ordered=True)\n\n    # Convert the new bin column to categorical type\n    _df[bin_column_name] = _df[bin_column_name].astype(cat_type)\n\n    return _df\n\n# Apply the smart binning function\ndf = smart_binning(df, 'rcri_score', 'rcri_bin')\n\n# Check if the rcri_bin column is ordinal\nif isinstance(df['rcri_bin'].dtype, CategoricalDtype) and df['rcri_bin'].dtype.ordered:\n    print(\"rcri_bin is ordinal.\")\nelse:\n    print(\"rcri_bin is not ordinal.\")\nprint('\\n')\n\n# Print the categories and their order\nprint(\"Categories and order:\", df['rcri_bin'].dtype.categories)\nprint('\\n')\n\n# Print the distribution of data in 'rcri_bin'\nprint(f\"The distribution of data in 'rcri_bin' is as follows:\\n{df['rcri_bin'].value_counts()}\")\nprint('\\n')\n\n\ndf['rcri_bin'] = df['rcri_bin'].astype('object')\n```\n:::\n\n\n![](images/clipboard-393517675.png)\n\n#### Visualize correlation for numerical column\n\n::: {#1e4aff92 .cell execution_count=11}\n``` {.python .cell-code}\ncorrelation_matrix = df.select_dtypes(include=['number']).corr()\nfontsize=8\nplt.figure(figsize=(6,6))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, cbar=True, annot_kws={'size': 8})\nplt.title('Correlation Matrix for Numerical Columns', fontsize=10)\nplt.xticks(fontsize=fontsize)\nplt.yticks(fontsize=fontsize)\nplt.show()\n```\n:::\n\n\n![](images/clipboard-2942571460.png)\n\n### Drop rows with at least 10 missing values in columns, 93.73% remaining\n\n::: {#c8c98f3b .cell execution_count=12}\n``` {.python .cell-code}\n# define the threshold for missing values\nthreshold = 10\nmissing_counts = df.isna().sum(axis=1)\n\n# Calculate the percentage of rows to keep\npercentage_remaining = round((df[missing_counts < threshold].shape[0] / df.shape[0]) * 100, 2)\nprint(f\"Percentage of rows with fewer than {threshold} missing values: {percentage_remaining}%\")\n\n# Drop rows with at least the specified number of missing values\ndf = df[missing_counts < threshold]\ndf.head()\n```\n:::\n\n\n![](images/clipboard-2865351042.png)\n\n#### Data Validation\n\n::: {#6564b7f0 .cell execution_count=13}\n``` {.python .cell-code}\n# Check the condition\ncondition_check = df[df['thirtydaymortality'] == True]['mortality'] == True\n\n# Verify if all values meet the condition\nif condition_check.all():\n    print('----------------------------------------')\n    print(\"All instances where 'thirtydaymortality' is True, 'mortality' is also True.\")\n    print(\"Data validation step 1, there's no logical errors found for mortality and thirtydaymortality\")\n    print('----------------------------------------')\nelse:\n    print(\"There are instances where 'thirtydaymortality' is True but 'mortality' is not True.\")\n    # Data validation:\n\nif 'mortality' in df.columns and 'daysbetweendeathandoperation' in df.columns:\n    # Check the conditions\n    condition_1 = df.loc[df['mortality'] == True, 'daysbetweendeathandoperation'].notnull().all()\n    condition_2 = df.loc[df['mortality'] == False, 'daysbetweendeathandoperation'].isnull().all()\n\n    # Print the results\n    print('----------------------------------------')\n    print(\"Condition 1 met:\", condition_1)\n    print(\"Condition 2 met:\", condition_2)\n    print(\"Data validation step 2, there's no logical errors found for mortality and daysbetweendeathandoperation\")\n    print('----------------------------------------')\nelse:\n    print(\"The DataFrame does not contain the required columns.\")\n```\n:::\n\n\n![](images/clipboard-2261022608.png)\n\n#### Outlier detection for numerical features\n\n::: {#ea7ab691 .cell execution_count=14}\n``` {.python .cell-code}\nnumerical_columns = df.select_dtypes(include=['number']).columns\nprint(\"Numerical columns:\", numerical_columns)\n\nplt.figure(figsize=(16, 10))\nfor i, column in enumerate(numerical_columns, 1):\n    plt.subplot(2, 4, i)\n    sns.boxplot(y=df[column])\n    plt.title(column)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n[ ](images/clipboard-884694652.png)\n\n### NUMERICAL columns (remaining columns except for DaysbetweenDeathandoperation): impute missing values with median)\n\n::: {#88f9d5e3 .cell execution_count=15}\n``` {.python .cell-code}\n# Initialize SimpleImputer to fill missing values with the median\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\n\n# List all numeric columns in the DataFrame\nnumeric_columns = df.select_dtypes(include=['number']).columns\nprint(\"Numeric columns:\", list(numeric_columns))  # we have also checked that all the numerical columns are correct, no binary variables are mistakenly treated as numeric variables.\n\n# Define specific numeric columns for imputation\nnumeric_cols = [col for col in numeric_columns if col != 'daysbetweendeathandoperation']\nprint(\"Numeric columns:\", list(numeric_cols))\n\n# Fit the imputer on the specified numeric columns\nimp.fit(df[numeric_cols])\n\n# Transform the specified numeric columns by imputing missing values with the median\ndf[numeric_cols] = imp.transform(df[numeric_cols])\n\n# Ensure the columns are of float32 type\ndf[numeric_cols] = df[numeric_cols].astype(np.float32)\n\n# Print the DataFrame with imputed data to verify the changes\nprint(\"DataFrame with imputed numeric columns:\")\nprint(df[numeric_cols])\n```\n:::\n\n\n![](images/clipboard-3764274460.png)\n\n### CATEGORICAL columns (remaining columns except for ['mortality', 'daysbetweendeathandoperation', ' @30daymortality', 'thirtydaymortality']): impute missing values with 'None'\n\n::: {#fd525c40 .cell execution_count=16}\n``` {.python .cell-code}\n# Initialize SimpleImputer for categorical columns to fill missing values with 'None'\ncat_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='None')\n\n# List all categorical columns in the DataFrame\ncategorical_columns  = df.select_dtypes(include=['object']).columns\nprint(\"Categorical columns:\", list(categorical_columns))\n\n# Columns to exclude\nexclude_columns = ['mortality', 'daysbetweendeathandoperation', '@30daymortality', 'thirtydaymortality']\n\n# Filter out the excluded columns from the list of categorical columns\ncategorical_columns = [col for col in categorical_columns if col not in exclude_columns]\n\n# Fit the imputer on the categorical columns\ncat_imp.fit(df[categorical_columns])\n\n# Transform the categorical columns by imputing missing values with 'None'\ndf[categorical_columns] = cat_imp.transform(df[categorical_columns])\n\n# find all unique values for categorical features\nfor column in df.select_dtypes(include=['object']).columns:\n    print(f'{column}: {df[column].unique()}')\n\n```\n:::\n\n\n[ ](images/clipboard-692645940.png)Outcome 1: Mortality within 30 days (thirtydaymortality)\n\n### 1. Frame the Problem\n\n-   Target variable: thirtydaymortality\n\n-   Type: Classification Problem\n\n-   Binary Outcome: (No,Yes)\n\n### 2. Data sensing, preprocessing:\n\n-   `Feature Selection:` Exclusion of variables that deomonstrates high redundancy or Multicollinearity with thirtydaymortality:\n\n    -   \\@30daymortality\n\n    -   mortality\n\n    -   daysbetweendeathandoperation\n\n::: {#2ba4645d .cell execution_count=17}\n``` {.python .cell-code}\n# Feature Selection:\n# drop unecessary columns, features that occur only after the target result should be excluded\nfeatures_to_exclude = ['mortality', 'daysbetweendeathandoperation', '@30daymortality']\ndf = df.drop(columns=features_to_exclude)\n```\n:::\n\n\n### 3. Data Wrangling, Transformation:\n\nNumerical features Transformation:\n\n-   Test for normality of numeric columns: ALL are not normally distributed\n\n-   DIstribution transfomation for columns\n\n-   Distribution normalisation and standardisation for columns\n\nCateogorical features Transformation:\n\n-   One-hot encoding to ensure smooth usage by tree learning algorithms\n\n#### Numerical features Transformation\n\n##### Test for Normality\n\n::: {#fb9ba331 .cell execution_count=18}\n``` {.python .cell-code}\n# Test for normality of numeric columns: ALL are not normally distributed\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Define specific numeric columns for imputation\nnumeric_cols = [col for col in numeric_columns if col != 'daysbetweendeathandoperation']\nprint(\"Numeric columns:\", list(numeric_cols))\n\n# Function to test for normal distribution\ndef test_normal_distribution(data, alpha=0.05):\n    \"\"\"\n    Test if the data follows a normal distribution.\n    \"\"\"\n    shapiro_stat, shapiro_p = stats.shapiro(data)\n    ks_stat, ks_p = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data, ddof=0)))\n\n    print(f\"Shapiro-Wilk: Statistic={shapiro_stat:.3f}, p-value={shapiro_p:.3f}\")\n    print(f\"Kolmogorov-Smirnov: Statistic={ks_stat:.3f}, p-value={ks_p:.3f}\")\n    print(\"Normally distributed:\" if shapiro_p > alpha and ks_p > alpha else \"Not normally distributed.\")\n    print()\n\n# Test normal distribution\nprint(\"Testing Normal Distribution:\")\nfor col in numeric_cols:\n    print(f\"{col}:\")\n    test_normal_distribution(df[col])\n\ndef create_qq_plots(columns):\n    # Number of columns to plot\n    num_cols = len(columns)\n\n    # Determine grid size for subplots\n    n_cols = 3  # Number of columns in the subplot grid\n    n_rows = (num_cols + n_cols - 1) // n_cols  # Calculate number of rows needed\n\n    # Create subplots\n    plt.figure(figsize=(15, 5 * n_rows))  # Adjust figure size as needed\n\n    for i, col in enumerate(columns, 1):\n        plt.subplot(n_rows, n_cols, i)\n        stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)\n        plt.title(f'Q-Q Plot of {col}')\n        plt.xlabel('Theoretical Quantiles')\n        plt.ylabel('Sample Quantiles')\n\n    plt.tight_layout()\n    plt.show()\n\n# Create Q-Q plots for numeric columns\ncreate_qq_plots(numeric_cols)\n```\n:::\n\n\n![](images/clipboard-1338966806.png)\n\n![](images/clipboard-1362939341.png)\n\n##### Distribution of numeric columns\n\n::: {#8e82d8fd .cell execution_count=19}\n``` {.python .cell-code}\ndef create_histograms(df,columns):\n    # Number of columns to plot\n    num_cols = len(columns)\n\n    # Determine grid size for subplots\n    n_cols = 3  # Number of columns in the subplot grid\n    n_rows = (num_cols + n_cols - 1) // n_cols  # Calculate number of rows needed\n\n    # Create subplots\n    plt.figure(figsize=(15, 5 * n_rows))  # Adjust figure size as needed\n\n    for i, col in enumerate(columns, 1):\n        plt.subplot(n_rows, n_cols, i)\n        sns.histplot(df[col].dropna(), kde=True, bins=30)  # Adjust bins as needed\n        plt.title(f'Distribution of {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\n\n# Create histograms for numeric columns\ncreate_histograms(df,numeric_cols)\n```\n:::\n\n\n![](images/clipboard-4289095336.png)\n\n##### Automation of transforming numeric columns based on skewness\n\n::: {#055e321c .cell execution_count=20}\n``` {.python .cell-code}\ndef calculate_skewness(df,columns):\n    # Dictionary to store skewness values\n    skewness_dict = {}\n\n    for col in columns:\n        # Drop NaN values for skewness calculation, there shouldn't be any left\n        data = df[col].dropna()\n\n        # Calculate skewness\n        skew_value = skew(data, nan_policy='omit')  # nan_policy='omit' ignores NaN values\n        skewness_dict[col] = skew_value\n\n    # Convert skewness dictionary to DataFrame for better readability\n    skewness_df = pd.DataFrame(list(skewness_dict.items()), columns=['Column', 'Skewness'])\n    return skewness_df\n\n\n# Calculate skewness for numeric columns\nskewness_df = calculate_skewness(df,numeric_cols)\n\n# Print skewness values\nprint(\"Skewness of Numeric Columns:\")\nprint(skewness_df)\n\n\n\ndef transform_based_on_skewness(df, columns):\n    transformed_df = df.copy()\n    new_columns_numeric = []\n    column_mapping = {}\n\n    for col in columns:\n        # Calculate skewness\n        skewness = skew(df[col].dropna(), nan_policy='omit')\n\n        # Initialize new_col to None\n        new_col = None\n\n        # Print the skewness for debugging\n        print(f\"Skewness for column {col}: {skewness}\")\n\n        # Choose transformation based on skewness\n        if skewness > 1:\n            # Highly positively skewed\n            if (transformed_df[col] > 0).all():  # Check if all values are positive\n                new_col = col + '_log'\n                transformed_df[new_col] = np.log1p(df[col])\n                print(f\"Applied log transformation on column {col}\")\n            else:\n                # Use Yeo-Johnson if data contains zero or negative values\n                new_col = col + '_yeojohnson'\n                pt = PowerTransformer(method='yeo-johnson')\n                transformed_df[new_col] = pt.fit_transform(df[[col]])\n                print(f\"Applied Yeo-Johnson transformation on column {col}\")\n\n        elif skewness > 0.5:\n            # Moderately positively skewed\n            new_col = col + '_sqrt'\n            transformed_df[new_col] = np.sqrt(df[col] + 1)  # Adding 1 to handle zero values\n            print(f\"Applied square root transformation on column {col}\")\n\n        elif skewness < -1:\n            # Highly negatively skewed\n            new_col = col + '_inv'\n            transformed_df[new_col] = 1 / (df[col] + 1)  # Adding 1 to avoid division by zero\n            print(f\"Applied inverse transformation on column {col}\")\n\n        elif skewness < -0.5:\n            # Moderately negatively skewed\n            new_col = col + '_inv_sqrt'\n            transformed_df[new_col] = 1 / np.sqrt(df[col] + 1)  # Adding 1 to handle zero values\n            print(f\"Applied inverse square root transformation on column {col}\")\n\n        else:\n            # Data is close to normal, no transformation needed\n            new_col = col + '_no_transform'\n            transformed_df[new_col] = df[col]\n\n       \n        if new_col:\n            new_columns_numeric.append(new_col)\n            column_mapping[col] = new_col\n\n    return transformed_df, new_columns_numeric, column_mapping\n\n# Example usage\n# Assuming df is your DataFrame\nnumeric_cols = ['age', 'rcri_score', 'preopegfrmdrd', 'preoptransfusionwithin30days',\n                'intraop', 'postopwithin30days', 'transfusionintraandpostop']\n\n# Apply transformations based on skewness\ntransformed_df, new_columns_numeric, column_mapping = transform_based_on_skewness(df, numeric_cols)\n\n\n# Print the first few rows of the transformed DataFrame to verify\nprint(transformed_df.head())\n# Print the list of newly transformed columns\nprint(new_columns_numeric)\n\n# Verify the columns are indeed in the DataFrame\nfor col in new_columns_numeric:\n    if col in transformed_df.columns:\n        print(f\"Column {col} exists in the DataFrame.\")\n    else:\n        print(f\"Column {col} does NOT exist in the DataFrame.\")\n\n\n\n# Create histograms for newly transformed columns\ncreate_histograms(transformed_df,new_columns_numeric)\n\n```\n:::\n\n\n![](images/clipboard-1165382113.png)\n\n![](images/clipboard-2447623812.png)\n\n![](images/clipboard-917996487.png)\n\n##### Comparison of newly transformed variables as compared to orignal in terms of skewness\n\n::: {#392ce6c1 .cell execution_count=21}\n``` {.python .cell-code}\n# Calculate skewness for original columns\nskewness_original = calculate_skewness(df,numeric_cols).set_index('Column')['Skewness'].to_dict()\n\n# Calculate skewness for transformed columns\nskewness_transformed = calculate_skewness(transformed_df,new_columns_numeric).set_index('Column')['Skewness'].to_dict()\n\n# Print column mapping\nprint(\"Column Mapping:\")\nprint(column_mapping)\n\n\n# Print skewness values\nprint(\"Skewness of Numeric Columns:\")\nprint(skewness_df)\nprint(skewness_transformed)\n\n# Print column mapping\nprint (column_mapping)\n\n# Create a comparison DataFrame\ncomparison_df = pd.DataFrame({\n    'Original Column': numeric_cols,\n    'Original Skewness': [skewness_original[col] for col in numeric_cols],\n    'Transformed Column': [column_mapping[col] if col in column_mapping else col + '_no_transform' for col in numeric_cols],\n    'Transformed Skewness': [skewness_transformed[column_mapping[col]] if col in column_mapping else skewness_original[col] for col in numeric_cols]\n})\n\n# Add a column to indicate if skewness has decreased\ncomparison_df['Skewness Decreased'] = abs(comparison_df['Original Skewness']) > abs(comparison_df['Transformed Skewness'])\n\nprint(\"Skewness Comparison:\")\ncomparison_df\n```\n:::\n\n\n![](images/clipboard-2181277663.png)\n\n##### Standardize all newly transformed columns\n\n::: {#fdb668cb .cell execution_count=22}\n``` {.python .cell-code}\nstd_scaler = StandardScaler()\ntransformed_df[new_columns_numeric] = std_scaler.fit_transform(transformed_df[new_columns_numeric])\n```\n:::\n\n\n#### Categorical features processing: One- hot encoding all features\n\n::: {#9fc92af8 .cell execution_count=23}\n``` {.python .cell-code}\n# List all categorical columns in the DataFrame\ncategorical_columns  = transformed_df.select_dtypes(include=['object']).columns\n\n# Columns to exclude (make sure names match exactly)\nexclude_columns = ['mortality', 'daysbetweendeathandoperation', '@30daymortality', 'thirtydaymortality']\n\n# Filter out the excluded columns from the list of categorical columns\ncategorical_columns = [col for col in categorical_columns if col not in exclude_columns]\n\n# One-hot encode categorical columns\none_hot_ed = pd.get_dummies(transformed_df[categorical_columns],drop_first=True)\n\n# Concatenate the one-hot encoded columns with the original DataFrame\ntransformed_df = pd.concat([transformed_df, one_hot_ed], axis=1)\n```\n:::\n\n\n#### Final Cleaning and Consolidation of Processed Numerical and Categorical columns\n\n::: {#6ae052df .cell execution_count=24}\n``` {.python .cell-code}\n# Drop the original categorical columns\ntransformed_df.drop(categorical_columns, axis=1, inplace=True)\n# Drop the original numeric columns\ntransformed_df.drop(numeric_cols, axis=1, inplace=True)\n```\n:::\n\n\n::: {#9a8bfb9b .cell execution_count=25}\n``` {.python .cell-code}\ntransformed_df.info()\n```\n:::\n\n\n![](images/clipboard-3862374559.png)![](images/clipboard-3784664671.png)\n\n### 4. In-depth analysis: Staistical modelling / Machine Learning\n\n#### Multivariate Analysis\n\n::: {#8eba7e37 .cell execution_count=26}\n``` {.python .cell-code}\nclass MultivariateAnalysis:\n    def __init__(self, transformed_df, target_variable, vif_threshold=10, corr_threshold=0.8):\n        self.df = transformed_df\n        self.target_variable = target_variable\n        self.vif_threshold = vif_threshold\n        self.corr_threshold = corr_threshold\n\n        # Drop target variable for analysis\n        self.features_df = self.df.drop(columns=[self.target_variable])\n\n    def preprocess_data(self):\n        # Convert Boolean columns to numeric\n        bool_cols = self.features_df.select_dtypes(include=['bool']).columns\n        self.features_df[bool_cols] = self.features_df[bool_cols].astype(int)\n\n    def plot_correlation_heatmap(self, high_corr_vars):\n        plt.figure(figsize=(20, 20))\n        corr = self.features_df[high_corr_vars].corr()\n        ax = sns.heatmap(corr, annot=True, fmt='.2g', vmin=-1, vmax=1, center=0,\n                         cmap='coolwarm_r', linecolor='black', linewidth=1, annot_kws={\"size\": 12})\n        ax.set_ylim(len(corr), 0)\n        plt.xticks(rotation=45, ha='right')\n        plt.title('Correlation Heatmap of High Correlation Variables')\n        plt.show()\n\n    def calculate_vif(self, df):\n        # Ensure only numeric columns are used\n        numeric_df = df.select_dtypes(include=['float64', 'int64'])\n        if numeric_df.empty:\n            raise ValueError(\"DataFrame contains no numeric columns.\")\n\n        # Add constant to the feature set for VIF calculation\n        numeric_df_with_const = add_constant(numeric_df, has_constant='add')\n        vif_data = pd.DataFrame()\n        vif_data[\"Variable\"] = numeric_df_with_const.columns\n        vif_data[\"VIF\"] = [variance_inflation_factor(numeric_df_with_const.values, i)\n                           for i in range(numeric_df_with_const.shape[1])]\n        return vif_data\n\n    def plot_vif(self, vif_data):\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x=\"VIF\", y=\"Variable\", data=vif_data.sort_values(\"VIF\", ascending=False))\n        plt.title('Variance Inflation Factor (VIF)')\n        plt.show()\n\n    def identify_high_corr_pairs(self):\n        corr_matrix = self.features_df.corr().abs()\n        high_corr_pairs = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n                           .stack()\n                           .reset_index()\n                           .rename(columns={0: 'correlation', 'level_0': 'feature1', 'level_1': 'feature2'}))\n        high_corr_pairs = high_corr_pairs[high_corr_pairs['correlation'] > self.corr_threshold]\n        return high_corr_pairs\n\n    def drop_high_corr_vars(self, high_corr_pairs):\n        to_drop = set()\n        for _, row in high_corr_pairs.iterrows():\n            if row['feature1'] in to_drop or row['feature2'] in to_drop:\n                continue\n            feature1_corr_sum = self.features_df.corr()[row['feature1']].abs().sum()\n            feature2_corr_sum = self.features_df.corr()[row['feature2']].abs().sum()\n            if feature1_corr_sum > feature2_corr_sum:\n                to_drop.add(row['feature1'])\n            else:\n                to_drop.add(row['feature2'])\n        self.features_df.drop(columns=to_drop, inplace=True)\n        return to_drop\n\n    def run_analysis(self):\n        # Preprocess data to include Boolean columns as numeric\n        self.preprocess_data()\n\n        # Identify and drop high correlation variables\n        high_corr_pairs = self.identify_high_corr_pairs()\n        dropped_corr_vars = self.drop_high_corr_vars(high_corr_pairs)\n\n        # Plot correlation heatmap for remaining variables\n        if not high_corr_pairs.empty:\n            high_corr_vars = list(set(high_corr_pairs['feature1']).union(set(high_corr_pairs['feature2'])))\n            high_corr_vars = [var for var in high_corr_vars if var in self.features_df.columns]\n            self.plot_correlation_heatmap(high_corr_vars)\n\n        # Calculate and plot VIF\n        vif_data = self.calculate_vif(self.features_df)\n        print(\"Original VIF Data:\")\n        print(vif_data)\n\n        # Variables to keep for VIF reporting\n        high_vif_vars = vif_data[vif_data[\"VIF\"] > self.vif_threshold]\n\n        # Print table for high VIF variables\n        if not high_vif_vars.empty:\n            print(\"\\nVariables with high VIF:\")\n            print(high_vif_vars)\n        else:\n            print(\"\\nNo variables exceed the VIF threshold.\")\n\n        # Drop variables with high VIF\n        reduced_df = self.features_df.drop(columns=high_vif_vars['Variable'], errors='ignore')\n\n        # Recalculate VIF on reduced dataset\n        reduced_vif_data = self.calculate_vif(reduced_df)\n        print(\"\\nNew VIF Data after dropping high VIF variables:\")\n        print(reduced_vif_data)\n\n        # Plot new VIF\n        self.plot_vif(reduced_vif_data)\n\n        # Add target variable back to reduced_df\n        reduced_df[self.target_variable] = self.df[self.target_variable]\n\n        return reduced_df, dropped_corr_vars, high_vif_vars, reduced_vif_data\n\n# Example usage\n# transformed_df is your DataFrame and 'target' is your target variable\nma = MultivariateAnalysis(transformed_df, target_variable='thirtydaymortality', corr_threshold=0.6)\nreduced_df, dropped_corr_vars, high_vif_vars, new_vif = ma.run_analysis()\n```\n:::\n\n\n![](images/clipboard-3373007464.png)\n\n![](images/clipboard-1854938830.png)\n\n::: {.notebox .lightbulb data-latex=\"lightbulb\"}\nThe `MultivariateAnalysis` class provides a comprehensive approach to handling multicollinearity in a dataset by leveraging Variance Inflation Factor (VIF) and correlation thresholds. This document summarizes the methods employed, presents the results, and evaluates the effectiveness of the analysis.\n\n**Methods Used**\n\n1.  **Data Preprocessing**\n\n-   *Objective:* Convert Boolean columns to numeric values.\n\n-   *Implementation:* Boolean columns are cast to integers to ensure compatibility with further analyses.\n\n2.  **Identifying High Correlation Pairs**\n\n-   *Objective:* Detect and address pairs of features with high correlation to mitigate multicollinearity.\n\n-   *Implementation:*\n\n    -   Compute the correlation matrix.\n\n    -   Identify pairs with correlation values exceeding the specified threshold.\n\n    -   Drop one feature from each high-correlation pair based on the overall sum of correlations.\n\n3.  **Variance Inflation Factor (VIF) Calculation**\n\n-   *Objective:* Assess multicollinearity for each feature.\n\n-   *Implementation:*\n\n    -   Compute VIF for all numeric features.\n\n    -   Features with VIF values above the specified threshold are flagged as problematic.\n\n4.  **Evaluation**\n\n-   By identifying and removing highly correlated variables and variables with high VIF, we improved the multicollinearity in our dataset. This process helps in creating more robust and interpretable models.\n\n-   The reduced dataset can now be used for further modeling and analysis with minimized multicollinearity issues.\n:::\n\n#### Estimate the Logistic Regression Model:\n\n::: {#a2ea6d36 .cell execution_count=27}\n``` {.python .cell-code}\n# Assuming 'transformed_df' is your DataFrame and 'thirtydaymortality' is your target variable\nX = reduced_df.drop(columns=['thirtydaymortality'])\ny = reduced_df['thirtydaymortality']\n\n# Convert data to float\nX = X.astype(float)\ny = LabelEncoder().fit_transform(y)\n\n# Apply SMOTE to balance the dataset\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Perform Grid Search with Logistic Regression using scikit-learn\nlogreg = LogisticRegression(solver='liblinear')\nparams = {'C': [1, 0.1, 0.01], 'penalty': ['l1', 'l2']}\ngs_logreg = GridSearchCV(logreg, param_grid=params, cv=5, scoring='roc_auc')\ngs_logreg.fit(X_resampled, y_resampled)\n\n# Display best parameters and score\nprint(\"\\nBest estimator average accuracy on train set: {:.4f}\".format(gs_logreg.best_score_))\nprint(\"Best C = {}\".format(gs_logreg.best_params_))\n\n# Predict on the test set\nX_test = X  # Assuming you're using the same data for demonstration purposes\ny_test = y\ny_pred = gs_logreg.predict(X_test)\ny_pred_proba = gs_logreg.predict_proba(X_test)[:, 1]\n\n# Print accuracy and classification report\nprint(\"\\nAccuracy on test set: {:.4f}\".format(gs_logreg.best_estimator_.score(X_test, y_test)))\nprint(\"Test set classification report:\\n\", classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred, labels=gs_logreg.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gs_logreg.classes_)\ndisp.plot()\nplt.title('Confusion Matrix')\nplt.show()\n\n# ROC Curve\nRocCurveDisplay.from_estimator(gs_logreg, X_test, y_test)\nplt.title('ROC Curve')\nplt.show()\n\n# Generate statistical report with p-values using statsmodels\nX_resampled_with_const = sm.add_constant(X_resampled)  # Add constant for intercept\nlogit_model_resampled = sm.Logit(y_resampled, X_resampled_with_const)\nresult_resampled = logit_model_resampled.fit_regularized()\n\n# Display the summary of the logistic regression model\nsummary_resampled = result_resampled.summary()\nprint(summary_resampled)\n\n# Set the maximum number of rows and columns to display\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# Print coefficients with p-values from the statsmodels summary\ncoef_df = pd.DataFrame({\n    'Feature': ['Intercept'] + list(X.columns),\n    'Coefficient': result_resampled.params,\n    'P-Value': result_resampled.pvalues\n})\n```\n:::\n\n\n![](images/clipboard-2592243296.png)\n\n![](images/clipboard-3249462092.png){width=\"387\"}\n\n![](images/clipboard-96420571.png){width=\"362\"}\n\n![](images/clipboard-1055715652.png){width=\"545\"}\n\n![](images/clipboard-2022614774.png){width=\"546\"}\n\n::: {.notebox .lightbulb data-latex=\"lightbulb\"}\n**Discussion on Results** - *Pseudo R-squared (0.5986):* Indicates that the model explains approximately 60% of the variability in the dependent variable, which suggests a good fit.\n\n-   *Log-Likelihood (-47051):* Measures how well the model fits the data. A higher (less negative) value would indicate a better fit.\n\n-   *Coefficients :*\n\n    -   *age_no_transform (1.3001):* For each unit increase in age, the log-odds of the event occurring increase by 1.3001. This translates to an increased probability of the event with age. Converting this to odds: an increase in age by one unit multiplies the odds of the event by exp(1.3001) ≈ 3.68. Hence, older individuals have significantly higher odds of the event occurring.\n\n    -   *gender_male (0.4632):* Being male increases the log-odds of the event by 0.4632. In terms of odds: exp(0.4632) ≈ 1.588. Thus, being male increases the odds of the event occurring by approximately 58.8%.\n\n-   *Significance (P\\>\\|z\\|):* Most coefficients have p-values of 0.000, indicating that these predictors are statistically significant. Coefficients with higher p-values (e.g., agecategory_30-49 (0.091)) are less significant, meaning they may not be as impactful in predicting the outcome.\n:::\n\n#### Machine Learning (Individual Models)\n\nnot needed here, we will proceed straight to AutoML with the models\n\n### 5. Comparison and Evaluation: Automate the evaluation process\n\nAutomate the model training and evaluation process, and generate comprehensive results and key evaluation reports to assess the validity and effectiveness of the chosen model.\n\n::: {#3b18edf6 .cell execution_count=28}\n``` {.python .cell-code}\nclass BaseModel:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123):\n        self.df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n\n        # Prepare features and target variable\n        self.X = self.df.drop(columns=[self.target_column])\n        self.y = self.df[self.target_column]\n\n        \n        # Label encode the target variable\n        label_encoder = LabelEncoder()\n        self.y = label_encoder.fit_transform(self.y)\n\n\n        # Split the data into training and test sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, test_size=0.3, random_state=self.random_state\n        )\n\n        # Initialize SMOTEENN if balance is set to 'imbalanced'\n        if self.balance == 'imbalanced':\n            self.smoteenn = SMOTEENN(sampling_strategy='auto', random_state=self.random_state)\n        else:\n            self.smoteenn = None\n\n    def apply_smoteenn(self):\n        if self.smoteenn:\n            return self.smoteenn.fit_resample(self.X_train, self.y_train)\n        else:\n            return self.X_train, self.y_train\n\n    def show_matrix(self, matrix, title='Confusion Matrix'):\n        # Plotting the confusion matrix\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n        plt.title(title)\n        plt.ylabel('Actual Class')\n        plt.xlabel('Predicted Class')\n        plt.show()\n\n    def plot_roc_auc(self, y_true, y_pred, title):\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\nclass DT(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = DecisionTreeClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Decision Tree...\")\n        # Define the parameter space\n        search_spaces = {\n            'max_features': Categorical(['sqrt', 'log2']),\n            'ccp_alpha': Real(0.001, 0.1, prior='log-uniform'),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass RandomForest(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = RandomForestClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Random Forest...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_features': Categorical(['sqrt', 'log2']),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass XGBoost(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for XGBoost...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass LightGBM(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = LGBMClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for LightGBM...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass AutoML:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, secondary_metric='precision'):\n        self.cleaned_df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.secondary_metric = secondary_metric\n        \n        # Initialize model classes\n        self.models = {\n            'DecisionTree': DT(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'RandomForest': RandomForest(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'XGBoost': XGBoost(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'LightGBM': LightGBM(cleaned_df, target_column, balance, n_splits, n_repeats, random_state)\n        }\n\n    def evaluate_models(self):\n        results = {}\n        for name, model in self.models.items():\n            print(f'\\nEvaluating {name}...')\n            best_model, y_pred = model.run_bayesian_search()\n            \n            # Compute metrics\n            accuracy = accuracy_score(model.y_test, y_pred)\n            precision = precision_score(model.y_test, y_pred, average='weighted')\n            recall = recall_score(model.y_test, y_pred, average='weighted')\n            f1 = 2 * (precision * recall) / (precision + recall)\n            \n            results[name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'classification_report': classification_report(model.y_test, y_pred),\n                'confusion_matrix': model.show_matrix(confusion_matrix(model.y_test, y_pred))\n            }\n            \n            # Print Classification Report and Confusion Matrix\n            print(f'Classification Report for {name}:\\n{results[name][\"classification_report\"]}')\n            \n            # ROC and AUC\n            y_test_binarized = label_binarize(model.y_test, classes=np.unique(model.y_test))\n            y_pred_binarized = label_binarize(y_pred, classes=np.unique(model.y_test))\n            if y_test_binarized.shape[1] > 1:\n                for i in range(y_test_binarized.shape[1]):\n                    model.plot_roc_auc(y_test_binarized[:, i], y_pred_binarized[:, i], f'{name} ROC Curve for Class {i}')\n            \n        # Determine the best model\n        sorted_results = sorted(results.items(), key=lambda x: (x[1]['accuracy'], x[1][self.secondary_metric]), reverse=True)\n        best_model_name, best_model_metrics = sorted_results[0]\n        print(f'\\nBest Model: {best_model_name}')\n        print(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\n        print(f'{self.secondary_metric.capitalize()}: {best_model_metrics[self.secondary_metric]}')\n        \n        return best_model_name, best_model_metrics\n\n# Example usage\n# Assuming `transformed_df` is your DataFrame and 'thirtydaymortality' is your target column\nauto_ml = AutoML(reduced_df, target_column='thirtydaymortality', balance='imbalanced', secondary_metric='recall')\nbest_model_name, best_model_metrics = auto_ml.evaluate_models()\n\nprint(f'\\nBest Model: {best_model_name}')\nprint(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\nprint(f'Precision: {best_model_metrics[\"precision\"]}')\nprint(f'Recall: {best_model_metrics[\"recall\"]}')\nprint(f'F1 Score: {best_model_metrics[\"f1\"]}')\nprint(f'Confusion Matrix:\\n{best_model_metrics[\"confusion_matrix\"]}')\n```\n:::\n\n\nEvaluating DecisionTree...\n\nTime taken = 38.33 sec\n\nBest score = 0.882 using params: OrderedDict(\\[('ccp_alpha', 0.001131086677421503), ('criterion', 'entropy'), ('max_depth', 24), ('max_features', 'log2'), ('min_samples_leaf', 3), ('min_samples_split', 3)\\])\\\n**Test accuracy = [0.85]{.underline}**\n\n![](images/clipboard-59382711.png)\n\n![](images/clipboard-3531112163.png)\n\nEvaluating RandomForest...\n\nTime taken = 891.38 sec Best score = 0.97 using params: OrderedDict(\\[('criterion', 'entropy'), ('max_depth', 27), ('max_features', 'sqrt'), ('min_samples_leaf', 2), ('min_samples_split', 3), ('n_estimators', 82)\\]) **Test accuracy = [0.956]{.underline}**\n\n![](images/clipboard-2689837012.png)\n\n![](images/clipboard-925563829.png)\n\nEvaluating XGBoost\n\nTime taken = 487.2 sec Best score = 0.973 using params: OrderedDict(\\[('colsample_bytree', 0.8636816590466002), ('learning_rate', 0.0645354435221669), ('max_depth', 29), ('n_estimators', 108), ('subsample', 0.9220560088509978)\\]) **Test accuracy = 0.952**\n\n![](images/clipboard-388346725.png)\n\n![](images/clipboard-1451616604.png)\n\nEvaluating LightGBM...\n\nTime taken = 230.12 sec Best score = 0.958 using params: OrderedDict(\\[('colsample_bytree', 0.5133739717626367), ('learning_rate', 0.1941545477346783), ('max_depth', 24), ('n_estimators', 131), ('subsample', 0.8487299825534435)\\]) **Test accuracy = 0.941**\n\n![![](images/clipboard-433414607.png)](images/clipboard-2732899064.png)\n\n[**Best Model: RandomForest**]{.underline}\n\nAccuracy: 0.9561266060795989\n\nRecall: 0.9561266060795989\n\n[**Best Model: RandomForest**]{.underline}\n\nAccuracy: 0.9561266060795989\n\nPrecision: 0.9905793259715808\n\nRecall: 0.9561266060795989\n\nF1 Score: 0.9730480946301695\n\n### 6. Tuning Model to industry and domain knowledge: avoiding false negatives\n\nIn cancer diagnostics, **avoiding false negatives is particularly crucial**. Missing a cancer diagnosis can lead to delayed treatment, disease progression, and reduced survival rates, which are far more serious than the consequences of false positives, such as psychological distress, additional testing, or unnecessary treatment. Consequently, cancer screening programs tend to prioritize AUC/sensitivity to minimize the number of missed cases, even if it results in a higher rate of false positives.\n\nIn this context, we fit and test the models similarly to the previous approach, but we specifically evaluate and **select the best model based on sensitivity**.\n\n::: {#a550a0c4 .cell execution_count=29}\n``` {.python .cell-code}\nclass BaseModel:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, evaluation_metric='recall'):\n        self.df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.evaluation_metric = evaluation_metric\n\n        # Prepare features and target variable\n        self.X = self.df.drop(columns=[self.target_column])\n        self.y = self.df[self.target_column]\n\n        # Label encode the target variable\n        label_encoder = LabelEncoder()\n        self.y = label_encoder.fit_transform(self.y)\n\n        # Split the data into training and test sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, test_size=0.3, random_state=self.random_state\n        )\n\n        # Initialize SMOTEENN if balance is set to 'imbalanced'\n        if self.balance == 'imbalanced':\n            self.smoteenn = SMOTEENN(sampling_strategy='auto', random_state=self.random_state)\n        else:\n            self.smoteenn = None\n\n    def apply_smoteenn(self):\n        if self.smoteenn:\n            return self.smoteenn.fit_resample(self.X_train, self.y_train)\n        else:\n            return self.X_train, self.y_train\n\n    def show_matrix(self, matrix, title='Confusion Matrix'):\n        # Plotting the confusion matrix\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n        plt.title(title)\n        plt.ylabel('Actual Class')\n        plt.xlabel('Predicted Class')\n        plt.show()\n\n    def plot_roc_auc(self, y_true, y_pred, title):\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.legend(loc=\"lower right\")\n        plt.show()\n        return roc_auc\n\n\n    def print_feature_importance(self, model_name):\n      if hasattr(self.clf, 'feature_importances_'):\n        importance = self.clf.feature_importances_\n        feature_names = self.X.columns\n\n        # Create a DataFrame for feature importances and sort it\n        feature_importance_df = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': importance\n        })\n        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n        # Plot feature importances\n        plt.figure(figsize=(10, 6))\n        plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n        plt.xlabel('Feature Importance Score')\n        plt.title(f'Feature Importance for {model_name}')\n        plt.show()\n\n    def print_tree_structure(self):\n      if isinstance(self.clf, tree.DecisionTreeClassifier):\n        # Plot the tree structure\n        plt.figure(figsize=(20, 10))\n        plot_tree(self.clf, feature_names=self.X.columns, filled=True, rounded=True)\n        plt.title(\"Decision Tree Visualization\")\n        plt.show()\n\nclass DT(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = tree.DecisionTreeClassifier(class_weight='balanced')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Decision Tree...\")\n        # Define the parameter space\n        search_spaces = {\n            'max_features': Categorical(['sqrt', 'log2']),\n            'ccp_alpha': Real(0.001, 0.1, prior='log-uniform'),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n\n      # Ensure the model is fitted before calling methods\n        if hasattr(self.clf, 'feature_importances_') or isinstance(self.clf, tree.DecisionTreeClassifier):\n            self.print_tree_structure()\n            self.print_feature_importance(\"Decision Tree\")\n\n        return bayes_result, y_pred\n\nclass RandomForest(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = RandomForestClassifier(class_weight='balanced')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Random Forest...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_features': Categorical(['sqrt', 'log2']),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        self.print_feature_importance(\"Random Forest\")\n        return bayes_result, y_pred\n\nclass XGBoost(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scale_pos_weight = len(self.y_train[self.y_train == 0]) / len(self.y_train[self.y_train == 1])\n        self.clf = xgb.XGBClassifier(\n            use_label_encoder=False,\n            eval_metric='logloss',\n            scale_pos_weight=self.scale_pos_weight\n        )\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for XGBoost...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        self.print_feature_importance(\"XGBoost\")\n        return bayes_result, y_pred\n\nclass LightGBM(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scale_pos_weight = len(self.y_train[self.y_train == 0]) / len(self.y_train[self.y_train == 1])\n        self.class_weight = {0: 1, 1: self.scale_pos_weight}\n        self.clf = LGBMClassifier(class_weight=self.class_weight)\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for LightGBM...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n\n        self.print_feature_importance(\"LightGBM\")\n        return bayes_result, y_pred\n\nclass AutoML:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, evaluation_metric='recall'):\n        self.cleaned_df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.evaluation_metric = evaluation_metric\n\n        # Initialize model classes\n        self.models = {\n            'DecisionTree': DT(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'RandomForest': RandomForest(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'XGBoost': XGBoost(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'LightGBM': LightGBM(cleaned_df, target_column, balance, n_splits, n_repeats, random_state)\n        }\n\n    def evaluate_models(self):\n        results = {}\n        for name, model in self.models.items():\n            print(f'\\nEvaluating {name}...')\n            best_model, y_pred = model.run_bayesian_search()\n\n            # Compute metrics\n            accuracy = accuracy_score(model.y_test, y_pred)\n            precision = precision_score(model.y_test, y_pred, average='weighted')\n            recall = recall_score(model.y_test, y_pred, average='weighted')\n            f1 = f1_score(model.y_test, y_pred, average='weighted')\n            cm = confusion_matrix(model.y_test, y_pred)\n            tn, fp, fn, tp = cm.ravel()\n\n            # Calculate sensitivity (recall for positive class)\n            sensitivity = recall_score(model.y_test, y_pred, pos_label=1)\n\n            # Calculate specificity\n            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n\n            # Determine false negatives\n            false_negatives = fn\n\n            # Determine false positives\n            false_positives = fp\n\n\n            # ROC and AUC\n            y_test_binarized = label_binarize(model.y_test, classes=np.unique(model.y_test))\n            y_pred_binarized = label_binarize(y_pred, classes=np.unique(model.y_test))\n            if y_test_binarized.shape[1] > 1:\n                auc_scores = []\n                for i in range(y_test_binarized.shape[1]):\n                    auc = model.plot_roc_auc(y_test_binarized[:, i], y_pred_binarized[:, i], f'{name} ROC Curve for Class {i}')\n                    auc_scores.append(auc)\n                mean_auc = np.mean(auc_scores)\n            else:\n                auc = model.plot_roc_auc(model.y_test, y_pred, f'{name} ROC Curve')\n                mean_auc = auc\n\n            # Store results\n            results[name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'false_negatives': false_negatives,\n                'false_positives': false_positives,\n                'sensitivity': sensitivity,\n                'specificity': specificity,\n                'classification_report': classification_report(model.y_test, y_pred),\n                'confusion_matrix': model.show_matrix(cm),\n                'auc': mean_auc\n            }\n\n            # Print Classification Report, Confusion Matrix, and AUC\n            print(f'Classification Report for {name}:\\n{results[name][\"classification_report\"]}')\n            print(f'AUC Score for {name}: {results[name][\"auc\"]}')\n            print(f'Sensitivity for {name}: {results[name][\"sensitivity\"]}')\n            print(f'Specificity for {name}: {results[name][\"specificity\"]}')\n\n        # Determine the best model\n        if self.evaluation_metric in ['false_negatives', 'false_positives']:\n            sorted_results = sorted(results.items(), key=lambda x: x[1][self.evaluation_metric])\n        else:\n            sorted_results = sorted(results.items(), key=lambda x: x[1][self.evaluation_metric], reverse=True)\n\n        best_model_name, best_model_metrics = sorted_results[0]\n        print(f'\\nBest Model: {best_model_name}')\n        print(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\n        print(f'Precision: {best_model_metrics[\"precision\"]}')\n        print(f'Recall: {best_model_metrics[\"recall\"]}')\n        print(f'F1 Score: {best_model_metrics[\"f1\"]}')\n        print(f'False Negatives: {best_model_metrics[\"false_negatives\"]}')\n        print(f'False Positives: {best_model_metrics[\"false_positives\"]}')\n        print(f'AUC Score: {best_model_metrics[\"auc\"]}')\n        print(f'Confusion Matrix:\\n{best_model_metrics[\"confusion_matrix\"]}')\n\n        return best_model_name, best_model_metrics\n\n\n# Example usage\n# Assuming `reduced_df` is your DataFrame and 'thirtydaymortality' is your target column\nauto_ml = AutoML(reduced_df, target_column='thirtydaymortality', balance='imbalanced', evaluation_metric='auc')\nbest_model_name, best_model_metrics = auto_ml.evaluate_models()\n```\n:::\n\n\nEvaluating DecisionTree...\n\nTime taken = 43.36 sec\n\nBest score = 0.921 using params: OrderedDict(\\[('ccp_alpha', 0.001131086677421503), ('criterion', 'entropy'), ('max_depth', 24), ('max_features', 'log2'), ('min_samples_leaf', 3), ('min_samples_split', 3)\\])\n\n**Test accuracy = 0.853**\n\n![](images/clipboard-2144372909.png){width=\"553\"}\n\n![](images/clipboard-3271840178.png){width=\"424\"}\n\n![](images/clipboard-3492499816.png){width=\"419\"}\n\n![](images/clipboard-759995167.png){width=\"410\"}\n\nEvaluating RandomForest...\n\nTime taken = 1131.54 sec Best score = 0.99 using params: OrderedDict(\\[('criterion', 'entropy'), ('max_depth', 27), ('max_features', 'sqrt'), ('min_samples_leaf', 2), ('min_samples_split', 3), ('n_estimators', 82)\\])\n\n**Test accuracy = 0.975**\n\n![](images/clipboard-472869976.png){width=\"581\"}\n\n![](images/clipboard-3293245603.png){width=\"427\"}\n\n![](images/clipboard-3815466419.png){width=\"412\"}\n\n![](images/clipboard-1034884692.png){width=\"416\"}\n\nEvaluating XGBoost...\n\nTime taken = 569.31 sec Best score = 0.999 using params: OrderedDict(\\[('colsample_bytree', 0.6654338738457878), ('learning_rate', 0.038882847597077025), ('max_depth', 21), ('n_estimators', 127), ('subsample', 0.7398225001914931)\\])\n\n**Test accuracy = 0.91**\n\n![](images/clipboard-1553979538.png){width=\"560\"}\n\n![](images/clipboard-1898839044.png){width=\"440\"}\n\n![](images/clipboard-414980368.png){width=\"447\"}\n\n![](images/clipboard-2446320798.png){width=\"446\"}\n\nEvaluating LightGBM...\n\nTime taken = 259.26 sec Best score = 1.0 using params: OrderedDict(\\[('colsample_bytree', 0.8840100626990215), ('learning_rate', 0.017347709137475083), ('max_depth', 20), ('n_estimators', 132), ('subsample', 0.9722507666816909)\\])\n\n**Test accuracy = 0.67**\n\n![](images/clipboard-2294252204.png){width=\"554\"}\n\n![](images/clipboard-3824491189.png){width=\"432\"}\n\n![](images/clipboard-4063952050.png){width=\"447\"}\n\n![](images/clipboard-1205127027.png){width=\"447\"}\n\n### 7. Model Evaluation Summary\n\n#### 1. Decision Tree\n\n**Best Parameters:** - `ccp_alpha`: 0.0011 - `criterion`: 'entropy' - `max_depth`: 24 - `max_features`: 'log2' - `min_samples_leaf`: 3 - `min_samples_split`: 3\n\n**Test Accuracy:** 0.853\n\n**Classification Report:**\n\n| Metric       | Class 0 | Class 1 |\n|--------------|---------|---------|\n| Precision    | 1.00    | 0.03    |\n| Recall       | 0.85    | 0.76    |\n| F1-Score     | 0.92    | 0.06    |\n| Macro Avg    | 0.51    | 0.81    |\n| Weighted Avg | 0.99    | 0.85    |\n\n**AUC Score:** 0.808\n\n**Sensitivity:** 0.764\\\n**Specificity:** 0.853\n\n**Discussion:**\n\n-   **Strengths:** The Decision Tree model has high accuracy (0.853) and relatively good recall for the majority class (0.76 for class 1). The AUC score of 0.808 indicates decent performance in distinguishing between classes.\n-   **Weaknesses:** Precision for the minority class is very low (0.03), and the F1-Score is also very low (0.06), reflecting poor performance in correctly identifying the positive class (class 1). The model struggles with class imbalance.\n\n#### 2. Random Forest\n\n**Best Parameters:** - `criterion`: 'entropy' - `max_depth`: 27 - `max_features`: 'sqrt' - `min_samples_leaf`: 2 - `min_samples_split`: 3 - `n_estimators`: 82\n\n**Test Accuracy:** 0.975\n\n**Classification Report:**\n\n| Metric       | Class 0 | Class 1 |\n|--------------|---------|---------|\n| Precision    | 1.00    | 0.09    |\n| Recall       | 0.98    | 0.35    |\n| F1-Score     | 0.99    | 0.14    |\n| Macro Avg    | 0.54    | 0.66    |\n| Weighted Avg | 0.99    | 0.97    |\n\n**AUC Score:** 0.665\n\n**Sensitivity:** 0.351\\\n**Specificity:** 0.978\n\n**Discussion:**\n\n-   **Strengths:** High accuracy (0.975) and good recall for the majority class (0.35 for class 1). The Random Forest model shows strong performance for the majority class.\n-   **Weaknesses:** Precision for the minority class is extremely low (0.09), indicating poor performance in identifying the positive class. The AUC score of 0.665 suggests weaker discriminatory performance compared to other models.\n\n#### 3. XGBoost\n\n**Best Parameters:** - `colsample_bytree`: 0.665 - `learning_rate`: 0.039 - `max_depth`: 21 - `n_estimators`: 127 - `subsample`: 0.740\n\n**Test Accuracy:** 0.910\n\n**Classification Report:**\n\n| Metric       | Class 0 | Class 1 |\n|--------------|---------|---------|\n| Precision    | 1.00    | 0.03    |\n| Recall       | 0.91    | 0.53    |\n| F1-Score     | 0.95    | 0.06    |\n| Macro Avg    | 0.52    | 0.72    |\n| Weighted Avg | 0.99    | 0.91    |\n\n**AUC Score:** 0.723\n\n**Sensitivity:** 0.534\\\n**Specificity:** 0.913\n\n**Discussion:**\n\n-   **Strengths:** High accuracy (0.910) and good recall for the majority class (0.53 for class 1).\n-   **Weaknesses:** The XGBoost model has low precision (0.03) and F1-Score (0.06) for the minority class. The AUC score of 0.723 shows limited effectiveness in distinguishing between classes.\n\n#### 4. LightGBM\n\n**Best Parameters:** - `colsample_bytree`: 0.884 - `learning_rate`: 0.017 - `max_depth`: 20 - `n_estimators`: 132 - `subsample`: 0.972\n\n**Test Accuracy:** 0.670\n\n**Classification Report:**\n\n| Metric       | Class 0 | Class 1 |\n|--------------|---------|---------|\n| Precision    | 1.00    | 0.01    |\n| Recall       | 0.67    | 0.84    |\n| F1-Score     | 0.80    | 0.03    |\n| Macro Avg    | 0.51    | 0.76    |\n| Weighted Avg | 0.99    | 0.67    |\n\n**AUC Score:** 0.757\n\n**Sensitivity:** 0.845\\\n**Specificity:** 0.669\n\n**Discussion:**\n\n-   **Strengths:** High recall for the minority class (0.84). The model achieves a good balance between precision and recall for the majority class.\n-   **Weaknesses:** Low precision (0.01) and F1-Score (0.03) for the minority class. The AUC score of 0.757 indicates limited ability to discriminate between the classes.\n\n#### Best Model (AUC): Decision Tree\n\n**Accuracy:** 0.853\\\n**Precision:** 0.993 (for class 0), 0.03 (for class 1)\\\n**Recall:** 0.85 (for class 0), 0.76 (for class 1)\\\n**F1 Score:** 0.92 (for class 0), 0.06 (for class 1)\\\n**False Negatives:** 35\\\n**AUC Score:** 0.808\n\n**Discussion:**\n\n-   **Strengths:** The Decision Tree model has the highest recall for the minority class among the models evaluated, with an AUC score of 0.808, indicating better performance in distinguishing between classes compared to other models.\n-   **Weaknesses:** Precision for the minority class remains low, which is a common issue in imbalanced datasets. The F1-Score for the minority class is also low, indicating that while the recall is better, precision is lacking.\n\n#### Overall Recommendations\n\nTo improve performance, consider the following strategies:\n\n-   **Resampling:** Experiment with more methods to deal with imbalance.\n-   **Class Weights:** Adjust class weights in the model to give more importance to the minority class.\n-   **Ensemble Methods:** Combine predictions from multiple models to improve performance on the minority class.\n-   **SVMs:** Consider using SVMs with dimensionality reduction techniques instead of dropping potentially critical variables.\n\n#### Performance Metrics Comparison\n\n| Model         | Accuracy | Precision | Recall (Sensitivity) | Specificity | AUC   |\n|------------|------------|------------|------------|------------|------------|\n| Decision Tree | 0.853    | 0.993     | 0.764                | 0.853       | 0.808 |\n| Random Forest | 0.975    | 0.990     | 0.351                | 0.978       | 0.665 |\n| XGBoost       | 0.910    | 0.999     | 0.534                | 0.913       | 0.723 |\n| LightGBM      | 0.670    | 1.000     | 0.845                | 0.669       | 0.757 |\n\n::: {.notebox .lightbulb data-latex=\"lightbulb\"}\n**Upon fine-tuning to context: Minimising False Negatives**\n\nThe **Decision Tree** model performs best in terms of recall for the minority class and AUC score among the models evaluated.\n\nHOWEVER, sensitivity score (0.845) is higher for **LightGBM** instead even though it has less AUC score (0.757). **LightGBM** is the ideal model for our scenario.\n:::\n\n",
    "supporting": [
      "AutoML_Binary_Classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}