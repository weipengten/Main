{
  "hash": "cf43149c74d7dcfe0e4418ca1774a068",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Image Learning on CIFAR-10\"\nauthor: \"Ten Wei Peng\"\ndescription: \"Deep Learning with CIFAR-10\"\nexecute: \n  eval: false\n  echo: true\n  warning: false\n  freeze: true\ndate: \"2024-07-20\"\nformat: html\n---\n\n![](images/clipboard-4123856362.png){width=\"475\"}\n\n## Introduction\n\nHello everyone, this post is intended to provide an in-depth comparison and discussion of various methods for image learning. I will add more detailed explanations as soon as I have the time.\n\n`Changelog:`\n\n`\\[1.0.0\\] - 2024-07-20 - Initial deployment after consolidating course work and initial refinement.`\n\nThis page demonstrates how to perform an image classification task using:\n\n-   **Multi-Layer Perceptron (MLP)**\n\n-   **Convolutional Neural Network (CNN)**\n\n-   **Transfer Learning using Resnet50** *(in-progress)*\n\nfor the CIFAR-10 dataset as a demonstration\n\n## Importing Libraries\n\n::: {#18d03693 .cell execution_count=1}\n``` {.python .cell-code}\n#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#Data preprocessing and modeling related functions\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n#Cross-validation and evaluation related functions\nfrom sklearn.model_selection import KFold\n\n#Datamodeling related functions\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import SGD\nfrom sklearn import model_selection, preprocessing, metrics\nfrom keras.applications import VGG16, ResNet50, InceptionV3\n```\n:::\n\n\n## Exploring the dataset\n\nThis Python code snippet uses TensorFlow's Keras API to load the CIFAR-10 dataset and print the shapes of the training and test sets. The CIFAR-10 dataset is a popular dataset used for training machine learning and computer vision algorithms. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class.\n\n::: {#9745cb7d .cell execution_count=2}\n``` {.python .cell-code}\n# Load the CIFAR-10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nprint(\"Training set shape:\", x_train.shape)\nprint(\"Test set shape:\", x_test.shape)\n```\n:::\n\n\n![](images/clipboard-929340846.png)\n\n::: callout-note\n-   The x_train.shape returns **(50000, 32, 32, 3)**, indicating that there are 50,000 training images, each of size 32x32 pixels with 3 color channels (RGB).\n\n-   The x_test.shape returns **(10000, 32, 32, 3)**, indicating that there are 10,000 test images, each of size 32x32 pixels with 3 color channels (RGB).\n:::\n\n**This code will display the first 10 images from the CIFAR-10 training set with their respective classes.**\n\n::: {#b3d381e7 .cell execution_count=3}\n``` {.python .cell-code}\nimages = range(0,10)\nclasses = list(np.unique(y_train))\nplt.figure(figsize=(20,10))\nfor i in images:\n        plt.subplot(2,5,1 + i).set_title(classes[y_train[i][0]])\n        plt.imshow(x_train[i])\n```\n:::\n\n\n![](images/clipboard-1174994514.png)\n\n**Next, let's reshape the arrays to have only 2 dimensions**\n\n::: {#8ec755b3 .cell execution_count=4}\n``` {.python .cell-code}\nx_train = x_train.reshape(50000, 32 * 32 * 3)\nx_test  = x_test.reshape (10000, 32 * 32 * 3)\n\nprint(\"Training set shape:\", x_train.shape)\nprint(\"Test set shape:\", x_test.shape)\n```\n:::\n\n\n![](images/clipboard-149929291.png)\n\n::: callout-note\nBy reshaping the images into **(50000, 3072)** and **(10000, 3072)**, each image is now represented as a 1-dimensional array (vector) of length 3072. This reshaping is commonly done to prepare the data for certain machine learning algorithms that expect input as 1-dimensional arrays rather than 3-dimensional images.\n:::\n\n**Next we perform the necessary pre-processing steps:**\n\n**Splitting the Training Dataset:** The training dataset is split into a new training set and a validation set using an 80/20 split to evaluate the model during training. The train_test_split function is used with test_size=0.2 and random_state=42 for reproducibility.\n\n**Data Type Conversion:** The feature arrays are converted to float32 data type to ensure compatibility with TensorFlow\n\n::: {#65664014 .cell execution_count=5}\n``` {.python .cell-code}\n## Splitting \"training\" dataset further into train,validation datasets\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n\nx_train = x_train.astype('float32')\nx_valid = x_valid.astype('float32')\nx_test = x_test.astype('float32')\n```\n:::\n\n\n**Min-Max Scaling:** . A Min-Max scaling function normalizes the pixel values of the images to the range \\[0, 1\\], which helps in faster and more stable training of the model.\n\n::: {#51f495b1 .cell execution_count=6}\n``` {.python .cell-code}\n## Transform method: MinMax - is preferred when working with TensorFlow\ndef min_max_scaler(x_train, x_valid, x_test):\n  x_train_ms= x_train/255\n  x_valid_ms= x_valid/255\n  x_test_ms = x_test/255\n  return x_train_ms,x_valid_ms,x_test_ms\n\nx_train_ms, x_valid_ms, x_test_ms = min_max_scaler(x_train, x_valid, x_test)\n```\n:::\n\n\n**One-Hot Encoding the Labels:** The class labels are converted to one-hot encoded vectors using to_categorical, which transforms the integer labels into binary vectors. This format is required for categorical classification tasks where the neural network's output layer expects binary vectors.\n\n::: {#4e06c52b .cell execution_count=7}\n``` {.python .cell-code}\n# By converting the target variables to one-hot format, we can ensure that they are compatible with the output layer of the MLP model, which expects the target variables to be represented as a vector of binary values.\ny_train_1hot = to_categorical(y_train, 10)\ny_valid_1hot = to_categorical(y_valid, 10)\ny_test_1hot  = to_categorical(y_test , 10)\n```\n:::\n\n\n## 1. Implementing a Multi-Layer Perceptron for CIFAR-10 Classification\n\n**MLP with one hidden layer consisting of 256 neurons,sgd**\n\n-   **Optimizer**: SGD\n\n-   **Loss**: Cross Entropy\n\n-   **Hidden Layers**: 1 with 256 neurons\n\n-   **Activation Layer**: (ReLU, softmax)\n\n::: {#8f0a8894 .cell execution_count=8}\n``` {.python .cell-code}\n# Define the model architecture\nmodel = keras.Sequential([\n    # Input Layer\n    keras.layers.Dense(512, activation='relu', input_shape=(32*32*3,)),\n\n    # Hidden Layers\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dropout(0.2),\n\n    # Output Layer\n    keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='SGD',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model using mini-batch learning\nbatch_size = 64\nepochs = 40\npatience = 5\nbest_acc = 0\nfor epoch in range(epochs):\n    for i in range(0, len(x_train_ms), batch_size):\n        batch_x, batch_y = x_train_ms[i:i+batch_size], y_train_1hot[i:i+batch_size]\n        model.train_on_batch(batch_x, batch_y)\n\n    # Evaluate the model on the validation set after each epoch\n    val_loss, val_acc = model.evaluate(x_valid_ms, y_valid_1hot)\n    print('Epoch %d: validation accuracy=%f' % (epoch+1, val_acc))\n\n    # Check if the validation accuracy has improved\n    if val_acc > best_acc:\n        best_acc = val_acc\n        patience = 0\n    else:\n        patience += 1\n        print(\"patience =\",patience)\n\n    # Stop training if the validation accuracy does not improve after a certain number of epochs\n    if patience == 5:\n        break\n\n# Evaluate final model on test set\ntest_pred = model.predict(x_test_ms)\ntest_accuracy = accuracy_score(np.argmax(y_test_1hot, axis=1), np.argmax(test_pred, axis=1))\n\nprint(\"Test Accuracy of MLP, 1 Hidden Layer with 256 neurons, SGD, ReLu = \",round(test_accuracy,2))\n```\n:::\n\n\n![](images/clipboard-3703383034.png)\n\n![](images/clipboard-2107656334.png)\n\n## 2. Implementing a Convolutional Neural Network (CNN) for CIFAR-10 Classification\n\n**Preprocessing**\n\nFirst off, let's reproduce some of the preprocessing steps we did earlier:\n\n::: {#de89d0d3 .cell execution_count=9}\n``` {.python .cell-code}\n# Load Data and Split into Train, Test dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Onehot Encoding\ny_train = to_categorical(y_train, 10)\ny_test  = to_categorical(y_test , 10)\n\n# Convert to float datatype\nx_train = x_train.astype('float32')\nx_test  = x_test.astype('float32')\n\n# Minmax Scaling\nx_train  /= 255\nx_test   /= 255\n```\n:::\n\n\n**Data Augmentation, Manual Splitting and Batch Processing**\n\nNext, this set of additional steps are required for preprocessing for CNN\n\n::: {#029b19a7 .cell execution_count=10}\n``` {.python .cell-code}\n# Data Augmentation\ntransform_train = ImageDataGenerator(\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        rotation_range = 10,\n        horizontal_flip=True)   # flip images horizontally\n\nvalidation_train = ImageDataGenerator()\n\n# Split Train dataset to get Validation dataset\nvalidation_train = ImageDataGenerator()\ntrain_set = transform_train.flow(x_train[:40000], y_train[:40000], batch_size=32)\nvalidation_set = validation_train.flow(x_train[40000:], y_train[40000:], batch_size=32)\n```\n:::\n\n\n::: callout-note\n**Why do we need additional preprocessing steps for CNN?**\n\nConvolutional Neural Networks (CNNs) are specifically designed to work with image data. To enhance their performance, particularly when working with relatively small datasets like CIFAR-10, additional preprocessing steps are often employed. These steps help improve the model's ability to generalize to unseen data and mitigate overfitting.\n\n-   **Data Augmentation:** Increases the diversity of the training dataset by applying random transformations such as shifts, rotations, and flips. This helps the CNN model generalize better by training on a wider variety of image presentations. This is different from MLP earlier as MLPs typically work with flattened input data and do not benefit as much from spatial data augmentation\n\n-   **Manual Splitting and Batch Processing:** The `ImageDataGenerator` in TensorFlow/Keras handles data augmentation and requires the data to be in a specific format (i.e., batches). Hence, the split is done manually to apply augmentation on the training set while keeping the validation set unchanged. CNNs typically require batch processing for efficient training. The flow method from `ImageDataGenerator`creates an iterator that generates batches of augmented data on-the-fly. This is especially useful when working with large datasets or when applying data augmentation. Manual splitting allows for better control over how data is augmented and fed into the model. By explicitly defining the training and validation datasets, we ensure that augmentation is applied only to the training data, not the validation data.\n:::\n\n### Building a Convolutional Neural Network for CIFAR-10!\n\n-   Batch Size: 64\n\n-   Epochs: 50\n\n-   Padding: Same\n\n-   Kernel: 3\\*3\n\n-   (Convolution-BatchNormalisation-Relu-MaxPooling) x 2\n\n-   Loss Function: Categorical Crossentropy\n\n-   Dropout: 0.25,0.25,0.5\n\n##### `Padding: Same:` This means that the output spatial dimensions will be the same as the input spatial dimensions after applying the convolution operation. This is achieved by adding zeros around the borders of the input tensor so that the output spatial dimensions match the input spatial dimensions.\n\n##### `Kernel: 3x3:` A kernel size of 3x3 is a standard choice for convolutional neural networks because it provides a good balance between capturing local patterns and not overfitting to the training data. Larger kernel sizes can capture more complex patterns but may lead to overfitting, while smaller kernel sizes may not capture enough information.\n\n##### `(Convolution-BatchNormalisation-Relu-MaxPooling) x 2`: This sequence of layers is commonly used in convolutional neural networks because it allows the model to learn increasingly complex features while reducing overfitting.\n\n##### `Batch normalization` normalize the activations to improve generalization\n\n##### `ReLU activation functions` introduce non-linearity to the model\n\n##### `Max pooling` layers downsample the spatial dimensions to reduce computational complexity and help the model learn spatial hierarchies.\n\n##### `Loss Function: Categorical Crossentropy:` This loss function is commonly used for multi-class classification problems like image classification. It measures the difference between the predicted probabilities and the true labels by calculating the negative log likelihood of the true class. This encourages the model to output high probabilities for the correct classes and low probabilities for the incorrect classe\n\n##### `Dropout` is a regularization technique that helps prevent overfitting by randomly setting some of the neurons to zero during training. By using a high dropout rate (0.25) and (0.5), it is likely to reduce overfitting and contribute to better performance.\n\n::: {#a1bd167c .cell execution_count=11}\n``` {.python .cell-code}\n## CNN Archeticure\ndef CNN():\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]) )\n    model.add(Activation('relu'))\n\n    model.add(Conv2D(32,(3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(32,(3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n\n    model.add(Dense(512))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n\n\n\n# Train model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ncnn = CNN()\n\n\ncnn.fit(train_set,\n        epochs=50,\n        batch_size=64,\n        validation_data=validation_set, shuffle=True)\n\nloss, acc = cnn.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test acc:', acc)\n```\n:::\n\n\n![](images/clipboard-3449070481.png)\n\n## 3. Leveraging a Pretrained Model for CIFAR-10\n\n**ResNet50** is known for its high accuracy and relatively efficient parameter count among pretrained models. We should adjust its input dimensions to fit the CIFAR-10 dataset rather than using its default settings. See <https://paperswithcode.com/sota/image-classification-on-imagenet> to see list of pretrained models and their performance, using ImageNet dataset as benchmark.\n\nFirst, let's illustrate a method that is **conceptually sound but inefficient** for using pretrained models.\n\n-   **Resizing layer** is added to adjust the image dimensions from the original 32x32 pixels to 224x224 pixels. This step is crucial because ResNet50 was originally trained on the ImageNet dataset, where images have a resolution of *224x224* pixels. By resizing our images to match this resolution, we align them with the input format the model was designed for, ensuring that the features learned from ImageNet are effectively utilized. This alignment helps maintain the integrity of the pretrained model’s architecture and improves its performance on our specific dataset.\n\n-   **Early Stopping** is employed to enhance training efficiency by halting the training process when the model's performance ceases to improve on the validation dataset. This prevents unnecessary computations and helps avoid overfitting.\n\n-   **ModelCheckpoint callback** is used to save the model’s weights at specific points during training, ensuring that the best-performing model (according to validation metrics) is preserved. This way, if the training process is interrupted or if we need to revert to a previous state, we can load the saved weights and continue from the best checkpoint. Together, these techniques help optimize both the training time and the final performance of the model.\n\n::: {#3f56d521 .cell execution_count=12}\n``` {.python .cell-code}\npretrained_model = ResNet50(input_shape=(224, 224, 3),\n                    include_top=False)\n\npretrained_model.trainable = True\n\nmodel = Sequential([\n          Resizing(224, 224),  # Resize images to 224x224\n            pretrained_model,\n            BatchNormalization(),\n            GlobalAveragePooling2D(),\n            Dense(1024, activation='relu'),\n            Dropout(0.4),\n            Dense(10, activation='softmax', dtype='float32'),\n        ])\n\n# Compile the model with the same optimizer and loss function\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Define early stopping parameters\nearly_stopping_patience = 5 # Stop training if no improvement after 5 epochs\nearly_stopping_min_delta = 0.01 # Stop training if improvement is less than 0.01%\n\n# Create EarlyStopping callback object\nearly_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, min_delta=early_stopping_min_delta)\n\n# Define where to save the best model based on validation accuracy\nbest_model_path = 'best_model.h5'\n\n# Create ModelCheckpoint callback object\nmodel_checkpoint = ModelCheckpoint(best_model_path + '.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n# Continue training from the last saved model\nmodel.fit(train_set,\n        epochs=50,\n        batch_size=64,\n        validation_data=validation_set, shuffle=True,callbacks=[early_stopping, model_checkpoint])\n\n# Evaluate final model on test set\nloss, acc = best_model.evaluate(x_test, y_test, verbose=1)\n\nprint('Test loss:', loss)\nprint('Test acc:', acc)\n```\n:::\n\n\n![](images/clipboard-3999430006.png)\n\n![](images/clipboard-239623973.png)\n\n::: callout-note\n**We achieved a high test score of 0.898, but the question remains: is this the best way to use transfer learning?**\n\nTo determine if our approach to transfer learning is correct, we first need to understand the principles behind it. Transfer learning is designed to enhance efficiency and reduce computational costs by leveraging pretrained models. Instead of retraining a model from scratch, we use an existing model that has already been trained on a large dataset. This typically involves \"freezing\" the earlier layers of the model, which means keeping their weights unchanged, and only training the final layers on our specific dataset. This strategy allows us to build on the learned features of the pretrained model, thus saving time and computational resources while potentially achieving better performance with less data.\n\nIn the case above, we had set `pretrained_model.trainable = True` which we should have set to `False` to fully leverage the full benefits of a pretrained model.\n:::\n\n**A Better way to utilise a pretrained model:** Firstly, instead of incorporating a resizing layer within the model, we should resize our images to 224x224 pixels during the preprocessing step.\n\n::: callout-note\n**Can you think of the rationale for this?**\n:::\n\n### To be Continued.....\n\n",
    "supporting": [
      "ImageLearningCifar10_files"
    ],
    "filters": [],
    "includes": {}
  }
}