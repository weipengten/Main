[
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Main.html",
    "href": "Main.html",
    "title": "Wei Peng’s Analytics Journey",
    "section": "",
    "text": "Welcome! In this website, I will be sharing my learning journey on in analytics."
  },
  {
    "objectID": "Main.html#geospatial-analytics",
    "href": "Main.html#geospatial-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Geospatial Analytics",
    "text": "Geospatial Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Main.html#visual-analytics",
    "href": "Main.html#visual-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Visual Analytics",
    "text": "Visual Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\nTen Wei Peng\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Main.html#tableau",
    "href": "Main.html#tableau",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Tableau",
    "text": "Tableau\n\n\n\n\n\n\n\nQuarterly Private Residential Property Watcher\n\n\nTableau Visualization\n\n\n\n\n\nurl\n\n\nhttps://public.tableau.com/app/profile/wei.peng.ten/viz/in-Class_2/QuaterlyPrivateResidentialPropertyWatcher2023-2024Q1#2\n\n\n\n\nthumbnail\n\n\nimages/quarterly_price.png\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "In this take-home exercise we are required to:\n\nSelect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\nCritic the submission in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices.\nRemake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\nFor this exercise, I have chosen our classmate ZOU JIAXUN’s visualization in his Take-home Exercise 1 submission to critique."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#original-design",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#original-design",
    "title": "Take Home Exercise 2",
    "section": "2.1 2.1 Original Design",
    "text": "2.1 2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "title": "Take Home Exercise 2",
    "section": "2.2 2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 2.2 Critique: Clarity and Aesthetics\n\n2.2.1 Clarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\n2.2.2 Aesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#proposed-sketch",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#proposed-sketch",
    "title": "Take Home Exercise 2",
    "section": "3.1 3.1 Proposed Sketch",
    "text": "3.1 3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#data-preparation",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#data-preparation",
    "title": "Take Home Exercise 2",
    "section": "3.2 3.2 Data Preparation",
    "text": "3.2 3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\n3.2.1 Original data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\n3.2.2 Modifying the prepared data\n\n3.2.2.1 Binning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\n3.2.2.2 Base Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\n3.2.2.3 Median Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\n3.2.2.4 Transaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\n3.2.2.5 Total Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\n3.2.2.6 Joining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\n3.2.2.7 Signal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "Main.html#analysis-using-rstudio",
    "href": "Main.html#analysis-using-rstudio",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Analysis using RStudio",
    "text": "Analysis using RStudio"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "In this take-home exercise we are required to:\n\nSelect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\nCritic the submission in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices.\nRemake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\nFor this exercise, I have chosen our classmate ZOU JIAXUN’s visualization in his Take-home Exercise 1 submission to critique."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#original-design",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#original-design",
    "title": "Take Home Exercise 2",
    "section": "2.1 2.1 Original Design",
    "text": "2.1 2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "title": "Take Home Exercise 2",
    "section": "2.2 2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 2.2 Critique: Clarity and Aesthetics\n\n2.2.1 Clarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\n2.2.2 Aesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#proposed-sketch",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#proposed-sketch",
    "title": "Take Home Exercise 2",
    "section": "3.1 3.1 Proposed Sketch",
    "text": "3.1 3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#data-preparation",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#data-preparation",
    "title": "Take Home Exercise 2",
    "section": "3.2 3.2 Data Preparation",
    "text": "3.2 3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\n3.2.1 Original data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\n3.2.2 Modifying the prepared data\n\n3.2.2.1 Binning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\n3.2.2.2 Base Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\n3.2.2.3 Median Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\n3.2.2.4 Transaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\n3.2.2.5 Total Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\n3.2.2.6 Joining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\n3.2.2.7 Signal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wei Peng’s Analytics Journey",
    "section": "",
    "text": "Welcome! In this website, I will be sharing my learning journey in analytics."
  },
  {
    "objectID": "index.html#geospatial-analytics",
    "href": "index.html#geospatial-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Geospatial Analytics",
    "text": "Geospatial Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#visual-analytics",
    "href": "index.html#visual-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Visual Analytics",
    "text": "Visual Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\nTen Wei Peng\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#tableau",
    "href": "index.html#tableau",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Tableau",
    "text": "Tableau\n\n\n\n\n\n\n\n\nBullet Graph and Sparkline\n\n\nTableau Visualization\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMapping in Tableau\n\n\nTableau Visualization\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuarterly Private Residential Property Watcher\n\n\nTableau Visualization\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "index.html#analysis-using-rstudio",
    "href": "index.html#analysis-using-rstudio",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Analysis using RStudio",
    "text": "Analysis using RStudio\n\nRStudio\n\n\n\n\n\n\n\n\nAnalyzing Bus Traffic Flows in Singapore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Better Visualisations\n\n\n\n\n\n\nTen Wei Peng\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html",
    "title": "Creating Better Visualisations",
    "section": "",
    "text": "In this take-home exercise we are required to:\n\nSelect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\nCritic the submission in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices.\nRemake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\nFor this exercise, I have chosen our classmate ZOU JIAXUN’s visualization in his Take-home Exercise 1 submission to critique."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "title": "Creating Better Visualisations",
    "section": "2.1 Original Design",
    "text": "2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "title": "Creating Better Visualisations",
    "section": "2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 Critique: Clarity and Aesthetics\n\nClarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\nAesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "title": "Creating Better Visualisations",
    "section": "3.1 Proposed Sketch",
    "text": "3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "title": "Creating Better Visualisations",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\nOriginal data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\nModifying the prepared data\n\nBinning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\nBase Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\nMedian Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\nTransaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\nTotal Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\nJoining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\nSignal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Open Government Data",
    "text": "Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Specially collected data",
    "text": "Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Geospatial Data Science",
    "text": "Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Spatial Interaction Modelling",
    "text": "Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "index.html#rstudio",
    "href": "index.html#rstudio",
    "title": "Wei Peng’s Analytics Journey",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\n\n\nAnalyzing Bus Traffic Flows in Singapore\n\n\nGeospatial Analytics\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Better Visualisations\n\n\nVisual Analytics\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tableau/Tableau_Visualization.html",
    "href": "Tableau/Tableau_Visualization.html",
    "title": "Quarterly Private Residential Property Watcher",
    "section": "",
    "text": "Quarterly Private Residential Property Watcher\n\nView Visualization"
  },
  {
    "objectID": "Tableau/Quarterly_Private_Residential_Property_Watcher.html",
    "href": "Tableau/Quarterly_Private_Residential_Property_Watcher.html",
    "title": "Quarterly Private Residential Property Watcher",
    "section": "",
    "text": "Quarterly Private Residential Property Watcher"
  },
  {
    "objectID": "Tableau/Mapping_In_Tableau.html",
    "href": "Tableau/Mapping_In_Tableau.html",
    "title": "Mapping in Tableau",
    "section": "",
    "text": "Note\n\n\n\nTableau utilizes geocoding services to convert geographical data, such as postal codes into latitude and longitude coordinates. This simplifies the process since we do not need to include these coordinates in the dataset. However, it’s important to remember that the accuracy of your geodata depends on the latest update of Tableau’s geocoding database.\n\n\nLatitude and Longitude for PropSymbol map: generated via Postal Code\nLatitude and Longitude for Treemap: generated via Planning Region"
  },
  {
    "objectID": "Tableau/Bullet_Graph_and_Sparkline.html",
    "href": "Tableau/Bullet_Graph_and_Sparkline.html",
    "title": "Bullet Graph and Sparkline",
    "section": "",
    "text": "Note\n\n\n\nBullet Graph and Sparklines are perfect to include in dashboards where space is limited but performance metrics need to be displayed clearly."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Python",
    "text": "Python\n\n\n\n\n\n\n\n\nImage Learning on CIFAR-10\n\n\nDeep Learning with CIFAR-10\n\n\n\nTen Wei Peng\n\n\nJul 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html",
    "href": "Python/ImageLearningCifar10.html",
    "title": "Image Learning on CIFAR-10",
    "section": "",
    "text": "mic# Testing Python in Quarto\nThis document demonstrates how to include and execute Python code in a Quarto document.\n\n# This is a Python code chunk\nimport numpy as np"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Open Government Data",
    "text": "Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Specially collected data",
    "text": "Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Geospatial Data Science",
    "text": "Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Spatial Interaction Modelling",
    "text": "Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "title": "Creating Better Visualisations",
    "section": "2.1 Original Design",
    "text": "2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "title": "Creating Better Visualisations",
    "section": "2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 Critique: Clarity and Aesthetics\n\nClarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\nAesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "title": "Creating Better Visualisations",
    "section": "3.1 Proposed Sketch",
    "text": "3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "title": "Creating Better Visualisations",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\nOriginal data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\nModifying the prepared data\n\nBinning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\nBase Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\nMedian Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\nTransaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\nTotal Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\nJoining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\nSignal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "Open Government Data",
    "text": "Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "Specially collected data",
    "text": "Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "Geospatial Data Science",
    "text": "Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "Spatial Interaction Modelling",
    "text": "Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#introduction",
    "href": "Python/ImageLearningCifar10.html#introduction",
    "title": "Image Learning on CIFAR-10",
    "section": "Introduction",
    "text": "Introduction\nHello everyone, this post is intended to provide an in-depth comparison and discussion of various methods for image learning. I will add more detailed explanations as soon as I have the time.\nChangelog:\n\\[1.0.0\\] - 2024-07-20 - Initial deployment after consolidating course work and initial refinement.\nThis page demonstrates how to perform an image classification task using:\n\nMulti-Layer Perceptron (MLP)\nConvolutional Neural Network (CNN)\nTransfer Learning using Resnet50 (in-progress)\n\nfor the CIFAR-10 dataset as a demonstration"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#importing-libraries",
    "href": "Python/ImageLearningCifar10.html#importing-libraries",
    "title": "Image Learning on CIFAR-10",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#Data preprocessing and modeling related functions\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n#Cross-validation and evaluation related functions\nfrom sklearn.model_selection import KFold\n\n#Datamodeling related functions\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import SGD\nfrom sklearn import model_selection, preprocessing, metrics\nfrom keras.applications import VGG16, ResNet50, InceptionV3"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#exploring-the-dataset",
    "href": "Python/ImageLearningCifar10.html#exploring-the-dataset",
    "title": "Image Learning on CIFAR-10",
    "section": "Exploring the dataset",
    "text": "Exploring the dataset\nThis Python code snippet uses TensorFlow’s Keras API to load the CIFAR-10 dataset and print the shapes of the training and test sets. The CIFAR-10 dataset is a popular dataset used for training machine learning and computer vision algorithms. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class.\n\n# Load the CIFAR-10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nprint(\"Training set shape:\", x_train.shape)\nprint(\"Test set shape:\", x_test.shape)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe x_train.shape returns (50000, 32, 32, 3), indicating that there are 50,000 training images, each of size 32x32 pixels with 3 color channels (RGB).\nThe x_test.shape returns (10000, 32, 32, 3), indicating that there are 10,000 test images, each of size 32x32 pixels with 3 color channels (RGB).\n\n\n\nThis code will display the first 10 images from the CIFAR-10 training set with their respective classes.\n\nimages = range(0,10)\nclasses = list(np.unique(y_train))\nplt.figure(figsize=(20,10))\nfor i in images:\n        plt.subplot(2,5,1 + i).set_title(classes[y_train[i][0]])\n        plt.imshow(x_train[i])\n\n\nNext, let’s reshape the arrays to have only 2 dimensions\n\nx_train = x_train.reshape(50000, 32 * 32 * 3)\nx_test  = x_test.reshape (10000, 32 * 32 * 3)\n\nprint(\"Training set shape:\", x_train.shape)\nprint(\"Test set shape:\", x_test.shape)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy reshaping the images into (50000, 3072) and (10000, 3072), each image is now represented as a 1-dimensional array (vector) of length 3072. This reshaping is commonly done to prepare the data for certain machine learning algorithms that expect input as 1-dimensional arrays rather than 3-dimensional images.\n\n\nNext we perform the necessary pre-processing steps:\nSplitting the Training Dataset: The training dataset is split into a new training set and a validation set using an 80/20 split to evaluate the model during training. The train_test_split function is used with test_size=0.2 and random_state=42 for reproducibility.\nData Type Conversion: The feature arrays are converted to float32 data type to ensure compatibility with TensorFlow\n\n## Splitting \"training\" dataset further into train,validation datasets\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n\nx_train = x_train.astype('float32')\nx_valid = x_valid.astype('float32')\nx_test = x_test.astype('float32')\n\nMin-Max Scaling: . A Min-Max scaling function normalizes the pixel values of the images to the range [0, 1], which helps in faster and more stable training of the model.\n\n## Transform method: MinMax - is preferred when working with TensorFlow\ndef min_max_scaler(x_train, x_valid, x_test):\n  x_train_ms= x_train/255\n  x_valid_ms= x_valid/255\n  x_test_ms = x_test/255\n  return x_train_ms,x_valid_ms,x_test_ms\n\nx_train_ms, x_valid_ms, x_test_ms = min_max_scaler(x_train, x_valid, x_test)\n\nOne-Hot Encoding the Labels: The class labels are converted to one-hot encoded vectors using to_categorical, which transforms the integer labels into binary vectors. This format is required for categorical classification tasks where the neural network’s output layer expects binary vectors.\n\n# By converting the target variables to one-hot format, we can ensure that they are compatible with the output layer of the MLP model, which expects the target variables to be represented as a vector of binary values.\ny_train_1hot = to_categorical(y_train, 10)\ny_valid_1hot = to_categorical(y_valid, 10)\ny_test_1hot  = to_categorical(y_test , 10)"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#implementing-a-multi-layer-perceptron-for-cifar-10-classification",
    "href": "Python/ImageLearningCifar10.html#implementing-a-multi-layer-perceptron-for-cifar-10-classification",
    "title": "Image Learning on CIFAR-10",
    "section": "1. Implementing a Multi-Layer Perceptron for CIFAR-10 Classification",
    "text": "1. Implementing a Multi-Layer Perceptron for CIFAR-10 Classification\nMLP with one hidden layer consisting of 256 neurons,sgd\n\nOptimizer: SGD\nLoss: Cross Entropy\nHidden Layers: 1 with 256 neurons\nActivation Layer: (ReLU, softmax)\n\n\n# Define the model architecture\nmodel = keras.Sequential([\n    # Input Layer\n    keras.layers.Dense(512, activation='relu', input_shape=(32*32*3,)),\n\n    # Hidden Layers\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dropout(0.2),\n\n    # Output Layer\n    keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='SGD',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model using mini-batch learning\nbatch_size = 64\nepochs = 40\npatience = 5\nbest_acc = 0\nfor epoch in range(epochs):\n    for i in range(0, len(x_train_ms), batch_size):\n        batch_x, batch_y = x_train_ms[i:i+batch_size], y_train_1hot[i:i+batch_size]\n        model.train_on_batch(batch_x, batch_y)\n\n    # Evaluate the model on the validation set after each epoch\n    val_loss, val_acc = model.evaluate(x_valid_ms, y_valid_1hot)\n    print('Epoch %d: validation accuracy=%f' % (epoch+1, val_acc))\n\n    # Check if the validation accuracy has improved\n    if val_acc &gt; best_acc:\n        best_acc = val_acc\n        patience = 0\n    else:\n        patience += 1\n        print(\"patience =\",patience)\n\n    # Stop training if the validation accuracy does not improve after a certain number of epochs\n    if patience == 5:\n        break\n\n# Evaluate final model on test set\ntest_pred = model.predict(x_test_ms)\ntest_accuracy = accuracy_score(np.argmax(y_test_1hot, axis=1), np.argmax(test_pred, axis=1))\n\nprint(\"Test Accuracy of MLP, 1 Hidden Layer with 256 neurons, SGD, ReLu = \",round(test_accuracy,2))"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#implementing-a-convolutional-neural-network-cnn-for-cifar-10-classification",
    "href": "Python/ImageLearningCifar10.html#implementing-a-convolutional-neural-network-cnn-for-cifar-10-classification",
    "title": "Image Learning on CIFAR-10",
    "section": "2. Implementing a Convolutional Neural Network (CNN) for CIFAR-10 Classification",
    "text": "2. Implementing a Convolutional Neural Network (CNN) for CIFAR-10 Classification\nPreprocessing\nFirst off, let’s reproduce some of the preprocessing steps we did earlier:\n\n# Load Data and Split into Train, Test dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Onehot Encoding\ny_train = to_categorical(y_train, 10)\ny_test  = to_categorical(y_test , 10)\n\n# Convert to float datatype\nx_train = x_train.astype('float32')\nx_test  = x_test.astype('float32')\n\n# Minmax Scaling\nx_train  /= 255\nx_test   /= 255\n\nData Augmentation, Manual Splitting and Batch Processing\nNext, this set of additional steps are required for preprocessing for CNN\n\n# Data Augmentation\ntransform_train = ImageDataGenerator(\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        rotation_range = 10,\n        horizontal_flip=True)   # flip images horizontally\n\nvalidation_train = ImageDataGenerator()\n\n# Split Train dataset to get Validation dataset\nvalidation_train = ImageDataGenerator()\ntrain_set = transform_train.flow(x_train[:40000], y_train[:40000], batch_size=32)\nvalidation_set = validation_train.flow(x_train[40000:], y_train[40000:], batch_size=32)\n\n\n\n\n\n\n\nNote\n\n\n\nWhy do we need additional preprocessing steps for CNN?\nConvolutional Neural Networks (CNNs) are specifically designed to work with image data. To enhance their performance, particularly when working with relatively small datasets like CIFAR-10, additional preprocessing steps are often employed. These steps help improve the model’s ability to generalize to unseen data and mitigate overfitting.\n\nData Augmentation: Increases the diversity of the training dataset by applying random transformations such as shifts, rotations, and flips. This helps the CNN model generalize better by training on a wider variety of image presentations. This is different from MLP earlier as MLPs typically work with flattened input data and do not benefit as much from spatial data augmentation\nManual Splitting and Batch Processing: The ImageDataGenerator in TensorFlow/Keras handles data augmentation and requires the data to be in a specific format (i.e., batches). Hence, the split is done manually to apply augmentation on the training set while keeping the validation set unchanged. CNNs typically require batch processing for efficient training. The flow method from ImageDataGeneratorcreates an iterator that generates batches of augmented data on-the-fly. This is especially useful when working with large datasets or when applying data augmentation. Manual splitting allows for better control over how data is augmented and fed into the model. By explicitly defining the training and validation datasets, we ensure that augmentation is applied only to the training data, not the validation data.\n\n\n\n\nBuilding a Convolutional Neural Network for CIFAR-10!\n\nBatch Size: 64\nEpochs: 50\nPadding: Same\nKernel: 3*3\n(Convolution-BatchNormalisation-Relu-MaxPooling) x 2\nLoss Function: Categorical Crossentropy\nDropout: 0.25,0.25,0.5\n\n\nPadding: Same: This means that the output spatial dimensions will be the same as the input spatial dimensions after applying the convolution operation. This is achieved by adding zeros around the borders of the input tensor so that the output spatial dimensions match the input spatial dimensions.\n\n\nKernel: 3x3: A kernel size of 3x3 is a standard choice for convolutional neural networks because it provides a good balance between capturing local patterns and not overfitting to the training data. Larger kernel sizes can capture more complex patterns but may lead to overfitting, while smaller kernel sizes may not capture enough information.\n\n\n(Convolution-BatchNormalisation-Relu-MaxPooling) x 2: This sequence of layers is commonly used in convolutional neural networks because it allows the model to learn increasingly complex features while reducing overfitting.\n\n\nBatch normalization normalize the activations to improve generalization\n\n\nReLU activation functions introduce non-linearity to the model\n\n\nMax pooling layers downsample the spatial dimensions to reduce computational complexity and help the model learn spatial hierarchies.\n\n\nLoss Function: Categorical Crossentropy: This loss function is commonly used for multi-class classification problems like image classification. It measures the difference between the predicted probabilities and the true labels by calculating the negative log likelihood of the true class. This encourages the model to output high probabilities for the correct classes and low probabilities for the incorrect classe\n\n\nDropout is a regularization technique that helps prevent overfitting by randomly setting some of the neurons to zero during training. By using a high dropout rate (0.25) and (0.5), it is likely to reduce overfitting and contribute to better performance.\n\n## CNN Archeticure\ndef CNN():\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]) )\n    model.add(Activation('relu'))\n\n    model.add(Conv2D(32,(3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(32,(3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n\n    model.add(Dense(512))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n\n\n\n# Train model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ncnn = CNN()\n\n\ncnn.fit(train_set,\n        epochs=50,\n        batch_size=64,\n        validation_data=validation_set, shuffle=True)\n\nloss, acc = cnn.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test acc:', acc)"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#leveraging-a-pretrained-model-for-cifar-10",
    "href": "Python/ImageLearningCifar10.html#leveraging-a-pretrained-model-for-cifar-10",
    "title": "Image Learning on CIFAR-10",
    "section": "3. Leveraging a Pretrained Model for CIFAR-10",
    "text": "3. Leveraging a Pretrained Model for CIFAR-10\nResNet50 is known for its high accuracy and relatively efficient parameter count among pretrained models. We should adjust its input dimensions to fit the CIFAR-10 dataset rather than using its default settings. See https://paperswithcode.com/sota/image-classification-on-imagenet to see list of pretrained models and their performance, using ImageNet dataset as benchmark.\nFirst, let’s illustrate a method that is conceptually sound but inefficient for using pretrained models.\n\nResizing layer is added to adjust the image dimensions from the original 32x32 pixels to 224x224 pixels. This step is crucial because ResNet50 was originally trained on the ImageNet dataset, where images have a resolution of 224x224 pixels. By resizing our images to match this resolution, we align them with the input format the model was designed for, ensuring that the features learned from ImageNet are effectively utilized. This alignment helps maintain the integrity of the pretrained model’s architecture and improves its performance on our specific dataset.\nEarly Stopping is employed to enhance training efficiency by halting the training process when the model’s performance ceases to improve on the validation dataset. This prevents unnecessary computations and helps avoid overfitting.\nModelCheckpoint callback is used to save the model’s weights at specific points during training, ensuring that the best-performing model (according to validation metrics) is preserved. This way, if the training process is interrupted or if we need to revert to a previous state, we can load the saved weights and continue from the best checkpoint. Together, these techniques help optimize both the training time and the final performance of the model.\n\n\npretrained_model = ResNet50(input_shape=(224, 224, 3),\n                    include_top=False)\n\npretrained_model.trainable = True\n\nmodel = Sequential([\n          Resizing(224, 224),  # Resize images to 224x224\n            pretrained_model,\n            BatchNormalization(),\n            GlobalAveragePooling2D(),\n            Dense(1024, activation='relu'),\n            Dropout(0.4),\n            Dense(10, activation='softmax', dtype='float32'),\n        ])\n\n# Compile the model with the same optimizer and loss function\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Define early stopping parameters\nearly_stopping_patience = 5 # Stop training if no improvement after 5 epochs\nearly_stopping_min_delta = 0.01 # Stop training if improvement is less than 0.01%\n\n# Create EarlyStopping callback object\nearly_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, min_delta=early_stopping_min_delta)\n\n# Define where to save the best model based on validation accuracy\nbest_model_path = 'best_model.h5'\n\n# Create ModelCheckpoint callback object\nmodel_checkpoint = ModelCheckpoint(best_model_path + '.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n# Continue training from the last saved model\nmodel.fit(train_set,\n        epochs=50,\n        batch_size=64,\n        validation_data=validation_set, shuffle=True,callbacks=[early_stopping, model_checkpoint])\n\n# Evaluate final model on test set\nloss, acc = best_model.evaluate(x_test, y_test, verbose=1)\n\nprint('Test loss:', loss)\nprint('Test acc:', acc)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe achieved a high test score of 0.898, but the question remains: is this the best way to use transfer learning?\nTo determine if our approach to transfer learning is correct, we first need to understand the principles behind it. Transfer learning is designed to enhance efficiency and reduce computational costs by leveraging pretrained models. Instead of retraining a model from scratch, we use an existing model that has already been trained on a large dataset. This typically involves “freezing” the earlier layers of the model, which means keeping their weights unchanged, and only training the final layers on our specific dataset. This strategy allows us to build on the learned features of the pretrained model, thus saving time and computational resources while potentially achieving better performance with less data.\nIn the case above, we had set pretrained_model.trainable = True which we should have set to False to fully leverage the full benefits of a pretrained model.\n\n\nA Better way to utilise a pretrained model: Firstly, instead of incorporating a resizing layer within the model, we should resize our images to 224x224 pixels during the preprocessing step.\n\n\n\n\n\n\nNote\n\n\n\nCan you think of the rationale for this?\n\n\n\nTo be Continued….."
  }
]