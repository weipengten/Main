[
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "href": "Visual_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Main.html",
    "href": "Main.html",
    "title": "Wei Peng’s Analytics Journey",
    "section": "",
    "text": "Welcome! In this website, I will be sharing my learning journey on in analytics."
  },
  {
    "objectID": "Main.html#geospatial-analytics",
    "href": "Main.html#geospatial-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Geospatial Analytics",
    "text": "Geospatial Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Main.html#visual-analytics",
    "href": "Main.html#visual-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Visual Analytics",
    "text": "Visual Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\nTen Wei Peng\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Main.html#tableau",
    "href": "Main.html#tableau",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Tableau",
    "text": "Tableau\n\n\n\n\n\n\n\nQuarterly Private Residential Property Watcher\n\n\nTableau Visualization\n\n\n\n\n\nurl\n\n\nhttps://public.tableau.com/app/profile/wei.peng.ten/viz/in-Class_2/QuaterlyPrivateResidentialPropertyWatcher2023-2024Q1#2\n\n\n\n\nthumbnail\n\n\nimages/quarterly_price.png\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Take-Home_Ex2/Take-Home_Ex2.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "href": "Visual_Analytics/Take-Home_Ex2/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "In this take-home exercise we are required to:\n\nSelect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\nCritic the submission in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices.\nRemake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\nFor this exercise, I have chosen our classmate ZOU JIAXUN’s visualization in his Take-home Exercise 1 submission to critique."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#original-design",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#original-design",
    "title": "Take Home Exercise 2",
    "section": "2.1 2.1 Original Design",
    "text": "2.1 2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "title": "Take Home Exercise 2",
    "section": "2.2 2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 2.2 Critique: Clarity and Aesthetics\n\n2.2.1 Clarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\n2.2.2 Aesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#proposed-sketch",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#proposed-sketch",
    "title": "Take Home Exercise 2",
    "section": "3.1 3.1 Proposed Sketch",
    "text": "3.1 3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#data-preparation",
    "href": "Visual_Analytics/Take-Home_Ex02/Take-Home_Ex02.html#data-preparation",
    "title": "Take Home Exercise 2",
    "section": "3.2 3.2 Data Preparation",
    "text": "3.2 3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\n3.2.1 Original data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\n3.2.2 Modifying the prepared data\n\n3.2.2.1 Binning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\n3.2.2.2 Base Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\n3.2.2.3 Median Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\n3.2.2.4 Transaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\n3.2.2.5 Total Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\n3.2.2.6 Joining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\n3.2.2.7 Signal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "Main.html#analysis-using-rstudio",
    "href": "Main.html#analysis-using-rstudio",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Analysis using RStudio",
    "text": "Analysis using RStudio"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex2.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "In this take-home exercise we are required to:\n\nSelect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\nCritic the submission in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices.\nRemake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\nFor this exercise, I have chosen our classmate ZOU JIAXUN’s visualization in his Take-home Exercise 1 submission to critique."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#original-design",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#original-design",
    "title": "Take Home Exercise 2",
    "section": "2.1 2.1 Original Design",
    "text": "2.1 2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#critique-clarity-and-aesthetics",
    "title": "Take Home Exercise 2",
    "section": "2.2 2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 2.2 Critique: Clarity and Aesthetics\n\n2.2.1 Clarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\n2.2.2 Aesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#proposed-sketch",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#proposed-sketch",
    "title": "Take Home Exercise 2",
    "section": "3.1 3.1 Proposed Sketch",
    "text": "3.1 3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#data-preparation",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Take-Home_Ex02.html#data-preparation",
    "title": "Take Home Exercise 2",
    "section": "3.2 3.2 Data Preparation",
    "text": "3.2 3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\n3.2.1 Original data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\n3.2.2 Modifying the prepared data\n\n3.2.2.1 Binning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\n3.2.2.2 Base Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\n3.2.2.3 Median Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\n3.2.2.4 Transaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\n3.2.2.5 Total Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\n3.2.2.6 Joining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\n3.2.2.7 Signal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wei Peng’s Analytics Journey",
    "section": "",
    "text": "Welcome! In this website, I will be sharing my learning journey in analytics."
  },
  {
    "objectID": "index.html#geospatial-analytics",
    "href": "index.html#geospatial-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Geospatial Analytics",
    "text": "Geospatial Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#visual-analytics",
    "href": "index.html#visual-analytics",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Visual Analytics",
    "text": "Visual Analytics\n\n\n\n\n\n\n\n\nTake Home Exercise 2\n\n\n\n\n\n\nTen Wei Peng\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#tableau",
    "href": "index.html#tableau",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Tableau",
    "text": "Tableau\n\n\n\n\n\n\n\n\nBullet Graph and Sparkline\n\n\nTableau Visualization\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMapping in Tableau\n\n\nTableau Visualization\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuarterly Private Residential Property Watcher\n\n\nTableau Visualization\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "3.1 Open Government Data",
    "text": "3.1 Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "3.2 Specially collected data",
    "text": "3.2 Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "4.1 Geospatial Data Science",
    "text": "4.1 Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "4.2 Spatial Interaction Modelling",
    "text": "4.2 Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "index.html#analysis-using-rstudio",
    "href": "index.html#analysis-using-rstudio",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Analysis using RStudio",
    "text": "Analysis using RStudio\n\nRStudio\n\n\n\n\n\n\n\n\nAnalyzing Bus Traffic Flows in Singapore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Better Visualisations\n\n\n\n\n\n\nTen Wei Peng\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html",
    "title": "Creating Better Visualisations",
    "section": "",
    "text": "In this take-home exercise we are required to:\n\nSelect one data visualisation from the Take-home Exercise 1 submission prepared by your classmate,\nCritic the submission in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices.\nRemake the original design by using ggplot2, ggplot2 extensions and tidyverse packages.\n\nFor this exercise, I have chosen our classmate ZOU JIAXUN’s visualization in his Take-home Exercise 1 submission to critique."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "title": "Creating Better Visualisations",
    "section": "2.1 Original Design",
    "text": "2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "title": "Creating Better Visualisations",
    "section": "2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 Critique: Clarity and Aesthetics\n\nClarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\nAesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "title": "Creating Better Visualisations",
    "section": "3.1 Proposed Sketch",
    "text": "3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "href": "Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "title": "Creating Better Visualisations",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\nOriginal data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\nModifying the prepared data\n\nBinning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\nBase Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\nMedian Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\nTransaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\nTotal Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\nJoining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\nSignal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Open Government Data",
    "text": "Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Specially collected data",
    "text": "Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Geospatial Data Science",
    "text": "Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "href": "Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Spatial Interaction Modelling",
    "text": "Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "index.html#rstudio",
    "href": "index.html#rstudio",
    "title": "Wei Peng’s Analytics Journey",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\n\n\nAnalyzing Bus Traffic Flows in Singapore\n\n\nGeospatial Analytics\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Better Visualisations\n\n\nVisual Analytics\n\n\n\nTen Wei Peng\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore\n\n\nGeospatial Analytics\n\n\n\nTen Wei Peng\n\n\nJul 22, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Tableau/Tableau_Visualization.html",
    "href": "Tableau/Tableau_Visualization.html",
    "title": "Quarterly Private Residential Property Watcher",
    "section": "",
    "text": "Quarterly Private Residential Property Watcher\n\nView Visualization"
  },
  {
    "objectID": "Tableau/Quarterly_Private_Residential_Property_Watcher.html",
    "href": "Tableau/Quarterly_Private_Residential_Property_Watcher.html",
    "title": "Quarterly Private Residential Property Watcher",
    "section": "",
    "text": "Quarterly Private Residential Property Watcher"
  },
  {
    "objectID": "Tableau/Mapping_In_Tableau.html",
    "href": "Tableau/Mapping_In_Tableau.html",
    "title": "Mapping in Tableau",
    "section": "",
    "text": "Tableau utilizes geocoding services to convert geographical data, such as postal codes into latitude and longitude coordinates. This simplifies the process since we do not need to include these coordinates in the dataset. However, it’s important to remember that the accuracy of your geodata depends on the latest update of Tableau’s geocoding database.\n\nLatitude and Longitude for PropSymbol map: generated via Postal Code\nLatitude and Longitude for Treemap: generated via Planning Region"
  },
  {
    "objectID": "Tableau/Bullet_Graph_and_Sparkline.html",
    "href": "Tableau/Bullet_Graph_and_Sparkline.html",
    "title": "Bullet Graph and Sparkline",
    "section": "",
    "text": "Bullet Graph and Sparklines are perfect to include in dashboards where space is limited but performance metrics need to be displayed clearly."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Wei Peng’s Analytics Journey",
    "section": "Python",
    "text": "Python\n\n\n\n\n\n\n\n\nAutoML for Binary Classification\n\n\nAutomated Machine Learning (In-Progress)\n\n\n\nTen Wei Peng\n\n\nAug 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImage Learning on CIFAR-10\n\n\nDeep Learning with CIFAR-10\n\n\n\nTen Wei Peng\n\n\nJul 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html",
    "href": "Python/ImageLearningCifar10.html",
    "title": "Image Learning on CIFAR-10",
    "section": "",
    "text": "mic# Testing Python in Quarto\nThis document demonstrates how to include and execute Python code in a Quarto document.\n\n# This is a Python code chunk\nimport numpy as np"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#open-government-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Open Government Data",
    "text": "Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#specially-collected-data",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Specially collected data",
    "text": "Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#geospatial-data-science",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Geospatial Data Science",
    "text": "Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html#spatial-interaction-modelling",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "Spatial Interaction Modelling",
    "text": "Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#original-design",
    "title": "Creating Better Visualisations",
    "section": "2.1 Original Design",
    "text": "2.1 Original Design\nFigure 1 below shows the original design of the Unit Price ($PSF) for Private Properties in Singapore during the first quarter of 2024."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#critique-clarity-and-aesthetics",
    "title": "Creating Better Visualisations",
    "section": "2.2 Critique: Clarity and Aesthetics",
    "text": "2.2 Critique: Clarity and Aesthetics\n\nClarity\n\nThis chart fails to demonstrate several observations made by our classmate:\n“…a clear disparity emerges between the average unit prices of condominiums and apartments, standing at approximately $1,500 and $2,000, respectively, for the period spanning January to March…”\n\nThe chart doesn’t provide clear information about the average unit prices of condominiums and apartments. The absence of labels, compounded by the use of a violin plot that highlights distribution, adds to the complexity.\n\n“…Noteworthy is the discernible uptick in both unit price and transaction volume from January to March 2024…”\n\nTransaction volume is particularly challenging to discern compared to unit price because the y-axis is focused on describing unit prices.\n\n“…Despite an overall reduction in total transactions vis-a-vis the preceding year, there is an unmistakable trend towards growth within specific sub-markets, suggesting an increasing inclination towards higher-value properties…”\n\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly..\n\nOther concerns:\nThe title provides a general description but lacks specificity. It does not specify the time period covered by the visualization, nor does it offer insight into the context or main focus of the visualization. A more detailed title and subtitle would enhance clarity and understanding for the audience.\nThe x-axis represents the three months of Q1 2024 instead of displaying transaction volume, a metric frequently referenced by the author. Utilizing the x-axis to depict transaction volume would enhance clarity significantly.\n\n\n\nAesthetics\n\nThe color selection is problematic due to the light-colored background, which matches the hue of the violin plot. This makes it strenuous for the eyes to distinguish between elements.\nFonts, titles, x-ticks, and legends are small and difficult to discern."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#proposed-sketch",
    "title": "Creating Better Visualisations",
    "section": "3.1 Proposed Sketch",
    "text": "3.1 Proposed Sketch\nA violin is often not well understood by the general public. In this case, the use of a violin plot is worse as the writer intends to make several comparisons that are not obvious from said plot.\nThis scenario seems appropriate for a similar makeover done by Michael Djohan for another student.\nAcknowledging that point plots may distract and add little value, we omitted them. Instead, a modified bar chart resembling a violin plot was adopted. This alternative retains the visual appeal of a violin plot while providing clearer and more informative comparisons. We also decided to add annotations for median prices and transaction volume. A facet plot is used to distinctively compare between the three months for Q1 2024."
  },
  {
    "objectID": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "href": "RStudio/Visual_Analytics/Creating_Better_Visualisations/Creating_Better_Visualisations.html#data-preparation",
    "title": "Creating Better Visualisations",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\nLet us first load the files and merge them into combined_data #Load packages\n\n\nShow the code\n#Load packages\npacman::p_load(cowplot, scales, patchwork, tidyverse)\n\n#Import data\nfile_list &lt;- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\ncombined_data &lt;- file_list %&gt;%\n  map_df(read_csv)\n\n\n\nOriginal data preparation done by student\nIn this chunk of code, we adopted exactly similar modifications to the data to ensure perfect replication. Nothing new is added here.\nThe data preparation here involves:\n\nformatting the Sale Date column into Quarters and Months using the dmy() function from the lubridate package.\nRemoving NA Values\nFilter for Q1 2024 data only\nFilter for “Apartment” and “Condominium” only\n\n\n\nShow the code\n# filtering the data\nfiltered_data &lt;- combined_data %&gt;%\n  mutate(Sale_Date = dmy(`Sale Date`)) %&gt;%\n  filter((year(Sale_Date) == 2023 & \n          month(Sale_Date) %in% 1:12) |\n         (year(Sale_Date) == 2024 & \n          month(Sale_Date) %in% 1:3)) %&gt;%\n  mutate(Quarter_Sale_Data = case_when(\n    between(Sale_Date, as.Date(\"2023-01-01\"), as.Date(\"2023-03-31\")) ~ \"Q1_2023\",\n    between(Sale_Date, as.Date(\"2023-04-01\"), as.Date(\"2023-06-30\")) ~ \"Q2_2023\",\n    between(Sale_Date, as.Date(\"2023-07-01\"), as.Date(\"2023-09-30\")) ~ \"Q3_2023\",\n    between(Sale_Date, as.Date(\"2023-10-01\"), as.Date(\"2023-12-31\")) ~ \"Q4_2023\",\n    between(Sale_Date, as.Date(\"2024-01-01\"), as.Date(\"2024-03-31\")) ~ \"Q1_2024\",\n    TRUE ~ NA_character_\n  )) %&gt;%\n  filter(!is.na(Quarter_Sale_Data)) %&gt;%\n  filter(Quarter_Sale_Data == \"Q1_2024\") %&gt;%\n  mutate(Month_Sale_Data = paste0(year(Sale_Date), \"-\", month(Sale_Date))) \n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(`Property Type` %in% c(\"Apartment\", \"Condominium\"))\n\n\n\n\nModifying the prepared data\n\nBinning for Unit Price ($ PSF)\nFrom here on, we begin to make our own modifications which are plenty and necessary. Firstly, we realize that we construct our desired plot we need our Unit Price ($ PSF)to be categorized into bins as Unit_Price_Bin\n\n\nShow the code\n# Creating bins\nbreaks &lt;- c(750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(!is.na(`Unit Price ($ PSF)`)) %&gt;%\n  filter(`Unit Price ($ PSF)` &lt;= 4000) %&gt;%\n  mutate(Unit_Price_Bin = cut(`Unit Price ($ PSF)`, \n                              breaks = breaks, \n                              labels = c(\"751-1000\", \"1001-1250\", \"1251-1500\", \"1501-1750\", \"1751-2000\", \"2001-2250\", \"2251-2500\", \"2501-2750\", \"2751-3000\", \"3001-3250\", \"3251-3500\", \"3501-3750\", \"3751-4000\"), \n                              include.lowest = TRUE,\n                              right = FALSE)) %&gt;%\n  filter(!is.na(Unit_Price_Bin))\n\n\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\nwrite_rds(filtered_data, \"data/rds/filtered_data.rds\")\nfiltered_data &lt;- read_rds(\"data/rds/filtered_data.rds\")\n\n\n\nBase Data Creation:\nThe code creates a base dataset (base_data) by grouping the filtered data (filtered_data) by month of sale, unit price bin, and property type. It calculates the count of transactions (Count) and assigns negative values to transactions related to apartments (Trans_Vol) to prepare for a pyramid chart.\n\n\nShow the code\n# Creating base data for pyramid\nbase_data &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(Count = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(Trans_Vol = ifelse(`Property Type` == \"Apartment\", Count * (-1), Count))\n\n\n\n\nMedian Unit Price Calculation:\nIt computes the median unit price (median_unit_price) for each month and property type combination, adjusting the values for apartments to negative if necessary.\n\n\nShow the code\n# median Unit Price\nUnitPrice_median &lt;- filtered_data %&gt;%\n  group_by(Month_Sale_Data, `Property Type`) %&gt;%\n  summarise(median_unit_price = median(`Unit Price ($ PSF)`)) %&gt;%\n  mutate(median_unit_price = ifelse(`Property Type` == \"Apartment\", median_unit_price * (-1), median_unit_price)) %&gt;%\n  ungroup()\n\n\n\n\nTransaction Volume Aggregation:\nThe code calculates the maximum and minimum transaction volumes (max_vol, min_vol) for each month and property type, as well as the total transaction volume for each month (sum_transvolume_month).\n\n# transvolume_maxmin as transaction volume by month\ntransvolume_maxmin &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data,`Property Type`) %&gt;%\n  summarise(max_vol = max(abs(Trans_Vol)),\n            min_vol = min(abs(Trans_Vol))) %&gt;%\n  ungroup()\n\n# transvolume_month as total volume by month \ntransvolume_month &lt;- base_data %&gt;%\n  group_by(Month_Sale_Data) %&gt;%\n  summarise(sum_transvolume_month = sum(abs(Trans_Vol))) %&gt;% \n  ungroup()\n\n\n\nTotal Transaction Volume by Property Type:\nIt determines the total transaction volume for each unit price bin and property type combination (sum_transvolume_property).\n\n# transvolume_property as total transaction volume by property type\ntransvolume_property &lt;- base_data %&gt;%\n  group_by(Unit_Price_Bin, `Property Type`) %&gt;%\n  summarise(sum_transvolume_property = sum(Trans_Vol)) %&gt;%\n  ungroup()\n\n\n\nJoining Tables\n\n# joining multiple tables\ntransaction_data &lt;- base_data %&gt;%\n  left_join(transvolume_maxmin, by = c('Month_Sale_Data', 'Property Type')) %&gt;%\n  left_join(transvolume_month, by = 'Month_Sale_Data') %&gt;%\n  left_join(transvolume_property, by = c('Unit_Price_Bin', 'Property Type')) %&gt;%\n  left_join(UnitPrice_median, by = c('Month_Sale_Data', 'Property Type'))\n\ntransaction_data$Unit_Price_Bin &lt;- factor(transaction_data$Unit_Price_Bin, exclude = NULL)\n\n\n\nSignal Creation:\nThe code generates signals (signal, signal1) to indicate whether a transaction volume or median unit price falls within certain ranges, differentiating between property types. These signals are based on conditions comparing transaction volumes and median unit prices with predefined ranges.\n\n\nShow the code\ntransaction_data &lt;- transaction_data %&gt;%\n   mutate(signal = case_when((min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Apartment\" ~ -1,\n                             (min_vol == abs(Trans_Vol) | max_vol == abs(Trans_Vol)) & `Property Type` == \"Condominium\" ~ 1, TRUE~0)) %&gt;%\n\n# Assuming `Unit_Price_Bin` is a factor with ordered levels representing price ranges\n\nmutate(signal1 = case_when(\n    `Property Type` == \"Apartment\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ -1,\n    \n    `Property Type` == \"Condominium\" & abs(median_unit_price) &gt;= as.numeric(sub(\"\\\\-(\\\\d+)$\", \"\", Unit_Price_Bin)) & \n    abs(median_unit_price) &lt;= as.numeric(sub(\"^\\\\d+\\\\-\", \"\", Unit_Price_Bin)) ~ 1,\n    \n    TRUE ~ 0\n))\n\n\n#Order factors\ntransaction_data$`Property Type` &lt;- fct_relevel(transaction_data$`Property Type`, \"Apartment\")\ntransaction_data$Unit_Price_Bin  &lt;- fct_relevel(transaction_data$Unit_Price_Bin, \"751-1000\", \"1001-1250\")"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#open-government-data",
    "title": "Take Home Exercise 2",
    "section": "Open Government Data",
    "text": "Open Government Data\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#specially-collected-data",
    "title": "Take Home Exercise 2",
    "section": "Specially collected data",
    "text": "Specially collected data\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#geospatial-data-science",
    "title": "Take Home Exercise 2",
    "section": "Geospatial Data Science",
    "text": "Geospatial Data Science\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).\nDescribe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).\nAssemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.\nCompute a distance matrix by using the analytical hexagon data derived earlier."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Take-Home_Ex02.knit.html#spatial-interaction-modelling",
    "title": "Take Home Exercise 2",
    "section": "Spatial Interaction Modelling",
    "text": "Spatial Interaction Modelling\n\nCalibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.\nPresent the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)\nWith reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual)."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/FinServ.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/entertn.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Business.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/F&B.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Liesure&Recreation.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/data/geospatial/Retails.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#introduction",
    "href": "Python/ImageLearningCifar10.html#introduction",
    "title": "Image Learning on CIFAR-10",
    "section": "Introduction",
    "text": "Introduction\nHello everyone, this post is intended to provide an in-depth comparison and discussion of various methods for image learning. I will add more detailed explanations as soon as I have the time.\nChangelog:\n\\[1.0.0\\] - 2024-07-20 - Initial deployment after consolidating course work and initial refinement.\nThis page demonstrates how to perform an image classification task using:\n\nMulti-Layer Perceptron (MLP)\nConvolutional Neural Network (CNN)\nTransfer Learning using Resnet50 (in-progress)\n\nfor the CIFAR-10 dataset as a demonstration"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#importing-libraries",
    "href": "Python/ImageLearningCifar10.html#importing-libraries",
    "title": "Image Learning on CIFAR-10",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#Data preprocessing and modeling related functions\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n#Cross-validation and evaluation related functions\nfrom sklearn.model_selection import KFold\n\n#Datamodeling related functions\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import SGD\nfrom sklearn import model_selection, preprocessing, metrics\nfrom keras.applications import VGG16, ResNet50, InceptionV3"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#exploring-the-dataset",
    "href": "Python/ImageLearningCifar10.html#exploring-the-dataset",
    "title": "Image Learning on CIFAR-10",
    "section": "Exploring the dataset",
    "text": "Exploring the dataset\nThis Python code snippet uses TensorFlow’s Keras API to load the CIFAR-10 dataset and print the shapes of the training and test sets. The CIFAR-10 dataset is a popular dataset used for training machine learning and computer vision algorithms. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class.\n\n# Load the CIFAR-10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nprint(\"Training set shape:\", x_train.shape)\nprint(\"Test set shape:\", x_test.shape)\n\n\n\n\nThe x_train.shape returns (50000, 32, 32, 3), indicating that there are 50,000 training images, each of size 32x32 pixels with 3 color channels (RGB).\nThe x_test.shape returns (10000, 32, 32, 3), indicating that there are 10,000 test images, each of size 32x32 pixels with 3 color channels (RGB).\n\n\nThis code will display the first 10 images from the CIFAR-10 training set with their respective classes.\n\nimages = range(0,10)\nclasses = list(np.unique(y_train))\nplt.figure(figsize=(20,10))\nfor i in images:\n        plt.subplot(2,5,1 + i).set_title(classes[y_train[i][0]])\n        plt.imshow(x_train[i])\n\n\nNext, let’s reshape the arrays to have only 2 dimensions\n\nx_train = x_train.reshape(50000, 32 * 32 * 3)\nx_test  = x_test.reshape (10000, 32 * 32 * 3)\n\nprint(\"Training set shape:\", x_train.shape)\nprint(\"Test set shape:\", x_test.shape)\n\n\n\nBy reshaping the images into (50000, 3072) and (10000, 3072), each image is now represented as a 1-dimensional array (vector) of length 3072. This reshaping is commonly done to prepare the data for certain machine learning algorithms that expect input as 1-dimensional arrays rather than 3-dimensional images.\n\nNext we perform the necessary pre-processing steps:\nSplitting the Training Dataset: The training dataset is split into a new training set and a validation set using an 80/20 split to evaluate the model during training. The train_test_split function is used with test_size=0.2 and random_state=42 for reproducibility.\nData Type Conversion: The feature arrays are converted to float32 data type to ensure compatibility with TensorFlow\n\n## Splitting \"training\" dataset further into train,validation datasets\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n\nx_train = x_train.astype('float32')\nx_valid = x_valid.astype('float32')\nx_test = x_test.astype('float32')\n\nMin-Max Scaling: . A Min-Max scaling function normalizes the pixel values of the images to the range [0, 1], which helps in faster and more stable training of the model.\n\n## Transform method: MinMax - is preferred when working with TensorFlow\ndef min_max_scaler(x_train, x_valid, x_test):\n  x_train_ms= x_train/255\n  x_valid_ms= x_valid/255\n  x_test_ms = x_test/255\n  return x_train_ms,x_valid_ms,x_test_ms\n\nx_train_ms, x_valid_ms, x_test_ms = min_max_scaler(x_train, x_valid, x_test)\n\nOne-Hot Encoding the Labels: The class labels are converted to one-hot encoded vectors using to_categorical, which transforms the integer labels into binary vectors. This format is required for categorical classification tasks where the neural network’s output layer expects binary vectors.\n\n# By converting the target variables to one-hot format, we can ensure that they are compatible with the output layer of the MLP model, which expects the target variables to be represented as a vector of binary values.\ny_train_1hot = to_categorical(y_train, 10)\ny_valid_1hot = to_categorical(y_valid, 10)\ny_test_1hot  = to_categorical(y_test , 10)"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#implementing-a-multi-layer-perceptron-for-cifar-10-classification",
    "href": "Python/ImageLearningCifar10.html#implementing-a-multi-layer-perceptron-for-cifar-10-classification",
    "title": "Image Learning on CIFAR-10",
    "section": "1. Implementing a Multi-Layer Perceptron for CIFAR-10 Classification",
    "text": "1. Implementing a Multi-Layer Perceptron for CIFAR-10 Classification\nMLP with one hidden layer consisting of 256 neurons,sgd\n\nOptimizer: SGD\nLoss: Cross Entropy\nHidden Layers: 1 with 256 neurons\nActivation Layer: (ReLU, softmax)\n\n\n# Define the model architecture\nmodel = keras.Sequential([\n    # Input Layer\n    keras.layers.Dense(512, activation='relu', input_shape=(32*32*3,)),\n\n    # Hidden Layers\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dropout(0.2),\n\n    # Output Layer\n    keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='SGD',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model using mini-batch learning\nbatch_size = 64\nepochs = 40\npatience = 5\nbest_acc = 0\nfor epoch in range(epochs):\n    for i in range(0, len(x_train_ms), batch_size):\n        batch_x, batch_y = x_train_ms[i:i+batch_size], y_train_1hot[i:i+batch_size]\n        model.train_on_batch(batch_x, batch_y)\n\n    # Evaluate the model on the validation set after each epoch\n    val_loss, val_acc = model.evaluate(x_valid_ms, y_valid_1hot)\n    print('Epoch %d: validation accuracy=%f' % (epoch+1, val_acc))\n\n    # Check if the validation accuracy has improved\n    if val_acc &gt; best_acc:\n        best_acc = val_acc\n        patience = 0\n    else:\n        patience += 1\n        print(\"patience =\",patience)\n\n    # Stop training if the validation accuracy does not improve after a certain number of epochs\n    if patience == 5:\n        break\n\n# Evaluate final model on test set\ntest_pred = model.predict(x_test_ms)\ntest_accuracy = accuracy_score(np.argmax(y_test_1hot, axis=1), np.argmax(test_pred, axis=1))\n\nprint(\"Test Accuracy of MLP, 1 Hidden Layer with 256 neurons, SGD, ReLu = \",round(test_accuracy,2))"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#implementing-a-convolutional-neural-network-cnn-for-cifar-10-classification",
    "href": "Python/ImageLearningCifar10.html#implementing-a-convolutional-neural-network-cnn-for-cifar-10-classification",
    "title": "Image Learning on CIFAR-10",
    "section": "2. Implementing a Convolutional Neural Network (CNN) for CIFAR-10 Classification",
    "text": "2. Implementing a Convolutional Neural Network (CNN) for CIFAR-10 Classification\nPreprocessing\nFirst off, let’s reproduce some of the preprocessing steps we did earlier:\n\n# Load Data and Split into Train, Test dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Onehot Encoding\ny_train = to_categorical(y_train, 10)\ny_test  = to_categorical(y_test , 10)\n\n# Convert to float datatype\nx_train = x_train.astype('float32')\nx_test  = x_test.astype('float32')\n\n# Minmax Scaling\nx_train  /= 255\nx_test   /= 255\n\nData Augmentation, Manual Splitting and Batch Processing\nNext, this set of additional steps are required for preprocessing for CNN\n\n# Data Augmentation\ntransform_train = ImageDataGenerator(\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        rotation_range = 10,\n        horizontal_flip=True)   # flip images horizontally\n\nvalidation_train = ImageDataGenerator()\n\n# Split Train dataset to get Validation dataset\nvalidation_train = ImageDataGenerator()\ntrain_set = transform_train.flow(x_train[:40000], y_train[:40000], batch_size=32)\nvalidation_set = validation_train.flow(x_train[40000:], y_train[40000:], batch_size=32)\n\n\nWhy do we need additional preprocessing steps for CNN?\nConvolutional Neural Networks (CNNs) are specifically designed to work with image data. To enhance their performance, particularly when working with relatively small datasets like CIFAR-10, additional preprocessing steps are often employed. These steps help improve the model’s ability to generalize to unseen data and mitigate overfitting.\n\nData Augmentation: Increases the diversity of the training dataset by applying random transformations such as shifts, rotations, and flips. This helps the CNN model generalize better by training on a wider variety of image presentations. This is different from MLP earlier as MLPs typically work with flattened input data and do not benefit as much from spatial data augmentation\nManual Splitting and Batch Processing: The ImageDataGenerator in TensorFlow/Keras handles data augmentation and requires the data to be in a specific format (i.e., batches). Hence, the split is done manually to apply augmentation on the training set while keeping the validation set unchanged. CNNs typically require batch processing for efficient training. The flow method from ImageDataGeneratorcreates an iterator that generates batches of augmented data on-the-fly. This is especially useful when working with large datasets or when applying data augmentation. Manual splitting allows for better control over how data is augmented and fed into the model. By explicitly defining the training and validation datasets, we ensure that augmentation is applied only to the training data, not the validation data.\n\n\n\nBuilding a Convolutional Neural Network for CIFAR-10!\n\nBatch Size: 64\nEpochs: 50\nPadding: Same\nKernel: 3*3\n(Convolution-BatchNormalisation-Relu-MaxPooling) x 2\nLoss Function: Categorical Crossentropy\nDropout: 0.25,0.25,0.5\n\n\nPadding: Same: This means that the output spatial dimensions will be the same as the input spatial dimensions after applying the convolution operation. This is achieved by adding zeros around the borders of the input tensor so that the output spatial dimensions match the input spatial dimensions.\n\n\nKernel: 3x3: A kernel size of 3x3 is a standard choice for convolutional neural networks because it provides a good balance between capturing local patterns and not overfitting to the training data. Larger kernel sizes can capture more complex patterns but may lead to overfitting, while smaller kernel sizes may not capture enough information.\n\n\n(Convolution-BatchNormalisation-Relu-MaxPooling) x 2: This sequence of layers is commonly used in convolutional neural networks because it allows the model to learn increasingly complex features while reducing overfitting.\n\n\nBatch normalization normalize the activations to improve generalization\n\n\nReLU activation functions introduce non-linearity to the model\n\n\nMax pooling layers downsample the spatial dimensions to reduce computational complexity and help the model learn spatial hierarchies.\n\n\nLoss Function: Categorical Crossentropy: This loss function is commonly used for multi-class classification problems like image classification. It measures the difference between the predicted probabilities and the true labels by calculating the negative log likelihood of the true class. This encourages the model to output high probabilities for the correct classes and low probabilities for the incorrect classe\n\n\nDropout is a regularization technique that helps prevent overfitting by randomly setting some of the neurons to zero during training. By using a high dropout rate (0.25) and (0.5), it is likely to reduce overfitting and contribute to better performance.\n\n## CNN Archeticure\ndef CNN():\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]) )\n    model.add(Activation('relu'))\n\n    model.add(Conv2D(32,(3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(32,(3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n\n    model.add(Dense(512))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n\n\n\n# Train model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\ncnn = CNN()\n\n\ncnn.fit(train_set,\n        epochs=50,\n        batch_size=64,\n        validation_data=validation_set, shuffle=True)\n\nloss, acc = cnn.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', loss)\nprint('Test acc:', acc)"
  },
  {
    "objectID": "Python/ImageLearningCifar10.html#leveraging-a-pretrained-model-for-cifar-10",
    "href": "Python/ImageLearningCifar10.html#leveraging-a-pretrained-model-for-cifar-10",
    "title": "Image Learning on CIFAR-10",
    "section": "3. Leveraging a Pretrained Model for CIFAR-10",
    "text": "3. Leveraging a Pretrained Model for CIFAR-10\nResNet50 is known for its high accuracy and relatively efficient parameter count among pretrained models. We should adjust its input dimensions to fit the CIFAR-10 dataset rather than using its default settings. See https://paperswithcode.com/sota/image-classification-on-imagenet to see list of pretrained models and their performance, using ImageNet dataset as benchmark.\nFirst, let’s illustrate a method that is conceptually sound but inefficient for using pretrained models.\n\nResizing layer is added to adjust the image dimensions from the original 32x32 pixels to 224x224 pixels. This step is crucial because ResNet50 was originally trained on the ImageNet dataset, where images have a resolution of 224x224 pixels. By resizing our images to match this resolution, we align them with the input format the model was designed for, ensuring that the features learned from ImageNet are effectively utilized. This alignment helps maintain the integrity of the pretrained model’s architecture and improves its performance on our specific dataset.\nEarly Stopping is employed to enhance training efficiency by halting the training process when the model’s performance ceases to improve on the validation dataset. This prevents unnecessary computations and helps avoid overfitting.\nModelCheckpoint callback is used to save the model’s weights at specific points during training, ensuring that the best-performing model (according to validation metrics) is preserved. This way, if the training process is interrupted or if we need to revert to a previous state, we can load the saved weights and continue from the best checkpoint. Together, these techniques help optimize both the training time and the final performance of the model.\n\n\npretrained_model = ResNet50(input_shape=(224, 224, 3),\n                    include_top=False)\n\npretrained_model.trainable = True\n\nmodel = Sequential([\n          Resizing(224, 224),  # Resize images to 224x224\n            pretrained_model,\n            BatchNormalization(),\n            GlobalAveragePooling2D(),\n            Dense(1024, activation='relu'),\n            Dropout(0.4),\n            Dense(10, activation='softmax', dtype='float32'),\n        ])\n\n# Compile the model with the same optimizer and loss function\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n# Define early stopping parameters\nearly_stopping_patience = 5 # Stop training if no improvement after 5 epochs\nearly_stopping_min_delta = 0.01 # Stop training if improvement is less than 0.01%\n\n# Create EarlyStopping callback object\nearly_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, min_delta=early_stopping_min_delta)\n\n# Define where to save the best model based on validation accuracy\nbest_model_path = 'best_model.h5'\n\n# Create ModelCheckpoint callback object\nmodel_checkpoint = ModelCheckpoint(best_model_path + '.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n# Continue training from the last saved model\nmodel.fit(train_set,\n        epochs=50,\n        batch_size=64,\n        validation_data=validation_set, shuffle=True,callbacks=[early_stopping, model_checkpoint])\n\n# Evaluate final model on test set\nloss, acc = best_model.evaluate(x_test, y_test, verbose=1)\n\nprint('Test loss:', loss)\nprint('Test acc:', acc)\n\n\n\n\nWe achieved a high test score of 0.898, but the question remains: is this the best way to use transfer learning?\nTo determine if our approach to transfer learning is correct, we first need to understand the principles behind it. Transfer learning is designed to enhance efficiency and reduce computational costs by leveraging pretrained models. Instead of retraining a model from scratch, we use an existing model that has already been trained on a large dataset. This typically involves “freezing” the earlier layers of the model, which means keeping their weights unchanged, and only training the final layers on our specific dataset. This strategy allows us to build on the learned features of the pretrained model, thus saving time and computational resources while potentially achieving better performance with less data.\nIn the case above, we had set pretrained_model.trainable = True which we should have set to False to fully leverage the full benefits of a pretrained model.\n\nA Better way to utilise a pretrained model: Firstly, instead of incorporating a resizing layer within the model, we should resize our images to 224x224 pixels during the preprocessing step.\n\nCan you think of the rationale for this?\n\n\nTo be Continued….."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html",
    "href": "RStudio/Geospatial_Analytics/Analyzing_Bus_Traffic_Flows/Analyzing_Bus_Traffic_Flows.html",
    "title": "Analyzing Bus Traffic Flows in Singapore",
    "section": "",
    "text": "Changelog:\n\\[1.0.0\\] - 2024-07-10 - Initial deployment. \\[1.0.1\\] - 2024-07-22 - Removal of Origin-Constrained and Doubly COnstrained Models."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "",
    "text": "Changelog:\n\\[1.0.0\\] - 2024-07-23 - Initial build"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#overview",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#overview",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "Overview",
    "text": "Overview\nIn the era of digital urbanization, city-wide infrastructures, encompassing transportation modes like buses, taxis, and mass transit, have undergone significant digitization. This transformation has yielded extensive datasets that serve as a fundamental framework for monitoring movement patterns across both space and time. This shift is particularly evident with the widespread adoption of pervasive computing technologies, including GPS and RFID, notably integrated into vehicles. For instance, the utilization of smart cards and GPS devices on public buses enables the collection of comprehensive data on routes and ridership. Within these vast datasets lie inherent structures and patterns that offer valuable insights into the characteristics of measured phenomena, providing a deeper understanding of human movement and behaviors within urban environments. The identification, analysis, and comparison of these patterns present opportunities for enhanced urban management, offering valuable information for both public and private urban transport service providers. Despite these possibilities, current practices often restrict the use of massive locational data to basic tracking and mapping through Geographic Information System (GIS) applications. This limitation arises from the insufficient capabilities of conventional GIS in effectively analyzing and modeling spatial and spatio-temporal data."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#objective",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#objective",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "Objective",
    "text": "Objective\nIn this study, we first perform\n\nExploratory Spatial Data Analysis (ESDA) to provide us an understanding of the movement patterns on a high level before proceeding to with either\nLocal Indicators of Spatial Association (LISA)\n\nto undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore in detail."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#task",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#task",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "Task",
    "text": "Task\nThe specific tasks of this take-home exercise are as follows:\n\nGeovisualisation and Analysis\n\nWith reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the geographical distribution of the passenger trips by using appropriate geovisualisation methods,\nDescribe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).\n\n\n\nLocal Indicators of Spatial Association (LISA) Analysis\n\nCompute LISA of the passengers trips generate by origin at hexagon level.\nDisplay the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value &lt; 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 200 words per visual)."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#the-data",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#the-data",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "The Data",
    "text": "The Data\n\nAspatial data\nFor the purpose of this take-home exercise, Passenger Volume by Origin Destination Bus Stops downloaded from LTA DataMall will be used.\n\n\nGeospatial data\nTwo geospatial data will be used in this study, they are:\n\nBus Stop Location from LTA DataMall. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\nMaster Plan 2019 Planning Sub-zone (No Sea) GIS data set of URA from data.gov.sg"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#install-r-package",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#install-r-package",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "Install R Package",
    "text": "Install R Package\n\npacman::p_load(sf, sfdep, magrittr, tidyverse, tmap, knitr, RColorBrewer, viridis)"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#importing-data",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#importing-data",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "1. Importing Data",
    "text": "1. Importing Data\nWe will import the data as a first step before proceeding with data cleaning, data wrangling and data exploration for the following:\n\nPassengerVolume, a csv file,\nBusStop, a point feature layer ESRI shapefile format\n\n\nPassenger VolumeBus Stop Location\n\n\nPassengerVolume is an aspatial data, we can import the data simply by using the read_csv function from tidyverse package and output it as a tibble dataframe called odbus\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nRows: 5694297 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): YEAR_MONTH, DAY_TYPE, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE\ndbl (2): TIME_PER_HOUR, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nBus Stop is a geospatial data in .shp file. We save it as a sf data frame called busstop using the st_read function of the sf package. The data is then geo-referenced to coordinates from the Singapore SVY21 coordinate system (EPSG: 3414)\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `BusStop' from data source \n  `C:\\weipengten\\Main\\RStudio\\Geospatial_Analytics\\Unveiling_spatio_temporal_mobility_patterns\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#data-wrangling",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#data-wrangling",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "2. Data Wrangling",
    "text": "2. Data Wrangling\n\nPassenger VolumeBus Stop LocationHexagonal DatasetCombining the Datasets\n\n\n\nData Exploration\n\nglimpse(odbus)\n\nAs we intend to utilize Bus-stop codes as our unique identifiers when joining with our other datasets, it is not advisable to have it remain as a chr datatype. In fact, we should change it to a factor datatype.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\nChecking for Duplicates\nThere is no duplicates\n\nduplicate &lt;- odbus %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n\nChecking for Missing Data\nThere is no missing data\n\nsummary(odbus)\n\n\n\nClassifying Peak Hours\nWith reference to the time intervals provided in the requirements, we computed the passenger trips generated by origin. The passenger trips by origin are saved in 4 dataframes based on their respective classifications namely:\n\nweekday_morning_peak\nweekday_afternoon_peak\nweekend_morning_peak\nweekend_evening_peak\n\n\n\nShow the code\nweekday_morning_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nweekday_afternoon_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nweekend_morning_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nweekend_evening_peak &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nwrite_rds(weekday_morning_peak, \"data/rds/weekday_morning_peak.rds\")\nweekday_morning_peak &lt;- read_rds(\"data/rds/weekday_morning_peak.rds\")\n\nwrite_rds(weekday_afternoon_peak, \"data/rds/weekday_afternoon_peak.rds\")\nweekday_afternoon_peak &lt;- read_rds(\"data/rds/weekday_afternoon_peak.rds\")\n\nwrite_rds(weekend_morning_peak, \"data/rds/weekend_morning_peak.rds\")\nweekend_morning_peak &lt;- read_rds(\"data/rds/weekend_morning_peak.rds\")\n\nwrite_rds(weekend_evening_peak, \"data/rds/weekend_evening_peak.rds\")\nweekend_evening_peak &lt;- read_rds(\"data/rds/weekend_evening_peak.rds\")\n\n\nIn the code above, we have did a summation of Origin trips , grouped by the origin bus stop number for the 4 classifications through filtering for weekdays from weekends and by the stated time bins.\nWe save our processed data into .rds data format files using the write_rds() of readr package. The output file is saved in rds sub-folder. We do this to reduce the loading time and more importantly, we can avoid uploading the large raw files onto GitHub.\n\n\n\nPassed initial checks for whole duplicate rows, however…\n\nduplicate &lt;- busstop %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\nduplicate bus stops found, removing duplicates directly…\n\n\nShow the code\nduplicates &lt;- busstop[duplicated(busstop$BUS_STOP_N), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n\n  # Remove duplicates from the original dataframe\n  busstop &lt;- busstop[!duplicated(busstop$BUS_STOP_N), ]\n  \n  cat(\"Duplicates removed from the BUS_STOP_N column.\\n\")\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n\n\nDuplicate values found in the BUS_STOP_N column.\nSimple feature collection with 16 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13488.02 ymin: 32604.36 xmax: 44055.75 ymax: 47934\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     BUS_STOP_N BUS_ROOF_N            LOC_DESC                  geometry\n338       58031        UNK     OPP CANBERRA DR POINT (27111.07 47517.77)\n2035      82221        B01              Blk 3A POINT (35308.74 33335.17)\n2038      97079        B14 OPP ST. JOHN'S CRES  POINT (44055.75 38908.5)\n2092      22501        B02            BLK 662A POINT (13488.02 35537.88)\n2237      62251        B03        BEF BLK 471B POINT (35500.36 39943.34)\n3158      53041        B07    Upp Thomson Road POINT (27956.34 37379.29)\n3261      77329        B03   Pasir Ris Central POINT (40728.15 39438.15)\n3265      96319        NIL     YUSEN LOGISTICS POINT (42187.23 34995.78)\n3303      52059        B09             BLK 219 POINT (30565.45 36133.15)\n3411      43709        B06             BLK 644 POINT (18952.02 36751.83)\nDuplicates removed from the BUS_STOP_N column.\n\n\nChecked duplicates removed successfully\n\n\nShow the code\nduplicates &lt;- busstop[duplicated(busstop$BUS_STOP_N), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the BUS_STOP_N column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the BUS_STOP_N column.\\n\")\n}\n\n\nNo duplicate values found in the BUS_STOP_N column.\n\n\n\nsummary(busstop)\n\n\n\n\nCreate Hexagon Dataset from busstop\nNext we proceed to fulfill our requirement of preparing a hexagon dataset with specified cell dimensions of 250 by 250 units called hexagon using the st_make_grid function from the sf package.\nWe convert it into a sf dataframe called hexagon_sf using the st_sf function of sf package.\nThe code also adds a new variable/column called “grid_id” to the sf object. The “grid_id” values are assigned incrementally, starting from 1 and corresponding to the order of the hexagons in the grid. This step essentially assigns a unique identifier to each hexagon in the grid, facilitating further spatial analysis or mapping.\n\n\nShow the code\nhexagon = st_make_grid(busstop, c(250, 250), what = \"polygons\", square = FALSE)\n\n# To sf and add grid ID\nhexagon_sf = st_sf(hexagon) %&gt;%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(hexagon)))\n\n\n\n\nShow the code\nduplicates &lt;- hexagon_sf[duplicated(hexagon_sf$grid_id), ]\n\n# Check if there are any duplicates\nif (nrow(duplicates) &gt; 0) {\n  cat(\"Duplicate values found in the grid_id column.\\n\")\n  print(duplicates)\n} else {\n  cat(\"No duplicate values found in the grid_id column.\\n\")\n}\n\n\nNo duplicate values found in the grid_id column.\n\n\n\n\nExamine The Grid\nA brief overplot shows that there are 22134 grids in total and 19003 are without bus stops. We have a max of 5 bus stops per ORIGIN_GRID.\n\n\nShow the code\nhexagon_sf$n_colli = lengths(st_intersects(hexagon_sf, busstop))\ncount_all_grid_ids &lt;- n_distinct(hexagon_sf$grid_id)\ncount_zero_bus_stops &lt;- hexagon_sf %&gt;%\n  filter(n_colli == 0) %&gt;%\n  summarize(count = n_distinct(grid_id)) %&gt;%\n  pull(count)\nprint(count_all_grid_ids)\n\n\n[1] 22134\n\n\nShow the code\nprint(count_zero_bus_stops)\n\n\n[1] 19007\n\n\nShow the code\nsummary(hexagon_sf$n_colli)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2325  0.0000  5.0000 \n\n\n\n\nImportant step to ensure this dataset will be useful for us\nFilter for only hexagon data with non-zero counts of bus stops\n\nhexagon_sf = filter(hexagon_sf, n_colli &gt; 0)\nwrite_rds(hexagon_sf, \"data/rds/hexagon_sf.rds\")\nhexagon_sf &lt;- read_rds(\"data/rds/hexagon_sf.rds\")\n\n\n\nVIsualising the dataset\nWe can also do a visualisation to analyze the distribution of busstops. We specify break points at 0,1,2,3,4 and 5\nFrom the map below, it is obvious that most hexagons have 1 or 2 bus stops in their grid with some having 4 or 5 bus stops. There is approximately one ‘cluster’ that are close to each other and having 4 or 5 bus stops in each region in North, East, South, West.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nmap_busstopcounts = tm_shape(hexagon_sf) +\n  tm_fill(\n    col = \"n_colli\",\n    palette = c(\"grey\",rev(viridis(5))),\n    breaks = c(0, 1, 2, 3, 4, 5),\n    title = \"Number of Busstops\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Number of collisions: \" = \"n_colli\"\n    ),\n    popup.format = list(\n      n_colli = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\nmap_busstopcounts\n\n\n\n\n\n\n\n\n\nA few notable findings were:\n\nIn the North-West, bus stops are scarce around the cemetery in Choa Chu Kang, the nearest bus stops in that area are those along Lim Chu Kang road. Tengah Airbase is also located in that area.\nAt the far East, bus stops are scarce around Changi Airport\n- “grid_id” = 22027 is an extreme outlier, we will need to drop it\nTowards the middle, we have Paya Lebar Airbase\nIn the middle, we have the Central Water Catchment\nA standalone bus stop in Sentosa Island\n- “grid_id” = 11471 is a potential outlier and should be considered for exclusion\nA few bus stops in Johor are surprisingly in our dataset too and in\n- “grid_id” = 7068 is an extreme outlier, we will need to drop it.\n- “grid_id” for 8113,8237,8351,8485 are potential outliers as well\nOther than those mentioned above, the positioning of the rest of the bus stops seem to be acceptable and will not skew our dataset too much.\n\n\n\nData Cleaning\nHence, let’s proceed straight to dropping these data that will likely cause problems for our analysis. After deeper consideration, we decided that we should drop grid_ids for 22027, 11471 and 7068\n\n\nShow the code\n# Combine Busstop and Hexagon\nhexagon_sf &lt;- hexagon_sf %&gt;%\n  filter(!grid_id %in% c(22027, 11471, 7068))\n\n\n\n\n\nWe needed to perform aggregation of passenger trips by Hexagon instead of Origin Bus Stop, hence we need to first integrate bus stop data and the hexagon dataset using the st_intersection function from the sf package. The intersection operation retains only the spatial elements (points) that overlap between the original bus stop locations and the hexagonal grid.The resulting busstop_hexagon dataset contains information about which hexagon grid each bus stop is located in.\n\n\nShow the code\n# Combine Busstop and Hexagon\nbusstop_hexagon &lt;- st_intersection(busstop, hexagon_sf) %&gt;%\n  select(BUS_STOP_N, grid_id) %&gt;%\n  st_drop_geometry\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nHowever, it is found that there is still one duplicate found despite the thorough cleaning we did earlier:\n- Some bus stops were found to be in multiple grids, this is illogical and should be dropped from analysis.\n-   Example: BUS_STOP 250559 appeared both in grid 4 and 128\n\n\nShow the code\nduplicate &lt;- busstop_hexagon %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n# A tibble: 2 × 2\n  BUS_STOP_N grid_id\n  &lt;chr&gt;        &lt;int&gt;\n1 25059            4\n2 25059          128\n\n\nThus, the following preprocessing steps needs to be done:\n\nfilter out bus stops that have multiple grid_ids\n\nThe output now shows that we have successful dealt with duplicates and erroneous data from the integration .\n\n\nShow the code\nbusstop_hexagon &lt;- busstop_hexagon %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  filter(n_distinct(grid_id)==1) %&gt;%\n  ungroup()\n\nduplicate &lt;- busstop_hexagon %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\nduplicate\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: BUS_STOP_N &lt;chr&gt;, grid_id &lt;int&gt;\n\n\nNext, we sum up the total passenger trips group by each hexagon grid as ORIGIN_GRID for the 4 dataframes seperately to get the resulting tibble dataframes.\n\nweekday_morning_peak_join_list &lt;- left_join(weekday_morning_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\nweekday_afternoon_peak_join_list &lt;- left_join(weekday_afternoon_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\nweekend_morning_peak_join_list &lt;- left_join(weekend_morning_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\n\n\nweekend_evening_peak_join_list &lt;- left_join(weekend_evening_peak , busstop_hexagon,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_GRID = grid_id) %&gt;%\n  group_by(ORIGIN_GRID) %&gt;%\n  summarise(TOT_TRIPS = sum(TRIPS))\n\nAfter that is done, we have to join back with our sf dataset using grid_id.\nThis code chunk below performs several operations to analyze the total number of trips (TOT_TRIPS) during weekday morning peak hours based on the origin bus stop and its corresponding hexagonal grid instead of its previous bus stop number we are using.\n\n\nShow the code\nweekday_morning_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekday_morning_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nweekday_afternoon_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekday_afternoon_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nweekend_morning_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekend_morning_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nweekend_evening_peak_join_geometry &lt;- left_join(hexagon_sf, \n                           weekend_evening_peak_join_list,\n                           by = c(\"grid_id\" = \"ORIGIN_GRID\"))\n\nwrite_rds(weekday_morning_peak_join_geometry, \"data/rds/weekday_morning_peak_join_geometry.rds\")\nweekday_morning_peak_join_geometry &lt;- read_rds(\"data/rds/weekday_morning_peak_join_geometry.rds\")\n\nwrite_rds(weekday_afternoon_peak_join_geometry, \"data/rds/weekday_afternoon_peak_join_geometry.rds\")\nweekday_afternoon_peak_join_geometry &lt;- read_rds(\"data/rds/weekday_afternoon_peak_join_geometry.rds\")\n\nwrite_rds(weekend_morning_peak_join_geometry, \"data/rds/weekend_morning_peak_join_geometry.rds\")\nweekend_morning_peak &lt;- read_rds(\"data/rds/weekend_morning_peak.rds\")\n\nwrite_rds(weekend_evening_peak_join_geometry, \"data/rds/weekend_evening_peak_join_geometry.rds\")\nweekend_evening_peak_join_geometry &lt;- read_rds(\"data/rds/weekend_evening_peak_join_geometry.rds\")"
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#exploratory-data-analysis-eda",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#exploratory-data-analysis-eda",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "3. Exploratory Data Analysis (EDA)",
    "text": "3. Exploratory Data Analysis (EDA)\n\nDistribution of Total TripsDistribution for Total Trips PER Bus StopDistribution for Total Trips Across 4 PeriodsDistribution for Total Trips PER Bus Stop Across 4 Periods\n\n\nWe discovered that the data has a right-tailed distribution for all time classifications.\n\n\nShow the code\ncombined_data &lt;- rbind(\n  transform(weekday_morning_peak_join_geometry, period = \"Weekday Morning Peak\"),\n  transform(weekday_afternoon_peak_join_geometry, period = \"Weekday Afternoon Peak\"),\n  transform(weekend_morning_peak_join_geometry, period = \"Weekend Morning Peak\"),\n  transform(weekend_evening_peak_join_geometry, period = \"Weekend Evening Peak\")\n)\n\n# Plot combined data\nggplot(data = combined_data, \n       aes(x = as.numeric(`TOT_TRIPS`))) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\") +\n  facet_wrap(~period, scales = \"free_y\") +\n  labs(title = \"Distribution of Passenger Trips during Different Time Periods\",\n       subtitle = \"Histograms show the distribution of total trips for different time periods\",\n       x = \"Total Trips\",\n       y = \"Frequency\")\n\n\nWarning: Removed 343 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, for trips per bus stop…\n\n\nShow the code\ncombined_density &lt;- combined_data %&gt;%\n  mutate(`trips_per_busstop` = (`TOT_TRIPS` / n_colli))\n\n\n# Plot combined data\nggplot(data = combined_density, \n       aes(x = as.numeric(`trips_per_busstop`))) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\") +\n  facet_wrap(~period, scales = \"free_y\") +\n  labs(title = \"Distribution of Passenger Trips during Different Time Periods\",\n       subtitle = \"Histograms show the distribution of total trips for different time periods\",\n       x = \"Total Trips Per BusStop\",\n       y = \"Frequency\")\n\n\nWarning: Removed 343 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\nA quick examination of the distribution of origin trips across all four periods reveals that the weekday morning peak has the highest number of trip counts, followed by the weekday afternoon peak, the weekend morning peak, and finally, the weekend evening peak.\nThis observation suggests that, during the specified time periods, there is a discernible pattern in the frequency of trips, with a notable concentration of trips during weekday mornings. This information could imply potential trends in commuting behavior or specific usage patterns during different times of the week.\nTransport agencies can allocate resources such as personnel and busses better with this information. Frequency of busses should also be increased for weekday afternoon peak period.\n\n\nShow the code\nggplot(combined_data, aes(x = factor(period), y = TOT_TRIPS, fill = factor(period))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Counts of Total Trips Grouped by Periods\",\n       x = \"Period\",\n       y = \"TOT_TRIPS Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\nWarning: Removed 343 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nTrips per bus stop turns out to demonstrate similar patterns as compared to total trips.\nThis actually suggest that the transport authorities have done well in the planning of decision of bus stop locations over the years.\nPerhaps this also mean that it is safe to analyse the choropleths using total trips by itself later on.\n\n\nShow the code\nggplot(combined_density, aes(x = factor(period), y = trips_per_busstop, fill = factor(period))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Counts of Trips Per Bus Stop Grouped by Periods\",\n       x = \"Period\",\n       y = \"Trips Per Bus Stop\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\nWarning: Removed 343 rows containing missing values or values outside the scale range\n(`geom_bar()`)."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#geovisualisation-and-analysis-1",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#geovisualisation-and-analysis-1",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "4. Geovisualisation and Analysis",
    "text": "4. Geovisualisation and Analysis\n\nImportant Considerations\n\nDue to the use of small hexagon tiles and a heavily right skewed distribution, quantile classification proves to provide little value. We decided that kmeans classification is best due to the ability to create discrete classes. This helps us to compare the 4 periods easily.\nWe previously derived number of Passenger Trips using the bus stops as the origin. This means that that some tiles could have missing data due to the lack of trips originating from there but that not necessary be the case for trips with that as the destination.\nWe previously excluded tiles with no bus stops earlier in hexagon_sf, hence any missing data present here is not due to missing bus stops\n\n\n\nGeneral Observations across all 4 interval classifications\n\nThe bus stops along Lim Chu Kang exhibit minimal to no origin trips on both weekdays and weekends. This can be attributed to the presence of a cemetery in that area, making it more practical for individuals to use private transportation.\nAdditionally, along the eastern coast, there are several bus stops without origin trips on weekdays. However, the situation changes on weekends and holidays, although the overall volume remains low. It is advisable to consider adjusting the bus schedule in that region for weekdays. A similar pattern is observed for the islands in the North-West..\nOrigin Trips for a few bus stops near the customs remain high through weekdays and wekends. Also, one bus stop in Johor is consistently high in origin trips.\nIn central areas, origin trips are not high during weekday mornings but are high during weekday afternoons. This is probably due to residential planning by the URA where residents travel from the other regions to the central and business districts.\n\n\nWeekday Morning PeakWeekday Afternoon PeakWeekend/holiday Morning PeakWeekend/holiday Evening Peak\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ninferno_palette &lt;- inferno(5)\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekday_morning_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\", \n          palette = viridis(5),\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekday morning peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tmap_style(\"natural\")\n\n\ntmap style set to \"natural\"\n\n\nother available styles are: \"white\", \"gray\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekday_afternoon_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\", \n          palette = viridis(5),\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekday afternoon peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tmap_style(\"natural\")\n\n\ntmap style set to \"natural\"\n\n\nother available styles are: \"white\", \"gray\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekend_morning_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\", \n          palette = viridis(5),\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekend/holiday morning peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)+\n  tmap_style(\"natural\")\n\n\ntmap style set to \"natural\"\n\n\nother available styles are: \"white\", \"gray\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntmap_options(check.and.fix = TRUE)\ntm_shape(weekend_evening_peak_join_geometry)+\n  tm_fill(\"TOT_TRIPS\", \n          style = \"kmeans\",\n          palette = viridis(5), \n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips during Weekend/holiday evening peak\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tmap_style(\"natural\")\n\n\ntmap style set to \"natural\"\n\n\nother available styles are: \"white\", \"gray\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\""
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#local-indicators-of-spatial-association-lisa-analysis-1",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#local-indicators-of-spatial-association-lisa-analysis-1",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "5. Local Indicators of Spatial Association (LISA) Analysis",
    "text": "5. Local Indicators of Spatial Association (LISA) Analysis\nLocal Indicators of Spatial Association (LISA) Analysis:\nLocal Indicators of Spatial Association (LISA) is a statistical technique used in spatial analysis to identify and assess spatial patterns of clustering or dispersion within a geographical dataset. LISA analysis helps to uncover local patterns of spatial autocorrelation, providing insights into whether similar values tend to cluster together or if there are areas with dissimilar values.\nLISA analysis is based on the concept of spatial autocorrelation, which measures the degree to which neighboring locations are similar or dissimilar in terms of a particular variable. (In Our case, it is Origin Passenger Trips)\nTwo key LISA statistics are Moran’s I and the associated p-value, for each spatial unit (hexagon grid in our case) to determine if they are part of a significant cluster, outlier, or exhibit no significant pattern.\nLocal Moran’s I identifies clusters by categorizing each unit as High-High (high value surrounded by high values), Low-Low (low value surrounded by low values), High-Low (high value surrounded by low values), or Low-High (low value surrounded by high values).\n- Decision-making process\nDue to the use of hexagon grids, we had many empty grids and this proves difficult to derive contiguity weights. Hence, we attempted to derive distance weights instead\nFixed distance weight matrix was used in deriving the weights and neighbors.\n\n5.1 Deriving adaptive distance weights\nThe summary statistics report below shows that the maximum nearest neighbour distance is 901.4m. By using a threshold value of 902m will ensure that each area will have at least one neighbour.\n\n\nShow the code\ngeo &lt;- sf::st_geometry(hexagon_sf)\nnb &lt;- st_knn(geo, longlat = TRUE)\n\n\n! Polygon provided. Using point on surface.\n\n\nShow the code\ndists &lt;- unlist(st_nb_dists(geo, nb))\n\n\n! Polygon provided. Using point on surface.\n\n\nShow the code\nsummary(dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  250.0   250.0   250.0   257.6   250.0   901.4 \n\n\nNow we will go ahead to compute the fixed distance weights by using the code chunk below.\nThe use of .allow_zero = TRUE option is to assign the value of 0 to rows with missing values for TOT_TRIPS as missing values will create problems for our analysis later\n\n\nShow the code\nwm_q_1 &lt;- weekday_morning_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0), \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\n! Polygon provided. Using point on surface.\n\n\nShow the code\nwm_q_2 &lt;- weekday_afternoon_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0),  \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\n! Polygon provided. Using point on surface.\n\n\nShow the code\nwm_q_3 &lt;- weekend_morning_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0),  \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\n! Polygon provided. Using point on surface.\n\n\nShow the code\nwm_q_4 &lt;- weekend_evening_peak_join_geometry %&gt;%\n  mutate(TOT_TRIPS = replace_na(TOT_TRIPS, 0),  \n         nb = st_dist_band(hexagon,\n                           upper = 902),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\n! Polygon provided. Using point on surface.\n\n\n\n\n5.2 Computing local Moran’s I\nIn this section, we will compute Local Moran’s I of Total Passenger Trips at county level by using local_moran() of sfdep package.\nThe provided code conducts a Local Moran’s I analysis on four distinct datasets (wm_q_1, wm_q_2, wm_q_3, wm_q_4), each associated with specific time periods or scenarios.\nThe analysis focuses on the spatial autocorrelation of the variable TOT_TRIPS within each dataset, employing the local_moran function with\n\nneighbors (nb) and\nweights (wt) and\n99 simulations.\n\nThe calculated Local Moran’s I statistic assesses whether nearby observations exhibit similar total trip values, revealing spatial patterns and clusters.\nThe use of unnest implies a need to extract detailed information about the spatial relationships between observations and their neighbors after the Local Moran’s I analysis.\n\n\nShow the code\nlisa_1 &lt;- wm_q_1 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2 &lt;- wm_q_2 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_3 &lt;- wm_q_3 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_4 &lt;- wm_q_4 %&gt;% \n  mutate(local_moran = local_moran(\n    TOT_TRIPS, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\n5.3 Visualising local Moran’s I\nIn the following code section, tmap functions are utilized to create a choropleth map based on the values in the ii field, representing the Local Moran’s I values. The chosen tmap_style option is set to albatross to suit the grid’s nature and emphasize clusters, where lighter colors indicate positive values and darker colors indicate negative values.\nIt’s important to note that a positive Local Moran’s I value signifies a feature’s membership in a cluster, while a negative value suggests that a feature is an outlier.\nExamining the map, regions shaded in various hues of green indicate their membership in one or more clusters.While there are overlapping areas among the maps generated for the four periods of interest, there are also discrepancies.\nHowever, relying solely on the local Moran’s score is insufficient for depicting spatial clustering, as it doesn’t provide information about whether the variable’s value (Total Passenger Trips) being examined is high or low, and whether the test result is statistically significant. We need to proceed with analyzing only the regions with statistically significant values of total passenger trips.\n\n5.3.1 Weekday Morning Peak5.3.2 Weekday Afternoon Peak5.3.3 Weekend Morning Peak5.3.4 Weekend Afternoon Peak\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_1) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekday morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_2) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekday afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_3) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekend morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_4) +\n  tm_fill(\"ii\",\n          style = \"kmeans\",\n         palette = viridis(5)) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"local Moran I of Bus Trips in weekend afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4 Visualising p-value of local Moran’s I\nIn the code chunk below, tmap functions are used to prepare a choropleth map by using value in the p_ii_sim field\nWe will visualize solely the statistically significant local Moran’s I values (p_ii_sim &lt; 0.05) through the subsequent code snippet.\n\n5.4.1 Weekday Morning Peak5.4.2 Weekday Afternoon Peak5.4.3 Weekend Morning Peak5.4.4 Weekend Afternoon Peak\n\n\nComparing local Moran’s I together with its p_ii_sim values, a few observations were revealed:\n\nThere are clusters near customs and throughout most of west area. They are all statistically significant.\nClusters and dispersions found in the South / Central are mostly not statistically significant and should be ignored.\nThere are both many clusters and dispersions found in Bedok, Tampines, Pasir Ris and Changi that are found to be statistically significant. However, not so much for other parts of East.\nClusters and Dispersions found in North east and North are also statistically significant.\n\nThese Clusters found seem to match our expectations as they are within residential zones. However, this is possible also because of bus interchanges and the wider options of buses avaliable near these bus interchanges.\nWhat about the dispersion that we are observing? It is easy to dismiss that. However they tend to happen around those clusters found. This can actually be explained with the fact that are OTHER options other than taking buses such as a little bit of walk or…. Cycling!!! as a form of commuting.\nThis trend may not be immediately apparent to individuals who don’t utilize public transportation or to foreigners. However, it’s becoming increasingly common for Singaporeans to use personal transport, such as bicycles, to reach bus interchanges instead of waiting for buses directly at their residences. This choice is driven by the realization that cycling to the bus interchange can be a more time-efficient option. The areas surrounding bus interchanges often face congestion with numerous bus stops and traffic lights, turning what should be a short journey into a lengthy one.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_1) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekday morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\ntmap style set to \"watercolor\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\" \n\n\n\n\n\n\n\n\n\n\n\nThere are few dispersions this time round and more clusters found instead.\nMost of previously residential areas identified are found to not display any statistically significant patterns, other than some parts around tampines and north east which could be hubs designated by the government.\nMany statistically significant clusters are found around west region which is also an industrial region.\nCentral business district areas do not exhibit any significant patterns, it seems most office workers do not take buses home but the MRT instead.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_2) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekday afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\ntmap style set to \"watercolor\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\" \n\n\n\n\n\n\n\n\n\n\n\nIt seems there are more dispersions during mornings peak hours even for weekends/holidays.\nThere are less clusters as compared to weekdays. The most significant clusters are those found around Bugis and Lavender.\nClusters found near NUS are statistically significant, it is unsure if it is due to the school itself.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_3) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekend morning\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\ntmap style set to \"watercolor\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\" \n\n\n\n\n\n\n\n\n\n\n\nThere are very few dispersions this time round, with three in Johor, this is not surprising as any Singapore would understand that most won’t choose that time to travel back to Singapore and this data only record trips between Singapore bus stops and does not include trips not under SBS.\nMore clusters are shown and are quite concentrated as compared to weekdays.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_4) +\n  tm_fill(\"p_ii_sim\",\n          palette = c(rev(viridis(5)), \"grey\"),\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran I in weekend afternoon\",\n            main.title.size = 0.8) +\n  tmap_style(\"watercolor\")\n\n\ntmap style set to \"watercolor\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\""
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#visualising-lisa-map",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/Unveiling_spatio_temporal_mobility_patterns.html#visualising-lisa-map",
    "title": "Unveiling the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore",
    "section": "6 Visualising LISA map",
    "text": "6 Visualising LISA map\nIn this visualisation, LISA categorises each region into one of four groups:\n\nHigh-High indicates grids with high number of origin trips located next to other grids with high number of origin trips\nLow-High indicates grids with low number of origin trips located next to other grids with high number of origin trips\nHigh-Low indicates grids with high number of origin trips located next to other grids with low number of origin trips\nLow-Low indicates grids with low number of origin trips located next to other grids with low number of origin trips\n\n\n6.1 Weekday Morning Peak6.2 Weekday Afternoon Peak6.3 Weekend Morning Peak6.4 Weekend Afternoon Peak\n\n\nIn this visualisation for Weekday Morning Peak, some observations were found:\n\nHigh-High regions were found throughout parts of North-East, North-West, Central and West, except South. These spots also seem to be nearby each other in their respective regions, seemingly signifying hubs.\nLow-High regions are few in existence and happened to be near High-High regions\nHigh-Low regions are almost none.\nLow-Low regions happened to be found near the borders of Singapore mostly. They coincide with non-residential areas like changi airport Tuas and near cemetries.\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nlisa_sig &lt;- lisa_1  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_1) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4)+\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\nHigh-High and High-Low regions are few here and definitely much less than during Weekday Morning Peak.\nThere are three clusters of High-High regions (Jurong west, Woodlands, Bedok), with some Low-High regions around..\nLow-Low regions are found in Tuas Industrial are and throughout parts of Singapore\nMost Residential areas show no patterns.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nlisa_sig &lt;- lisa_2  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_2) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\nMore High-High clusters are found with the previous three High-High clusters remaining strong for (Jurong west, Woodlands, Tampines). In addition to those three, are additional clusters found around Ang Mo Kio, Toa Payoh, Bedok and Bugis.\nLow-Low areas are found near Tuas and Changi Airport again and the stretch along cemeteries in Lim Chu Kang\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nlisa_sig &lt;- lisa_3  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_3) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n\n\n\n\nThe patterns for this section seems almost similar to Weekend Morning Peak, most clusters remain but shows less activity compared to its morning.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\nlisa_sig &lt;- lisa_4  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(lisa_4) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"median\",\n          palette = c(viridis(5))) + \n  tm_borders(alpha = 0.4) +\n  tmap_style(\"albatross\")\n\n\ntmap style set to \"albatross\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them)."
  },
  {
    "objectID": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/data/geospatial/MPSZ-2019.html",
    "href": "RStudio/Geospatial_Analytics/Unveiling_spatio_temporal_mobility_patterns/data/geospatial/MPSZ-2019.html",
    "title": "Wei Peng's Analytics Journey",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#importing-libraries",
    "href": "Python/AutoML_Binary_Classification.html#importing-libraries",
    "title": "AutoML for Binary Classification",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n#Importing necessary libraries\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pandas.api.types import CategoricalDtype\n\nfrom sklearn.impute import SimpleImputerfrom scipy.stats import skew\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer,StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split,RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import label_binarize,LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, roc_curve, auc\n\n\nfrom imblearn.combine import SMOTEENN\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\n \n \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#load-the-dataset",
    "href": "Python/AutoML_Binary_Classification.html#load-the-dataset",
    "title": "AutoML for Binary Classification",
    "section": "Load the Dataset",
    "text": "Load the Dataset\nThis Python code snippet uses TensorFlow’s Keras API to load the CIFAR-10 dataset and print the shapes of the training and test sets. The CIFAR-10 dataset is a popular dataset used for training machine learning and computer vision algorithms. It consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class.\n\nurl = 'https://raw.githubusercontent.com/weipengten/ISSS623GroupProject--Applied-Healthcare-/main/CARES_data.xlsx'\ndf = pd.read_excel(url, index_col='Indexno')\n\nprint(df.shape)\ndf.head()"
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#data-cleaning",
    "href": "Python/AutoML_Binary_Classification.html#data-cleaning",
    "title": "AutoML for Binary Classification",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nReplace missing values with nan\nString formatting\nCreate bins for age as age_bins\nCreate bins for rcri_score as rcri_bin\nDrop rows with at least 10 missing values in columns, 93.73% remaining\nNUMERICAL columns (remaining columns except for DaysbetweenDeathandoperation): impute missing values with median\nCATEGORICAL columns (remaining columns except for [‘mortality’, ‘daysbetweendeathandoperation’, ‘@30daymortality’, ‘thirtydaymortality’]): impute missing values with ‘None’\n\n\nReplace missing values with nan\n\n# replace missing values with pd.NA\nnull_values = ['#NULL!', 'BLANK', 'none', 'NA', '&lt;NA&gt;', 'None']\ndf.replace(null_values, np.nan, inplace=True)\n\n# check data types and missing values\ndf.info()\n\n\n\n# statistical summary\ndf.describe()\n\n\n\n\nString formatting\n\n# replace all column names with lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# replace all string values in the DataFrame with lowercase\ndf = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n\n\n\nReplace missing values with nan\n\n# replace missing values with pd.NA\nnull_values = ['#NULL!', 'BLANK', 'none', 'NA', '&lt;NA&gt;', 'None']\ndf.replace(null_values, np.nan, inplace=True)\n\n# check data types and missing values\ndf.info()\n\n\n\nCheck potential duplicates\n\n# check potential duplicates\ndf.duplicated().sum()\n\n# We will not drop duplicates in this dataste as\n# they represent multiple legitimate entries representing different patients\n\n\n\n\nFind all unique values for categorical features\n\n# find all unique values for categorical features\nfor column in df.select_dtypes(include=['object']).columns:\n    print(f'{column}: {df[column].unique()}')\n\n\n\n\n\nCreate bins for age as age_bins\n\n# bin the age column\nage_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\nage_labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '101-110']\ndf['age_binned'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n\n# summarize frequency counts\nage_binned_counts = df['age_binned'].value_counts().sort_index()\nprint(age_binned_counts, '\\n')\nplt.figure(figsize=(6, 4))\nsns.barplot(x=age_binned_counts.index, y=age_binned_counts.values, palette=\"viridis\")\nplt.xlabel('Age Bins')\nplt.ylabel('Frequency')\nplt.title('Frequency of Age Bins')\nplt.xticks(rotation=45)\nplt.show()\n\ndf['age_binned'] = df['age_binned'].astype('object')\n\n\n\n\nCreate bins for rcri_score as rcri_bin\n\n# Function to bin 'RCRI score' into distinct ordinal categories\ndef smart_binning(df, column_name, bin_column_name):\n    # Create a copy of the DataFrame to avoid modifying the original\n    _df = df.copy()\n\n    # Create a mapping for the bins, including a label for NaN values\n    bin_labels = {1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6'}\n    _df[bin_column_name] = _df[column_name].map(bin_labels)\n\n    # Handle NaN values by assigning them to a specific category\n    _df[bin_column_name] = _df[bin_column_name].fillna('No_RCRI')\n\n    # Define the categorical type with ordered categories, including 'No_RCRI'\n    cat_type = CategoricalDtype(categories=['1', '2', '3', '4', '5', '6', 'No_RCRI'], ordered=True)\n\n    # Convert the new bin column to categorical type\n    _df[bin_column_name] = _df[bin_column_name].astype(cat_type)\n\n    return _df\n\n# Apply the smart binning function\ndf = smart_binning(df, 'rcri_score', 'rcri_bin')\n\n# Check if the rcri_bin column is ordinal\nif isinstance(df['rcri_bin'].dtype, CategoricalDtype) and df['rcri_bin'].dtype.ordered:\n    print(\"rcri_bin is ordinal.\")\nelse:\n    print(\"rcri_bin is not ordinal.\")\nprint('\\n')\n\n# Print the categories and their order\nprint(\"Categories and order:\", df['rcri_bin'].dtype.categories)\nprint('\\n')\n\n# Print the distribution of data in 'rcri_bin'\nprint(f\"The distribution of data in 'rcri_bin' is as follows:\\n{df['rcri_bin'].value_counts()}\")\nprint('\\n')\n\n\ndf['rcri_bin'] = df['rcri_bin'].astype('object')\n\n\n\nVisualize correlation for numerical column\n\ncorrelation_matrix = df.select_dtypes(include=['number']).corr()\nfontsize=8\nplt.figure(figsize=(6,6))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, cbar=True, annot_kws={'size': 8})\nplt.title('Correlation Matrix for Numerical Columns', fontsize=10)\nplt.xticks(fontsize=fontsize)\nplt.yticks(fontsize=fontsize)\nplt.show()\n\n\n\n\n\nDrop rows with at least 10 missing values in columns, 93.73% remaining\n\n# define the threshold for missing values\nthreshold = 10\nmissing_counts = df.isna().sum(axis=1)\n\n# Calculate the percentage of rows to keep\npercentage_remaining = round((df[missing_counts &lt; threshold].shape[0] / df.shape[0]) * 100, 2)\nprint(f\"Percentage of rows with fewer than {threshold} missing values: {percentage_remaining}%\")\n\n# Drop rows with at least the specified number of missing values\ndf = df[missing_counts &lt; threshold]\ndf.head()\n\n\n\nData Validation\n\n# Check the condition\ncondition_check = df[df['thirtydaymortality'] == True]['mortality'] == True\n\n# Verify if all values meet the condition\nif condition_check.all():\n    print('----------------------------------------')\n    print(\"All instances where 'thirtydaymortality' is True, 'mortality' is also True.\")\n    print(\"Data validation step 1, there's no logical errors found for mortality and thirtydaymortality\")\n    print('----------------------------------------')\nelse:\n    print(\"There are instances where 'thirtydaymortality' is True but 'mortality' is not True.\")\n    # Data validation:\n\nif 'mortality' in df.columns and 'daysbetweendeathandoperation' in df.columns:\n    # Check the conditions\n    condition_1 = df.loc[df['mortality'] == True, 'daysbetweendeathandoperation'].notnull().all()\n    condition_2 = df.loc[df['mortality'] == False, 'daysbetweendeathandoperation'].isnull().all()\n\n    # Print the results\n    print('----------------------------------------')\n    print(\"Condition 1 met:\", condition_1)\n    print(\"Condition 2 met:\", condition_2)\n    print(\"Data validation step 2, there's no logical errors found for mortality and daysbetweendeathandoperation\")\n    print('----------------------------------------')\nelse:\n    print(\"The DataFrame does not contain the required columns.\")\n\n\n\n\nOutlier detection for numerical features\n\nnumerical_columns = df.select_dtypes(include=['number']).columns\nprint(\"Numerical columns:\", numerical_columns)\n\nplt.figure(figsize=(16, 10))\nfor i, column in enumerate(numerical_columns, 1):\n    plt.subplot(2, 4, i)\n    sns.boxplot(y=df[column])\n    plt.title(column)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nNUMERICAL columns (remaining columns except for DaysbetweenDeathandoperation): impute missing values with median)\n\n# Initialize SimpleImputer to fill missing values with the median\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\n\n# List all numeric columns in the DataFrame\nnumeric_columns = df.select_dtypes(include=['number']).columns\nprint(\"Numeric columns:\", list(numeric_columns))  # we have also checked that all the numerical columns are correct, no binary variables are mistakenly treated as numeric variables.\n\n# Define specific numeric columns for imputation\nnumeric_cols = [col for col in numeric_columns if col != 'daysbetweendeathandoperation']\nprint(\"Numeric columns:\", list(numeric_cols))\n\n# Fit the imputer on the specified numeric columns\nimp.fit(df[numeric_cols])\n\n# Transform the specified numeric columns by imputing missing values with the median\ndf[numeric_cols] = imp.transform(df[numeric_cols])\n\n# Ensure the columns are of float32 type\ndf[numeric_cols] = df[numeric_cols].astype(np.float32)\n\n# Print the DataFrame with imputed data to verify the changes\nprint(\"DataFrame with imputed numeric columns:\")\nprint(df[numeric_cols])\n\n\n\n\nCATEGORICAL columns (remaining columns except for ['mortality', 'daysbetweendeathandoperation', ' @30daymortality', 'thirtydaymortality']): impute missing values with ‘None’\n\n# Initialize SimpleImputer for categorical columns to fill missing values with 'None'\ncat_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='None')\n\n# List all categorical columns in the DataFrame\ncategorical_columns  = df.select_dtypes(include=['object']).columns\nprint(\"Categorical columns:\", list(categorical_columns))\n\n# Columns to exclude\nexclude_columns = ['mortality', 'daysbetweendeathandoperation', '@30daymortality', 'thirtydaymortality']\n\n# Filter out the excluded columns from the list of categorical columns\ncategorical_columns = [col for col in categorical_columns if col not in exclude_columns]\n\n# Fit the imputer on the categorical columns\ncat_imp.fit(df[categorical_columns])\n\n# Transform the categorical columns by imputing missing values with 'None'\ndf[categorical_columns] = cat_imp.transform(df[categorical_columns])\n\n# find all unique values for categorical features\nfor column in df.select_dtypes(include=['object']).columns:\n    print(f'{column}: {df[column].unique()}')\n\nOutcome 1: Mortality within 30 days (thirtydaymortality)\n\n\n1. Frame the Problem\n\nTarget variable: thirtydaymortality\nType: Classification Problem\nBinary Outcome: (No,Yes)\n\n\n\n2. Data sensing, preprocessing:\n\nFeature Selection: Exclusion of variables that deomonstrates high redundancy or Multicollinearity with thirtydaymortality:\n\n@30daymortality\nmortality\ndaysbetweendeathandoperation\n\n\n\n# Feature Selection:\n# drop unecessary columns, features that occur only after the target result should be excluded\nfeatures_to_exclude = ['mortality', 'daysbetweendeathandoperation', '@30daymortality']\ndf = df.drop(columns=features_to_exclude)\n\n\n\n3. Data Wrangling, Transformation:\nNumerical features Transformation:\n\nTest for normality of numeric columns: ALL are not normally distributed\nDIstribution transfomation for columns\nDistribution normalisation and standardisation for columns\n\nCateogorical features Transformation:\n\nOne-hot encoding to ensure smooth usage by tree learning algorithms\n\n\nNumerical features Transformation\n\nTest for Normality\n\n# Test for normality of numeric columns: ALL are not normally distributed\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Define specific numeric columns for imputation\nnumeric_cols = [col for col in numeric_columns if col != 'daysbetweendeathandoperation']\nprint(\"Numeric columns:\", list(numeric_cols))\n\n# Function to test for normal distribution\ndef test_normal_distribution(data, alpha=0.05):\n    \"\"\"\n    Test if the data follows a normal distribution.\n    \"\"\"\n    shapiro_stat, shapiro_p = stats.shapiro(data)\n    ks_stat, ks_p = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data, ddof=0)))\n\n    print(f\"Shapiro-Wilk: Statistic={shapiro_stat:.3f}, p-value={shapiro_p:.3f}\")\n    print(f\"Kolmogorov-Smirnov: Statistic={ks_stat:.3f}, p-value={ks_p:.3f}\")\n    print(\"Normally distributed:\" if shapiro_p &gt; alpha and ks_p &gt; alpha else \"Not normally distributed.\")\n    print()\n\n# Test normal distribution\nprint(\"Testing Normal Distribution:\")\nfor col in numeric_cols:\n    print(f\"{col}:\")\n    test_normal_distribution(df[col])\n\ndef create_qq_plots(columns):\n    # Number of columns to plot\n    num_cols = len(columns)\n\n    # Determine grid size for subplots\n    n_cols = 3  # Number of columns in the subplot grid\n    n_rows = (num_cols + n_cols - 1) // n_cols  # Calculate number of rows needed\n\n    # Create subplots\n    plt.figure(figsize=(15, 5 * n_rows))  # Adjust figure size as needed\n\n    for i, col in enumerate(columns, 1):\n        plt.subplot(n_rows, n_cols, i)\n        stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)\n        plt.title(f'Q-Q Plot of {col}')\n        plt.xlabel('Theoretical Quantiles')\n        plt.ylabel('Sample Quantiles')\n\n    plt.tight_layout()\n    plt.show()\n\n# Create Q-Q plots for numeric columns\ncreate_qq_plots(numeric_cols)\n\n\n\n\n\nDistribution of numeric columns\n\ndef create_histograms(df,columns):\n    # Number of columns to plot\n    num_cols = len(columns)\n\n    # Determine grid size for subplots\n    n_cols = 3  # Number of columns in the subplot grid\n    n_rows = (num_cols + n_cols - 1) // n_cols  # Calculate number of rows needed\n\n    # Create subplots\n    plt.figure(figsize=(15, 5 * n_rows))  # Adjust figure size as needed\n\n    for i, col in enumerate(columns, 1):\n        plt.subplot(n_rows, n_cols, i)\n        sns.histplot(df[col].dropna(), kde=True, bins=30)  # Adjust bins as needed\n        plt.title(f'Distribution of {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\n\n# Create histograms for numeric columns\ncreate_histograms(df,numeric_cols)\n\n\n\n\nAutomation of transforming numeric columns based on skewness\n\ndef calculate_skewness(df,columns):\n    # Dictionary to store skewness values\n    skewness_dict = {}\n\n    for col in columns:\n        # Drop NaN values for skewness calculation, there shouldn't be any left\n        data = df[col].dropna()\n\n        # Calculate skewness\n        skew_value = skew(data, nan_policy='omit')  # nan_policy='omit' ignores NaN values\n        skewness_dict[col] = skew_value\n\n    # Convert skewness dictionary to DataFrame for better readability\n    skewness_df = pd.DataFrame(list(skewness_dict.items()), columns=['Column', 'Skewness'])\n    return skewness_df\n\n\n# Calculate skewness for numeric columns\nskewness_df = calculate_skewness(df,numeric_cols)\n\n# Print skewness values\nprint(\"Skewness of Numeric Columns:\")\nprint(skewness_df)\n\n\n\ndef transform_based_on_skewness(df, columns):\n    transformed_df = df.copy()\n    new_columns_numeric = []\n    column_mapping = {}\n\n    for col in columns:\n        # Calculate skewness\n        skewness = skew(df[col].dropna(), nan_policy='omit')\n\n        # Initialize new_col to None\n        new_col = None\n\n        # Print the skewness for debugging\n        print(f\"Skewness for column {col}: {skewness}\")\n\n        # Choose transformation based on skewness\n        if skewness &gt; 1:\n            # Highly positively skewed\n            if (transformed_df[col] &gt; 0).all():  # Check if all values are positive\n                new_col = col + '_log'\n                transformed_df[new_col] = np.log1p(df[col])\n                print(f\"Applied log transformation on column {col}\")\n            else:\n                # Use Yeo-Johnson if data contains zero or negative values\n                new_col = col + '_yeojohnson'\n                pt = PowerTransformer(method='yeo-johnson')\n                transformed_df[new_col] = pt.fit_transform(df[[col]])\n                print(f\"Applied Yeo-Johnson transformation on column {col}\")\n\n        elif skewness &gt; 0.5:\n            # Moderately positively skewed\n            new_col = col + '_sqrt'\n            transformed_df[new_col] = np.sqrt(df[col] + 1)  # Adding 1 to handle zero values\n            print(f\"Applied square root transformation on column {col}\")\n\n        elif skewness &lt; -1:\n            # Highly negatively skewed\n            new_col = col + '_inv'\n            transformed_df[new_col] = 1 / (df[col] + 1)  # Adding 1 to avoid division by zero\n            print(f\"Applied inverse transformation on column {col}\")\n\n        elif skewness &lt; -0.5:\n            # Moderately negatively skewed\n            new_col = col + '_inv_sqrt'\n            transformed_df[new_col] = 1 / np.sqrt(df[col] + 1)  # Adding 1 to handle zero values\n            print(f\"Applied inverse square root transformation on column {col}\")\n\n        else:\n            # Data is close to normal, no transformation needed\n            new_col = col + '_no_transform'\n            transformed_df[new_col] = df[col]\n\n       \n        if new_col:\n            new_columns_numeric.append(new_col)\n            column_mapping[col] = new_col\n\n    return transformed_df, new_columns_numeric, column_mapping\n\n# Example usage\n# Assuming df is your DataFrame\nnumeric_cols = ['age', 'rcri_score', 'preopegfrmdrd', 'preoptransfusionwithin30days',\n                'intraop', 'postopwithin30days', 'transfusionintraandpostop']\n\n# Apply transformations based on skewness\ntransformed_df, new_columns_numeric, column_mapping = transform_based_on_skewness(df, numeric_cols)\n\n\n# Print the first few rows of the transformed DataFrame to verify\nprint(transformed_df.head())\n# Print the list of newly transformed columns\nprint(new_columns_numeric)\n\n# Verify the columns are indeed in the DataFrame\nfor col in new_columns_numeric:\n    if col in transformed_df.columns:\n        print(f\"Column {col} exists in the DataFrame.\")\n    else:\n        print(f\"Column {col} does NOT exist in the DataFrame.\")\n\n\n\n# Create histograms for newly transformed columns\ncreate_histograms(transformed_df,new_columns_numeric)\n\n\n\n\n\n\nComparison of newly transformed variables as compared to orignal in terms of skewness\n\n# Calculate skewness for original columns\nskewness_original = calculate_skewness(df,numeric_cols).set_index('Column')['Skewness'].to_dict()\n\n# Calculate skewness for transformed columns\nskewness_transformed = calculate_skewness(transformed_df,new_columns_numeric).set_index('Column')['Skewness'].to_dict()\n\n# Print column mapping\nprint(\"Column Mapping:\")\nprint(column_mapping)\n\n\n# Print skewness values\nprint(\"Skewness of Numeric Columns:\")\nprint(skewness_df)\nprint(skewness_transformed)\n\n# Print column mapping\nprint (column_mapping)\n\n# Create a comparison DataFrame\ncomparison_df = pd.DataFrame({\n    'Original Column': numeric_cols,\n    'Original Skewness': [skewness_original[col] for col in numeric_cols],\n    'Transformed Column': [column_mapping[col] if col in column_mapping else col + '_no_transform' for col in numeric_cols],\n    'Transformed Skewness': [skewness_transformed[column_mapping[col]] if col in column_mapping else skewness_original[col] for col in numeric_cols]\n})\n\n# Add a column to indicate if skewness has decreased\ncomparison_df['Skewness Decreased'] = abs(comparison_df['Original Skewness']) &gt; abs(comparison_df['Transformed Skewness'])\n\nprint(\"Skewness Comparison:\")\ncomparison_df\n\n\n\n\nStandardize all newly transformed columns\n\nstd_scaler = StandardScaler()\ntransformed_df[new_columns_numeric] = std_scaler.fit_transform(transformed_df[new_columns_numeric])\n\n\n\n\nCategorical features processing: One- hot encoding all features\n\n# List all categorical columns in the DataFrame\ncategorical_columns  = transformed_df.select_dtypes(include=['object']).columns\n\n# Columns to exclude (make sure names match exactly)\nexclude_columns = ['mortality', 'daysbetweendeathandoperation', '@30daymortality', 'thirtydaymortality']\n\n# Filter out the excluded columns from the list of categorical columns\ncategorical_columns = [col for col in categorical_columns if col not in exclude_columns]\n\n# One-hot encode categorical columns\none_hot_ed = pd.get_dummies(transformed_df[categorical_columns],drop_first=True)\n\n# Concatenate the one-hot encoded columns with the original DataFrame\ntransformed_df = pd.concat([transformed_df, one_hot_ed], axis=1)\n\n\n\nFinal Cleaning and Consolidation of Processed Numerical and Categorical columns\n\n# Drop the original categorical columns\ntransformed_df.drop(categorical_columns, axis=1, inplace=True)\n# Drop the original numeric columns\ntransformed_df.drop(numeric_cols, axis=1, inplace=True)\n\n\ntransformed_df.info()\n\n\n\n\n\n4. In-depth analysis: Staistical modelling / Machine Learning\n\nMultivariate Analysis\n\nclass MultivariateAnalysis:\n    def __init__(self, transformed_df, target_variable, vif_threshold=10, corr_threshold=0.8):\n        self.df = transformed_df\n        self.target_variable = target_variable\n        self.vif_threshold = vif_threshold\n        self.corr_threshold = corr_threshold\n\n        # Drop target variable for analysis\n        self.features_df = self.df.drop(columns=[self.target_variable])\n\n    def preprocess_data(self):\n        # Convert Boolean columns to numeric\n        bool_cols = self.features_df.select_dtypes(include=['bool']).columns\n        self.features_df[bool_cols] = self.features_df[bool_cols].astype(int)\n\n    def plot_correlation_heatmap(self, high_corr_vars):\n        plt.figure(figsize=(20, 20))\n        corr = self.features_df[high_corr_vars].corr()\n        ax = sns.heatmap(corr, annot=True, fmt='.2g', vmin=-1, vmax=1, center=0,\n                         cmap='coolwarm_r', linecolor='black', linewidth=1, annot_kws={\"size\": 12})\n        ax.set_ylim(len(corr), 0)\n        plt.xticks(rotation=45, ha='right')\n        plt.title('Correlation Heatmap of High Correlation Variables')\n        plt.show()\n\n    def calculate_vif(self, df):\n        # Ensure only numeric columns are used\n        numeric_df = df.select_dtypes(include=['float64', 'int64'])\n        if numeric_df.empty:\n            raise ValueError(\"DataFrame contains no numeric columns.\")\n\n        # Add constant to the feature set for VIF calculation\n        numeric_df_with_const = add_constant(numeric_df, has_constant='add')\n        vif_data = pd.DataFrame()\n        vif_data[\"Variable\"] = numeric_df_with_const.columns\n        vif_data[\"VIF\"] = [variance_inflation_factor(numeric_df_with_const.values, i)\n                           for i in range(numeric_df_with_const.shape[1])]\n        return vif_data\n\n    def plot_vif(self, vif_data):\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x=\"VIF\", y=\"Variable\", data=vif_data.sort_values(\"VIF\", ascending=False))\n        plt.title('Variance Inflation Factor (VIF)')\n        plt.show()\n\n    def identify_high_corr_pairs(self):\n        corr_matrix = self.features_df.corr().abs()\n        high_corr_pairs = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n                           .stack()\n                           .reset_index()\n                           .rename(columns={0: 'correlation', 'level_0': 'feature1', 'level_1': 'feature2'}))\n        high_corr_pairs = high_corr_pairs[high_corr_pairs['correlation'] &gt; self.corr_threshold]\n        return high_corr_pairs\n\n    def drop_high_corr_vars(self, high_corr_pairs):\n        to_drop = set()\n        for _, row in high_corr_pairs.iterrows():\n            if row['feature1'] in to_drop or row['feature2'] in to_drop:\n                continue\n            feature1_corr_sum = self.features_df.corr()[row['feature1']].abs().sum()\n            feature2_corr_sum = self.features_df.corr()[row['feature2']].abs().sum()\n            if feature1_corr_sum &gt; feature2_corr_sum:\n                to_drop.add(row['feature1'])\n            else:\n                to_drop.add(row['feature2'])\n        self.features_df.drop(columns=to_drop, inplace=True)\n        return to_drop\n\n    def run_analysis(self):\n        # Preprocess data to include Boolean columns as numeric\n        self.preprocess_data()\n\n        # Identify and drop high correlation variables\n        high_corr_pairs = self.identify_high_corr_pairs()\n        dropped_corr_vars = self.drop_high_corr_vars(high_corr_pairs)\n\n        # Plot correlation heatmap for remaining variables\n        if not high_corr_pairs.empty:\n            high_corr_vars = list(set(high_corr_pairs['feature1']).union(set(high_corr_pairs['feature2'])))\n            high_corr_vars = [var for var in high_corr_vars if var in self.features_df.columns]\n            self.plot_correlation_heatmap(high_corr_vars)\n\n        # Calculate and plot VIF\n        vif_data = self.calculate_vif(self.features_df)\n        print(\"Original VIF Data:\")\n        print(vif_data)\n\n        # Variables to keep for VIF reporting\n        high_vif_vars = vif_data[vif_data[\"VIF\"] &gt; self.vif_threshold]\n\n        # Print table for high VIF variables\n        if not high_vif_vars.empty:\n            print(\"\\nVariables with high VIF:\")\n            print(high_vif_vars)\n        else:\n            print(\"\\nNo variables exceed the VIF threshold.\")\n\n        # Drop variables with high VIF\n        reduced_df = self.features_df.drop(columns=high_vif_vars['Variable'], errors='ignore')\n\n        # Recalculate VIF on reduced dataset\n        reduced_vif_data = self.calculate_vif(reduced_df)\n        print(\"\\nNew VIF Data after dropping high VIF variables:\")\n        print(reduced_vif_data)\n\n        # Plot new VIF\n        self.plot_vif(reduced_vif_data)\n\n        # Add target variable back to reduced_df\n        reduced_df[self.target_variable] = self.df[self.target_variable]\n\n        return reduced_df, dropped_corr_vars, high_vif_vars, reduced_vif_data\n\n# Example usage\n# transformed_df is your DataFrame and 'target' is your target variable\nma = MultivariateAnalysis(transformed_df, target_variable='thirtydaymortality', corr_threshold=0.6)\nreduced_df, dropped_corr_vars, high_vif_vars, new_vif = ma.run_analysis()\n\n\n\n\nThe MultivariateAnalysis class provides a comprehensive approach to handling multicollinearity in a dataset by leveraging Variance Inflation Factor (VIF) and correlation thresholds. This document summarizes the methods employed, presents the results, and evaluates the effectiveness of the analysis.\nMethods Used\n\nData Preprocessing\n\n\nObjective: Convert Boolean columns to numeric values.\nImplementation: Boolean columns are cast to integers to ensure compatibility with further analyses.\n\n\nIdentifying High Correlation Pairs\n\n\nObjective: Detect and address pairs of features with high correlation to mitigate multicollinearity.\nImplementation:\n\nCompute the correlation matrix.\nIdentify pairs with correlation values exceeding the specified threshold.\nDrop one feature from each high-correlation pair based on the overall sum of correlations.\n\n\n\nVariance Inflation Factor (VIF) Calculation\n\n\nObjective: Assess multicollinearity for each feature.\nImplementation:\n\nCompute VIF for all numeric features.\nFeatures with VIF values above the specified threshold are flagged as problematic.\n\n\n\nEvaluation\n\n\nBy identifying and removing highly correlated variables and variables with high VIF, we improved the multicollinearity in our dataset. This process helps in creating more robust and interpretable models.\nThe reduced dataset can now be used for further modeling and analysis with minimized multicollinearity issues.\n\n\n\n\nEstimate the Logistic Regression Model:\n\n# Assuming 'transformed_df' is your DataFrame and 'thirtydaymortality' is your target variable\nX = reduced_df.drop(columns=['thirtydaymortality'])\ny = reduced_df['thirtydaymortality']\n\n# Convert data to float\nX = X.astype(float)\ny = LabelEncoder().fit_transform(y)\n\n# Apply SMOTE to balance the dataset\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Perform Grid Search with Logistic Regression using scikit-learn\nlogreg = LogisticRegression(solver='liblinear')\nparams = {'C': [1, 0.1, 0.01], 'penalty': ['l1', 'l2']}\ngs_logreg = GridSearchCV(logreg, param_grid=params, cv=5, scoring='roc_auc')\ngs_logreg.fit(X_resampled, y_resampled)\n\n# Display best parameters and score\nprint(\"\\nBest estimator average accuracy on train set: {:.4f}\".format(gs_logreg.best_score_))\nprint(\"Best C = {}\".format(gs_logreg.best_params_))\n\n# Predict on the test set\nX_test = X  # Assuming you're using the same data for demonstration purposes\ny_test = y\ny_pred = gs_logreg.predict(X_test)\ny_pred_proba = gs_logreg.predict_proba(X_test)[:, 1]\n\n# Print accuracy and classification report\nprint(\"\\nAccuracy on test set: {:.4f}\".format(gs_logreg.best_estimator_.score(X_test, y_test)))\nprint(\"Test set classification report:\\n\", classification_report(y_test, y_pred))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred, labels=gs_logreg.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gs_logreg.classes_)\ndisp.plot()\nplt.title('Confusion Matrix')\nplt.show()\n\n# ROC Curve\nRocCurveDisplay.from_estimator(gs_logreg, X_test, y_test)\nplt.title('ROC Curve')\nplt.show()\n\n# Generate statistical report with p-values using statsmodels\nX_resampled_with_const = sm.add_constant(X_resampled)  # Add constant for intercept\nlogit_model_resampled = sm.Logit(y_resampled, X_resampled_with_const)\nresult_resampled = logit_model_resampled.fit_regularized()\n\n# Display the summary of the logistic regression model\nsummary_resampled = result_resampled.summary()\nprint(summary_resampled)\n\n# Set the maximum number of rows and columns to display\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# Print coefficients with p-values from the statsmodels summary\ncoef_df = pd.DataFrame({\n    'Feature': ['Intercept'] + list(X.columns),\n    'Coefficient': result_resampled.params,\n    'P-Value': result_resampled.pvalues\n})\n\n\n\n\n\n\n\nDiscussion on Results - Pseudo R-squared (0.5986): Indicates that the model explains approximately 60% of the variability in the dependent variable, which suggests a good fit.\n\nLog-Likelihood (-47051): Measures how well the model fits the data. A higher (less negative) value would indicate a better fit.\nCoefficients :\n\nage_no_transform (1.3001): For each unit increase in age, the log-odds of the event occurring increase by 1.3001. This translates to an increased probability of the event with age. Converting this to odds: an increase in age by one unit multiplies the odds of the event by exp(1.3001) ≈ 3.68. Hence, older individuals have significantly higher odds of the event occurring.\ngender_male (0.4632): Being male increases the log-odds of the event by 0.4632. In terms of odds: exp(0.4632) ≈ 1.588. Thus, being male increases the odds of the event occurring by approximately 58.8%.\n\nSignificance (P&gt;|z|): Most coefficients have p-values of 0.000, indicating that these predictors are statistically significant. Coefficients with higher p-values (e.g., agecategory_30-49 (0.091)) are less significant, meaning they may not be as impactful in predicting the outcome.\n\n\n\n\nMachine Learning (Individual Models)\nnot needed here, we will proceed straight to AutoML with the models\n\n\n\n5. Comparison and Evaluation: Automate the evaluation process\nAutomate the model training and evaluation process, and generate comprehensive results and key evaluation reports to assess the validity and effectiveness of the chosen model.\n\nclass BaseModel:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123):\n        self.df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n\n        # Prepare features and target variable\n        self.X = self.df.drop(columns=[self.target_column])\n        self.y = self.df[self.target_column]\n\n        \n        # Label encode the target variable\n        label_encoder = LabelEncoder()\n        self.y = label_encoder.fit_transform(self.y)\n\n\n        # Split the data into training and test sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, test_size=0.3, random_state=self.random_state\n        )\n\n        # Initialize SMOTEENN if balance is set to 'imbalanced'\n        if self.balance == 'imbalanced':\n            self.smoteenn = SMOTEENN(sampling_strategy='auto', random_state=self.random_state)\n        else:\n            self.smoteenn = None\n\n    def apply_smoteenn(self):\n        if self.smoteenn:\n            return self.smoteenn.fit_resample(self.X_train, self.y_train)\n        else:\n            return self.X_train, self.y_train\n\n    def show_matrix(self, matrix, title='Confusion Matrix'):\n        # Plotting the confusion matrix\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n        plt.title(title)\n        plt.ylabel('Actual Class')\n        plt.xlabel('Predicted Class')\n        plt.show()\n\n    def plot_roc_auc(self, y_true, y_pred, title):\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\nclass DT(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = DecisionTreeClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Decision Tree...\")\n        # Define the parameter space\n        search_spaces = {\n            'max_features': Categorical(['sqrt', 'log2']),\n            'ccp_alpha': Real(0.001, 0.1, prior='log-uniform'),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass RandomForest(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = RandomForestClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Random Forest...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_features': Categorical(['sqrt', 'log2']),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass XGBoost(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for XGBoost...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass LightGBM(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = LGBMClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for LightGBM...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass AutoML:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, secondary_metric='precision'):\n        self.cleaned_df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.secondary_metric = secondary_metric\n        \n        # Initialize model classes\n        self.models = {\n            'DecisionTree': DT(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'RandomForest': RandomForest(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'XGBoost': XGBoost(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'LightGBM': LightGBM(cleaned_df, target_column, balance, n_splits, n_repeats, random_state)\n        }\n\n    def evaluate_models(self):\n        results = {}\n        for name, model in self.models.items():\n            print(f'\\nEvaluating {name}...')\n            best_model, y_pred = model.run_bayesian_search()\n            \n            # Compute metrics\n            accuracy = accuracy_score(model.y_test, y_pred)\n            precision = precision_score(model.y_test, y_pred, average='weighted')\n            recall = recall_score(model.y_test, y_pred, average='weighted')\n            f1 = 2 * (precision * recall) / (precision + recall)\n            \n            results[name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'classification_report': classification_report(model.y_test, y_pred),\n                'confusion_matrix': model.show_matrix(confusion_matrix(model.y_test, y_pred))\n            }\n            \n            # Print Classification Report and Confusion Matrix\n            print(f'Classification Report for {name}:\\n{results[name][\"classification_report\"]}')\n            \n            # ROC and AUC\n            y_test_binarized = label_binarize(model.y_test, classes=np.unique(model.y_test))\n            y_pred_binarized = label_binarize(y_pred, classes=np.unique(model.y_test))\n            if y_test_binarized.shape[1] &gt; 1:\n                for i in range(y_test_binarized.shape[1]):\n                    model.plot_roc_auc(y_test_binarized[:, i], y_pred_binarized[:, i], f'{name} ROC Curve for Class {i}')\n            \n        # Determine the best model\n        sorted_results = sorted(results.items(), key=lambda x: (x[1]['accuracy'], x[1][self.secondary_metric]), reverse=True)\n        best_model_name, best_model_metrics = sorted_results[0]\n        print(f'\\nBest Model: {best_model_name}')\n        print(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\n        print(f'{self.secondary_metric.capitalize()}: {best_model_metrics[self.secondary_metric]}')\n        \n        return best_model_name, best_model_metrics\n\n# Example usage\n# Assuming `transformed_df` is your DataFrame and 'thirtydaymortality' is your target column\nauto_ml = AutoML(reduced_df, target_column='thirtydaymortality', balance='imbalanced', secondary_metric='recall')\nbest_model_name, best_model_metrics = auto_ml.evaluate_models()\n\nprint(f'\\nBest Model: {best_model_name}')\nprint(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\nprint(f'Precision: {best_model_metrics[\"precision\"]}')\nprint(f'Recall: {best_model_metrics[\"recall\"]}')\nprint(f'F1 Score: {best_model_metrics[\"f1\"]}')\nprint(f'Confusion Matrix:\\n{best_model_metrics[\"confusion_matrix\"]}')\n\nEvaluating DecisionTree…\nTime taken = 38.33 sec\nBest score = 0.882 using params: OrderedDict([(‘ccp_alpha’, 0.001131086677421503), (‘criterion’, ‘entropy’), (‘max_depth’, 24), (‘max_features’, ‘log2’), (‘min_samples_leaf’, 3), (‘min_samples_split’, 3)])\nTest accuracy = 0.85\n\n\nEvaluating RandomForest…\nTime taken = 891.38 sec Best score = 0.97 using params: OrderedDict([(‘criterion’, ‘entropy’), (‘max_depth’, 27), (‘max_features’, ‘sqrt’), (‘min_samples_leaf’, 2), (‘min_samples_split’, 3), (‘n_estimators’, 82)]) Test accuracy = 0.956\n\n\nEvaluating XGBoost\nTime taken = 487.2 sec Best score = 0.973 using params: OrderedDict([(‘colsample_bytree’, 0.8636816590466002), (‘learning_rate’, 0.0645354435221669), (‘max_depth’, 29), (‘n_estimators’, 108), (‘subsample’, 0.9220560088509978)]) Test accuracy = 0.952\n\n\nEvaluating LightGBM…\nTime taken = 230.12 sec Best score = 0.958 using params: OrderedDict([(‘colsample_bytree’, 0.5133739717626367), (‘learning_rate’, 0.1941545477346783), (‘max_depth’, 24), (‘n_estimators’, 131), (‘subsample’, 0.8487299825534435)]) Test accuracy = 0.941\n\n\n\n\n\n\nBest Model: RandomForest\nAccuracy: 0.9561266060795989\nRecall: 0.9561266060795989\nBest Model: RandomForest\nAccuracy: 0.9561266060795989\nPrecision: 0.9905793259715808\nRecall: 0.9561266060795989\nF1 Score: 0.9730480946301695\n\n\n6. Tuning Model to industry and domain knowledge: avoiding false negatives\nIn cancer diagnostics, avoiding false negatives is particularly crucial. Missing a cancer diagnosis can lead to delayed treatment, disease progression, and reduced survival rates, which are far more serious than the consequences of false positives, such as psychological distress, additional testing, or unnecessary treatment. Consequently, cancer screening programs tend to prioritize AUC/sensitivity to minimize the number of missed cases, even if it results in a higher rate of false positives.\nIn this context, we fit and test the models similarly to the previous approach, but we specifically evaluate and select the best model based on sensitivity.\n\nclass BaseModel:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, evaluation_metric='recall'):\n        self.df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.evaluation_metric = evaluation_metric\n\n        # Prepare features and target variable\n        self.X = self.df.drop(columns=[self.target_column])\n        self.y = self.df[self.target_column]\n\n        # Label encode the target variable\n        label_encoder = LabelEncoder()\n        self.y = label_encoder.fit_transform(self.y)\n\n        # Split the data into training and test sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, test_size=0.3, random_state=self.random_state\n        )\n\n        # Initialize SMOTEENN if balance is set to 'imbalanced'\n        if self.balance == 'imbalanced':\n            self.smoteenn = SMOTEENN(sampling_strategy='auto', random_state=self.random_state)\n        else:\n            self.smoteenn = None\n\n    def apply_smoteenn(self):\n        if self.smoteenn:\n            return self.smoteenn.fit_resample(self.X_train, self.y_train)\n        else:\n            return self.X_train, self.y_train\n\n    def show_matrix(self, matrix, title='Confusion Matrix'):\n        # Plotting the confusion matrix\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n        plt.title(title)\n        plt.ylabel('Actual Class')\n        plt.xlabel('Predicted Class')\n        plt.show()\n\n    def plot_roc_auc(self, y_true, y_pred, title):\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.legend(loc=\"lower right\")\n        plt.show()\n        return roc_auc\n\n\n    def print_feature_importance(self, model_name):\n      if hasattr(self.clf, 'feature_importances_'):\n        importance = self.clf.feature_importances_\n        feature_names = self.X.columns\n\n        # Create a DataFrame for feature importances and sort it\n        feature_importance_df = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': importance\n        })\n        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n        # Plot feature importances\n        plt.figure(figsize=(10, 6))\n        plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n        plt.xlabel('Feature Importance Score')\n        plt.title(f'Feature Importance for {model_name}')\n        plt.show()\n\n    def print_tree_structure(self):\n      if isinstance(self.clf, tree.DecisionTreeClassifier):\n        # Plot the tree structure\n        plt.figure(figsize=(20, 10))\n        plot_tree(self.clf, feature_names=self.X.columns, filled=True, rounded=True)\n        plt.title(\"Decision Tree Visualization\")\n        plt.show()\n\nclass DT(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = tree.DecisionTreeClassifier(class_weight='balanced')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Decision Tree...\")\n        # Define the parameter space\n        search_spaces = {\n            'max_features': Categorical(['sqrt', 'log2']),\n            'ccp_alpha': Real(0.001, 0.1, prior='log-uniform'),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n\n      # Ensure the model is fitted before calling methods\n        if hasattr(self.clf, 'feature_importances_') or isinstance(self.clf, tree.DecisionTreeClassifier):\n            self.print_tree_structure()\n            self.print_feature_importance(\"Decision Tree\")\n\n        return bayes_result, y_pred\n\nclass RandomForest(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = RandomForestClassifier(class_weight='balanced')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Random Forest...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_features': Categorical(['sqrt', 'log2']),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        self.print_feature_importance(\"Random Forest\")\n        return bayes_result, y_pred\n\nclass XGBoost(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scale_pos_weight = len(self.y_train[self.y_train == 0]) / len(self.y_train[self.y_train == 1])\n        self.clf = xgb.XGBClassifier(\n            use_label_encoder=False,\n            eval_metric='logloss',\n            scale_pos_weight=self.scale_pos_weight\n        )\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for XGBoost...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        self.print_feature_importance(\"XGBoost\")\n        return bayes_result, y_pred\n\nclass LightGBM(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scale_pos_weight = len(self.y_train[self.y_train == 0]) / len(self.y_train[self.y_train == 1])\n        self.class_weight = {0: 1, 1: self.scale_pos_weight}\n        self.clf = LGBMClassifier(class_weight=self.class_weight)\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for LightGBM...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring=self.evaluation_metric, random_state=self.random_state, verbose=1, n_jobs=-1)\n\n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n\n        # Update self.clf with the best estimator\n        self.clf = bayes_result.best_estimator_\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n\n        self.print_feature_importance(\"LightGBM\")\n        return bayes_result, y_pred\n\nclass AutoML:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, evaluation_metric='recall'):\n        self.cleaned_df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.evaluation_metric = evaluation_metric\n\n        # Initialize model classes\n        self.models = {\n            'DecisionTree': DT(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'RandomForest': RandomForest(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'XGBoost': XGBoost(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'LightGBM': LightGBM(cleaned_df, target_column, balance, n_splits, n_repeats, random_state)\n        }\n\n    def evaluate_models(self):\n        results = {}\n        for name, model in self.models.items():\n            print(f'\\nEvaluating {name}...')\n            best_model, y_pred = model.run_bayesian_search()\n\n            # Compute metrics\n            accuracy = accuracy_score(model.y_test, y_pred)\n            precision = precision_score(model.y_test, y_pred, average='weighted')\n            recall = recall_score(model.y_test, y_pred, average='weighted')\n            f1 = f1_score(model.y_test, y_pred, average='weighted')\n            cm = confusion_matrix(model.y_test, y_pred)\n            tn, fp, fn, tp = cm.ravel()\n\n            # Calculate sensitivity (recall for positive class)\n            sensitivity = recall_score(model.y_test, y_pred, pos_label=1)\n\n            # Calculate specificity\n            specificity = tn / (tn + fp) if (tn + fp) &gt; 0 else 0\n\n            # Determine false negatives\n            false_negatives = fn\n\n            # Determine false positives\n            false_positives = fp\n\n\n            # ROC and AUC\n            y_test_binarized = label_binarize(model.y_test, classes=np.unique(model.y_test))\n            y_pred_binarized = label_binarize(y_pred, classes=np.unique(model.y_test))\n            if y_test_binarized.shape[1] &gt; 1:\n                auc_scores = []\n                for i in range(y_test_binarized.shape[1]):\n                    auc = model.plot_roc_auc(y_test_binarized[:, i], y_pred_binarized[:, i], f'{name} ROC Curve for Class {i}')\n                    auc_scores.append(auc)\n                mean_auc = np.mean(auc_scores)\n            else:\n                auc = model.plot_roc_auc(model.y_test, y_pred, f'{name} ROC Curve')\n                mean_auc = auc\n\n            # Store results\n            results[name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'false_negatives': false_negatives,\n                'false_positives': false_positives,\n                'sensitivity': sensitivity,\n                'specificity': specificity,\n                'classification_report': classification_report(model.y_test, y_pred),\n                'confusion_matrix': model.show_matrix(cm),\n                'auc': mean_auc\n            }\n\n            # Print Classification Report, Confusion Matrix, and AUC\n            print(f'Classification Report for {name}:\\n{results[name][\"classification_report\"]}')\n            print(f'AUC Score for {name}: {results[name][\"auc\"]}')\n            print(f'Sensitivity for {name}: {results[name][\"sensitivity\"]}')\n            print(f'Specificity for {name}: {results[name][\"specificity\"]}')\n\n        # Determine the best model\n        if self.evaluation_metric in ['false_negatives', 'false_positives']:\n            sorted_results = sorted(results.items(), key=lambda x: x[1][self.evaluation_metric])\n        else:\n            sorted_results = sorted(results.items(), key=lambda x: x[1][self.evaluation_metric], reverse=True)\n\n        best_model_name, best_model_metrics = sorted_results[0]\n        print(f'\\nBest Model: {best_model_name}')\n        print(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\n        print(f'Precision: {best_model_metrics[\"precision\"]}')\n        print(f'Recall: {best_model_metrics[\"recall\"]}')\n        print(f'F1 Score: {best_model_metrics[\"f1\"]}')\n        print(f'False Negatives: {best_model_metrics[\"false_negatives\"]}')\n        print(f'False Positives: {best_model_metrics[\"false_positives\"]}')\n        print(f'AUC Score: {best_model_metrics[\"auc\"]}')\n        print(f'Confusion Matrix:\\n{best_model_metrics[\"confusion_matrix\"]}')\n\n        return best_model_name, best_model_metrics\n\n\n# Example usage\n# Assuming `reduced_df` is your DataFrame and 'thirtydaymortality' is your target column\nauto_ml = AutoML(reduced_df, target_column='thirtydaymortality', balance='imbalanced', evaluation_metric='auc')\nbest_model_name, best_model_metrics = auto_ml.evaluate_models()\n\nEvaluating DecisionTree…\nTime taken = 43.36 sec\nBest score = 0.921 using params: OrderedDict([(‘ccp_alpha’, 0.001131086677421503), (‘criterion’, ‘entropy’), (‘max_depth’, 24), (‘max_features’, ‘log2’), (‘min_samples_leaf’, 3), (‘min_samples_split’, 3)])\nTest accuracy = 0.853\n\n\n\n\nEvaluating RandomForest…\nTime taken = 1131.54 sec Best score = 0.99 using params: OrderedDict([(‘criterion’, ‘entropy’), (‘max_depth’, 27), (‘max_features’, ‘sqrt’), (‘min_samples_leaf’, 2), (‘min_samples_split’, 3), (‘n_estimators’, 82)])\nTest accuracy = 0.975\n\n\n\n\nEvaluating XGBoost…\nTime taken = 569.31 sec Best score = 0.999 using params: OrderedDict([(‘colsample_bytree’, 0.6654338738457878), (‘learning_rate’, 0.038882847597077025), (‘max_depth’, 21), (‘n_estimators’, 127), (‘subsample’, 0.7398225001914931)])\nTest accuracy = 0.91\n\n\n\n\nEvaluating LightGBM…\nTime taken = 259.26 sec Best score = 1.0 using params: OrderedDict([(‘colsample_bytree’, 0.8840100626990215), (‘learning_rate’, 0.017347709137475083), (‘max_depth’, 20), (‘n_estimators’, 132), (‘subsample’, 0.9722507666816909)])\nTest accuracy = 0.67\n\n\n\n\n\n\n7. Model Evaluation Summary\n\n1. Decision Tree\nBest Parameters: - ccp_alpha: 0.0011 - criterion: ‘entropy’ - max_depth: 24 - max_features: ‘log2’ - min_samples_leaf: 3 - min_samples_split: 3\nTest Accuracy: 0.853\nClassification Report:\n\n\n\nMetric\nClass 0\nClass 1\n\n\n\n\nPrecision\n1.00\n0.03\n\n\nRecall\n0.85\n0.76\n\n\nF1-Score\n0.92\n0.06\n\n\nMacro Avg\n0.51\n0.81\n\n\nWeighted Avg\n0.99\n0.85\n\n\n\nAUC Score: 0.808\nSensitivity: 0.764\nSpecificity: 0.853\nDiscussion:\n\nStrengths: The Decision Tree model has high accuracy (0.853) and relatively good recall for the majority class (0.76 for class 1). The AUC score of 0.808 indicates decent performance in distinguishing between classes.\nWeaknesses: Precision for the minority class is very low (0.03), and the F1-Score is also very low (0.06), reflecting poor performance in correctly identifying the positive class (class 1). The model struggles with class imbalance.\n\n\n\n2. Random Forest\nBest Parameters: - criterion: ‘entropy’ - max_depth: 27 - max_features: ‘sqrt’ - min_samples_leaf: 2 - min_samples_split: 3 - n_estimators: 82\nTest Accuracy: 0.975\nClassification Report:\n\n\n\nMetric\nClass 0\nClass 1\n\n\n\n\nPrecision\n1.00\n0.09\n\n\nRecall\n0.98\n0.35\n\n\nF1-Score\n0.99\n0.14\n\n\nMacro Avg\n0.54\n0.66\n\n\nWeighted Avg\n0.99\n0.97\n\n\n\nAUC Score: 0.665\nSensitivity: 0.351\nSpecificity: 0.978\nDiscussion:\n\nStrengths: High accuracy (0.975) and good recall for the majority class (0.35 for class 1). The Random Forest model shows strong performance for the majority class.\nWeaknesses: Precision for the minority class is extremely low (0.09), indicating poor performance in identifying the positive class. The AUC score of 0.665 suggests weaker discriminatory performance compared to other models.\n\n\n\n3. XGBoost\nBest Parameters: - colsample_bytree: 0.665 - learning_rate: 0.039 - max_depth: 21 - n_estimators: 127 - subsample: 0.740\nTest Accuracy: 0.910\nClassification Report:\n\n\n\nMetric\nClass 0\nClass 1\n\n\n\n\nPrecision\n1.00\n0.03\n\n\nRecall\n0.91\n0.53\n\n\nF1-Score\n0.95\n0.06\n\n\nMacro Avg\n0.52\n0.72\n\n\nWeighted Avg\n0.99\n0.91\n\n\n\nAUC Score: 0.723\nSensitivity: 0.534\nSpecificity: 0.913\nDiscussion:\n\nStrengths: High accuracy (0.910) and good recall for the majority class (0.53 for class 1).\nWeaknesses: The XGBoost model has low precision (0.03) and F1-Score (0.06) for the minority class. The AUC score of 0.723 shows limited effectiveness in distinguishing between classes.\n\n\n\n4. LightGBM\nBest Parameters: - colsample_bytree: 0.884 - learning_rate: 0.017 - max_depth: 20 - n_estimators: 132 - subsample: 0.972\nTest Accuracy: 0.670\nClassification Report:\n\n\n\nMetric\nClass 0\nClass 1\n\n\n\n\nPrecision\n1.00\n0.01\n\n\nRecall\n0.67\n0.84\n\n\nF1-Score\n0.80\n0.03\n\n\nMacro Avg\n0.51\n0.76\n\n\nWeighted Avg\n0.99\n0.67\n\n\n\nAUC Score: 0.757\nSensitivity: 0.845\nSpecificity: 0.669\nDiscussion:\n\nStrengths: High recall for the minority class (0.84). The model achieves a good balance between precision and recall for the majority class.\nWeaknesses: Low precision (0.01) and F1-Score (0.03) for the minority class. The AUC score of 0.757 indicates limited ability to discriminate between the classes.\n\n\n\nBest Model (AUC): Decision Tree\nAccuracy: 0.853\nPrecision: 0.993 (for class 0), 0.03 (for class 1)\nRecall: 0.85 (for class 0), 0.76 (for class 1)\nF1 Score: 0.92 (for class 0), 0.06 (for class 1)\nFalse Negatives: 35\nAUC Score: 0.808\nDiscussion:\n\nStrengths: The Decision Tree model has the highest recall for the minority class among the models evaluated, with an AUC score of 0.808, indicating better performance in distinguishing between classes compared to other models.\nWeaknesses: Precision for the minority class remains low, which is a common issue in imbalanced datasets. The F1-Score for the minority class is also low, indicating that while the recall is better, precision is lacking.\n\n\n\nOverall Recommendations\nTo improve performance, consider the following strategies:\n\nResampling: Experiment with more methods to deal with imbalance.\nClass Weights: Adjust class weights in the model to give more importance to the minority class.\nEnsemble Methods: Combine predictions from multiple models to improve performance on the minority class.\nSVMs: Consider using SVMs with dimensionality reduction techniques instead of dropping potentially critical variables.\n\n\n\nPerformance Metrics Comparison\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall (Sensitivity)\nSpecificity\nAUC\n\n\n\n\nDecision Tree\n0.853\n0.993\n0.764\n0.853\n0.808\n\n\nRandom Forest\n0.975\n0.990\n0.351\n0.978\n0.665\n\n\nXGBoost\n0.910\n0.999\n0.534\n0.913\n0.723\n\n\nLightGBM\n0.670\n1.000\n0.845\n0.669\n0.757\n\n\n\n\nUpon fine-tuning to context: Minimising False Negatives\nThe Decision Tree model performs best in terms of recall for the minority class and AUC score among the models evaluated.\nHOWEVER, sensitivity score (0.845) is higher for LightGBM instead even though it has less AUC score (0.757). LightGBM is the ideal model for our scenario."
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#outcome-1-mortality-within-30-days-thirtydaymortality",
    "href": "Python/AutoML_Binary_Classification.html#outcome-1-mortality-within-30-days-thirtydaymortality",
    "title": "AutoML for Binary Classification",
    "section": "Outcome 1: Mortality within 30 days (thirtydaymortality)",
    "text": "Outcome 1: Mortality within 30 days (thirtydaymortality)\n\n1. Frame the Problem\n\nTarget variable: thirtydaymortality\nType: Classification Problem\nBinary Outcome: (No,Yes)\n\n\n\n2. Data sensing, preprocessing:\n\nFeature Selection: Exclusion of variables that deomonstrates high redundancy or Multicollinearity with thirtydaymortality:\n\n@30daymortality\nmortality\ndaysbetweendeathandoperation\n\n\n\n# Feature Selection:\n# drop unecessary columns, features that occur only after the target result should be excluded\nfeatures_to_exclude = ['mortality', 'daysbetweendeathandoperation', '@30daymortality']\ndf = df.drop(columns=features_to_exclude)\n\n\n\n3. Data Wrangling, Transformation:\nNumerical features Transformation:\n\nTest for normality of numeric columns: ALL are not normally distributed\nDIstribution transfomation for columns\nDistribution normalisation and standardisation for columns\n\nCateogorical features Transformation:\n\nOne-hot encoding to ensure smooth usage by tree learning algorithms\n\n\nNumerical features Transformation\n\nTest for Normality\n\n# Test for normality of numeric columns: ALL are not normally distributed\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Define specific numeric columns for imputation\nnumeric_cols = [col for col in numeric_columns if col != 'daysbetweendeathandoperation']\nprint(\"Numeric columns:\", list(numeric_cols))\n\n# Function to test for normal distribution\ndef test_normal_distribution(data, alpha=0.05):\n    \"\"\"\n    Test if the data follows a normal distribution.\n    \"\"\"\n    shapiro_stat, shapiro_p = stats.shapiro(data)\n    ks_stat, ks_p = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data, ddof=0)))\n\n    print(f\"Shapiro-Wilk: Statistic={shapiro_stat:.3f}, p-value={shapiro_p:.3f}\")\n    print(f\"Kolmogorov-Smirnov: Statistic={ks_stat:.3f}, p-value={ks_p:.3f}\")\n    print(\"Normally distributed:\" if shapiro_p &gt; alpha and ks_p &gt; alpha else \"Not normally distributed.\")\n    print()\n\n# Test normal distribution\nprint(\"Testing Normal Distribution:\")\nfor col in numeric_cols:\n    print(f\"{col}:\")\n    test_normal_distribution(df[col])\n\ndef create_qq_plots(columns):\n    # Number of columns to plot\n    num_cols = len(columns)\n\n    # Determine grid size for subplots\n    n_cols = 3  # Number of columns in the subplot grid\n    n_rows = (num_cols + n_cols - 1) // n_cols  # Calculate number of rows needed\n\n    # Create subplots\n    plt.figure(figsize=(15, 5 * n_rows))  # Adjust figure size as needed\n\n    for i, col in enumerate(columns, 1):\n        plt.subplot(n_rows, n_cols, i)\n        stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)\n        plt.title(f'Q-Q Plot of {col}')\n        plt.xlabel('Theoretical Quantiles')\n        plt.ylabel('Sample Quantiles')\n\n    plt.tight_layout()\n    plt.show()\n\n# Create Q-Q plots for numeric columns\ncreate_qq_plots(numeric_cols)\n\n\n\n\n\n\n\nDistribution of numeric columns\n\ndef create_histograms(df,columns):\n    # Number of columns to plot\n    num_cols = len(columns)\n\n    # Determine grid size for subplots\n    n_cols = 3  # Number of columns in the subplot grid\n    n_rows = (num_cols + n_cols - 1) // n_cols  # Calculate number of rows needed\n\n    # Create subplots\n    plt.figure(figsize=(15, 5 * n_rows))  # Adjust figure size as needed\n\n    for i, col in enumerate(columns, 1):\n        plt.subplot(n_rows, n_cols, i)\n        sns.histplot(df[col].dropna(), kde=True, bins=30)  # Adjust bins as needed\n        plt.title(f'Distribution of {col}')\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n\n    plt.tight_layout()\n    plt.show()\n\n\n# Create histograms for numeric columns\ncreate_histograms(df,numeric_cols)\n\n\n\n\n\n\nAutomation of transforming numeric columns based on skewness\n\ndef calculate_skewness(df,columns):\n    # Dictionary to store skewness values\n    skewness_dict = {}\n\n    for col in columns:\n        # Drop NaN values for skewness calculation, there shouldn't be any left\n        data = df[col].dropna()\n\n        # Calculate skewness\n        skew_value = skew(data, nan_policy='omit')  # nan_policy='omit' ignores NaN values\n        skewness_dict[col] = skew_value\n\n    # Convert skewness dictionary to DataFrame for better readability\n    skewness_df = pd.DataFrame(list(skewness_dict.items()), columns=['Column', 'Skewness'])\n    return skewness_df\n\n\n# Calculate skewness for numeric columns\nskewness_df = calculate_skewness(df,numeric_cols)\n\n# Print skewness values\nprint(\"Skewness of Numeric Columns:\")\nprint(skewness_df)\n\n\n\ndef transform_based_on_skewness(df, columns):\n    transformed_df = df.copy()\n    new_columns_numeric = []\n    column_mapping = {}\n\n    for col in columns:\n        # Calculate skewness\n        skewness = skew(df[col].dropna(), nan_policy='omit')\n\n        # Initialize new_col to None\n        new_col = None\n\n        # Print the skewness for debugging\n        print(f\"Skewness for column {col}: {skewness}\")\n\n        # Choose transformation based on skewness\n        if skewness &gt; 1:\n            # Highly positively skewed\n            if (transformed_df[col] &gt; 0).all():  # Check if all values are positive\n                new_col = col + '_log'\n                transformed_df[new_col] = np.log1p(df[col])\n                print(f\"Applied log transformation on column {col}\")\n            else:\n                # Use Yeo-Johnson if data contains zero or negative values\n                new_col = col + '_yeojohnson'\n                pt = PowerTransformer(method='yeo-johnson')\n                transformed_df[new_col] = pt.fit_transform(df[[col]])\n                print(f\"Applied Yeo-Johnson transformation on column {col}\")\n\n        elif skewness &gt; 0.5:\n            # Moderately positively skewed\n            new_col = col + '_sqrt'\n            transformed_df[new_col] = np.sqrt(df[col] + 1)  # Adding 1 to handle zero values\n            print(f\"Applied square root transformation on column {col}\")\n\n        elif skewness &lt; -1:\n            # Highly negatively skewed\n            new_col = col + '_inv'\n            transformed_df[new_col] = 1 / (df[col] + 1)  # Adding 1 to avoid division by zero\n            print(f\"Applied inverse transformation on column {col}\")\n\n        elif skewness &lt; -0.5:\n            # Moderately negatively skewed\n            new_col = col + '_inv_sqrt'\n            transformed_df[new_col] = 1 / np.sqrt(df[col] + 1)  # Adding 1 to handle zero values\n            print(f\"Applied inverse square root transformation on column {col}\")\n\n        else:\n            # Data is close to normal, no transformation needed\n            new_col = col + '_no_transform'\n            transformed_df[new_col] = df[col]\n\n       \n        if new_col:\n            new_columns_numeric.append(new_col)\n            column_mapping[col] = new_col\n\n    return transformed_df, new_columns_numeric, column_mapping\n\n# Example usage\n# Assuming df is your DataFrame\nnumeric_cols = ['age', 'rcri_score', 'preopegfrmdrd', 'preoptransfusionwithin30days',\n                'intraop', 'postopwithin30days', 'transfusionintraandpostop']\n\n# Apply transformations based on skewness\ntransformed_df, new_columns_numeric, column_mapping = transform_based_on_skewness(df, numeric_cols)\n\n\n# Print the first few rows of the transformed DataFrame to verify\nprint(transformed_df.head())\n# Print the list of newly transformed columns\nprint(new_columns_numeric)\n\n# Verify the columns are indeed in the DataFrame\nfor col in new_columns_numeric:\n    if col in transformed_df.columns:\n        print(f\"Column {col} exists in the DataFrame.\")\n    else:\n        print(f\"Column {col} does NOT exist in the DataFrame.\")\n\n\n\n# Create histograms for newly transformed columns\ncreate_histograms(transformed_df,new_columns_numeric)\n\n\n\n\n\n\n\n\nComparison of newly transformed variables as compared to orignal in terms of skewness\n\n# Calculate skewness for original columns\nskewness_original = calculate_skewness(df,numeric_cols).set_index('Column')['Skewness'].to_dict()\n\n# Calculate skewness for transformed columns\nskewness_transformed = calculate_skewness(transformed_df,new_columns_numeric).set_index('Column')['Skewness'].to_dict()\n\n# Print column mapping\nprint(\"Column Mapping:\")\nprint(column_mapping)\n\n\n# Print skewness values\nprint(\"Skewness of Numeric Columns:\")\nprint(skewness_df)\nprint(skewness_transformed)\n\n# Print column mapping\nprint (column_mapping)\n\n# Create a comparison DataFrame\ncomparison_df = pd.DataFrame({\n    'Original Column': numeric_cols,\n    'Original Skewness': [skewness_original[col] for col in numeric_cols],\n    'Transformed Column': [column_mapping[col] if col in column_mapping else col + '_no_transform' for col in numeric_cols],\n    'Transformed Skewness': [skewness_transformed[column_mapping[col]] if col in column_mapping else skewness_original[col] for col in numeric_cols]\n})\n\n# Add a column to indicate if skewness has decreased\ncomparison_df['Skewness Decreased'] = abs(comparison_df['Original Skewness']) &gt; abs(comparison_df['Transformed Skewness'])\n\nprint(\"Skewness Comparison:\")\ncomparison_df\n\n\n\n\n\n\nStandardize all newly transformed columns\n\nstd_scaler = StandardScaler()\ntransformed_df[new_columns_numeric] = std_scaler.fit_transform(transformed_df[new_columns_numeric])\n\n\n\n\nCategorical features processing: One- hot encoding all features\n\n# List all categorical columns in the DataFrame\ncategorical_columns  = transformed_df.select_dtypes(include=['object']).columns\n\n# Columns to exclude (make sure names match exactly)\nexclude_columns = ['mortality', 'daysbetweendeathandoperation', '@30daymortality', 'thirtydaymortality']\n\n# Filter out the excluded columns from the list of categorical columns\ncategorical_columns = [col for col in categorical_columns if col not in exclude_columns]\n\n# One-hot encode categorical columns\none_hot_ed = pd.get_dummies(transformed_df[categorical_columns],drop_first=True)\n\n# Concatenate the one-hot encoded columns with the original DataFrame\ntransformed_df = pd.concat([transformed_df, one_hot_ed], axis=1)\n\n\n\nFinal Cleaning and Consolidation of Processed Numerical and Categorical columns\n\n# Drop the original categorical columns\ntransformed_df.drop(categorical_columns, axis=1, inplace=True)\n# Drop the original numeric columns\ntransformed_df.drop(numeric_cols, axis=1, inplace=True)\n\n\ntransformed_df.info()\n\n\n\n\n\n\n\n4. In-depth analysis: Satistical modelling / Machine Learning\n\nMultivariate Analysis\n\nclass MultivariateAnalysis:\n    def __init__(self, transformed_df, target_variable, vif_threshold=10, corr_threshold=0.8):\n        self.df = transformed_df\n        self.target_variable = target_variable\n        self.vif_threshold = vif_threshold\n        self.corr_threshold = corr_threshold\n        \n        # Drop target variable for analysis\n        self.features_df = self.df.drop(columns=[self.target_variable])\n\n    def preprocess_data(self):\n        # Convert Boolean columns to numeric\n        bool_cols = self.features_df.select_dtypes(include=['bool']).columns\n        self.features_df[bool_cols] = self.features_df[bool_cols].astype(int)\n\n    def plot_correlation_heatmap(self, high_corr_vars):\n        plt.figure(figsize=(20, 20))\n        corr = self.features_df[high_corr_vars].corr()\n        ax = sns.heatmap(corr, annot=True, fmt='.2g', vmin=-1, vmax=1, center=0,\n                         cmap='coolwarm_r', linecolor='black', linewidth=1, annot_kws={\"size\": 12})\n        ax.set_ylim(len(corr), 0)\n        plt.xticks(rotation=45, ha='right')\n        plt.title('Correlation Heatmap of High Correlation Variables')\n        plt.show()\n\n    def calculate_vif(self, df):\n        # Ensure only numeric columns are used\n        numeric_df = df.select_dtypes(include=['float64', 'int64'])\n        if numeric_df.empty:\n            raise ValueError(\"DataFrame contains no numeric columns.\")\n        \n        # Add constant to the feature set for VIF calculation\n        numeric_df_with_const = add_constant(numeric_df, has_constant='add')\n        vif_data = pd.DataFrame()\n        vif_data[\"Variable\"] = numeric_df_with_const.columns\n        vif_data[\"VIF\"] = [variance_inflation_factor(numeric_df_with_const.values, i) \n                           for i in range(numeric_df_with_const.shape[1])]\n        return vif_data\n\n    def plot_vif(self, vif_data):\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x=\"VIF\", y=\"Variable\", data=vif_data.sort_values(\"VIF\", ascending=False))\n        plt.title('Variance Inflation Factor (VIF)')\n        plt.show()\n\n    def identify_high_corr_pairs(self):\n        corr_matrix = self.features_df.corr().abs()\n        high_corr_pairs = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n                           .stack()\n                           .reset_index()\n                           .rename(columns={0: 'correlation', 'level_0': 'feature1', 'level_1': 'feature2'}))\n        high_corr_pairs = high_corr_pairs[high_corr_pairs['correlation'] &gt; self.corr_threshold]\n        return high_corr_pairs\n\n    def drop_high_corr_vars(self, high_corr_pairs):\n        to_drop = set()\n        for _, row in high_corr_pairs.iterrows():\n            if row['feature1'] in to_drop or row['feature2'] in to_drop:\n                continue\n            feature1_corr_sum = self.features_df.corr()[row['feature1']].abs().sum()\n            feature2_corr_sum = self.features_df.corr()[row['feature2']].abs().sum()\n            if feature1_corr_sum &gt; feature2_corr_sum:\n                to_drop.add(row['feature1'])\n            else:\n                to_drop.add(row['feature2'])\n        self.features_df.drop(columns=to_drop, inplace=True)\n        return to_drop\n\n    def run_analysis(self):\n        # Preprocess data to include Boolean columns as numeric\n        self.preprocess_data()\n        \n        # Identify and drop high correlation variables\n        high_corr_pairs = self.identify_high_corr_pairs()\n        dropped_corr_vars = self.drop_high_corr_vars(high_corr_pairs)\n        \n        # Plot correlation heatmap for remaining variables\n        if not high_corr_pairs.empty:\n            high_corr_vars = list(set(high_corr_pairs['feature1']).union(set(high_corr_pairs['feature2'])))\n            high_corr_vars = [var for var in high_corr_vars if var in self.features_df.columns]\n            self.plot_correlation_heatmap(high_corr_vars)\n        \n        # Calculate and plot VIF\n        vif_data = self.calculate_vif(self.features_df)\n        print(\"Original VIF Data:\")\n        print(vif_data)\n        \n        # Variables to keep for VIF reporting\n        high_vif_vars = vif_data[vif_data[\"VIF\"] &gt; self.vif_threshold]\n        \n        # Print table for high VIF variables\n        if not high_vif_vars.empty:\n            print(\"\\nVariables with high VIF:\")\n            print(high_vif_vars)\n        else:\n            print(\"\\nNo variables exceed the VIF threshold.\")\n\n        # Drop variables with high VIF\n        reduced_df = self.features_df.drop(columns=high_vif_vars['Variable'], errors='ignore')\n        \n        # Recalculate VIF on reduced dataset\n        reduced_vif_data = self.calculate_vif(reduced_df)\n        print(\"\\nNew VIF Data after dropping high VIF variables:\")\n        print(reduced_vif_data)\n        \n        # Plot new VIF\n        self.plot_vif(reduced_vif_data)\n        \n        # Add target variable back to reduced_df\n        reduced_df[self.target_variable] = self.df[self.target_variable]\n        \n        return reduced_df, dropped_corr_vars, high_vif_vars, reduced_vif_data\n\n# Example usage\n# transformed_df is your DataFrame and 'target' is your target variable\nma = MultivariateAnalysis(transformed_df, target_variable='thirtydaymortality', corr_threshold=0.6)\nreduced_df, dropped_corr_vars, high_vif_vars, new_vif = ma.run_analysis()\n\n\n\n\n\n\n\n\n\nMachine Learning (Individual Models)\nnot needed here, we will proceed straight to AutoML with the models\n\n\n\n5. Comparison and Evaluation: Automate the evaluation process\nAutomate the model training and evaluation process, and generate comprehensive results and key evaluation reports to assess the validity and effectiveness of the chosen model.\n\nclass BaseModel:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123):\n        self.df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n\n        # Prepare features and target variable\n        self.X = self.df.drop(columns=[self.target_column])\n        self.y = self.df[self.target_column]\n\n        \n        # Label encode the target variable\n        label_encoder = LabelEncoder()\n        self.y = label_encoder.fit_transform(self.y)\n\n\n        # Split the data into training and test sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, test_size=0.3, random_state=self.random_state\n        )\n\n        # Initialize SMOTEENN if balance is set to 'imbalanced'\n        if self.balance == 'imbalanced':\n            self.smoteenn = SMOTEENN(sampling_strategy='auto', random_state=self.random_state)\n        else:\n            self.smoteenn = None\n\n    def apply_smoteenn(self):\n        if self.smoteenn:\n            return self.smoteenn.fit_resample(self.X_train, self.y_train)\n        else:\n            return self.X_train, self.y_train\n\n    def show_matrix(self, matrix, title='Confusion Matrix'):\n        # Plotting the confusion matrix\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=True, yticklabels=True)\n        plt.title(title)\n        plt.ylabel('Actual Class')\n        plt.xlabel('Predicted Class')\n        plt.show()\n\n    def plot_roc_auc(self, y_true, y_pred, title):\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\nclass DT(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = DecisionTreeClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Decision Tree...\")\n        # Define the parameter space\n        search_spaces = {\n            'max_features': Categorical(['sqrt', 'log2']),\n            'ccp_alpha': Real(0.001, 0.1, prior='log-uniform'),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass RandomForest(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = RandomForestClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for Random Forest...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_features': Categorical(['sqrt', 'log2']),\n            'max_depth': Integer(20, 30),\n            'criterion': Categorical(['gini', 'entropy']),\n            'min_samples_split': Integer(2, 4),\n            'min_samples_leaf': Integer(2, 4)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass XGBoost(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for XGBoost...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass LightGBM(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.clf = LGBMClassifier()\n\n    def run_bayesian_search(self):\n        print(\"\\nRunning Bayesian Search for LightGBM...\")\n        # Define the parameter space\n        search_spaces = {\n            'n_estimators': Integer(80, 150),\n            'max_depth': Integer(20, 30),\n            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n            'subsample': Real(0.5, 1.0),\n            'colsample_bytree': Real(0.5, 1.0)\n        }\n\n        # Initialize BayesSearchCV\n        cv = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=self.random_state)\n        bayes_search = BayesSearchCV(estimator=self.clf, search_spaces=search_spaces, cv=cv, n_iter=10, scoring='accuracy', random_state=self.random_state, verbose=1, n_jobs=-1)\n        \n        # Apply SMOTEENN to the training data if balance is 'imbalanced'\n        X_resampled, y_resampled = self.apply_smoteenn()\n\n        # Fit BayesSearchCV with the resampled data\n        t0 = time.time()\n        bayes_result = bayes_search.fit(X_resampled, y_resampled)\n        print(f'\\nTime taken = {round(time.time() - t0, 2)} sec')\n\n        # Print best score and parameters\n        print(f'Best score = {round(bayes_result.best_score_, 3)} using params: {bayes_result.best_params_}')\n\n        # Evaluate on test data\n        y_pred = bayes_result.best_estimator_.predict(self.X_test)\n        accuracy = accuracy_score(self.y_test, y_pred)\n        print(f'Test accuracy = {round(accuracy, 3)}')\n\n        return bayes_result, y_pred\n\nclass AutoML:\n    def __init__(self, cleaned_df, target_column, balance='imbalanced', n_splits=5, n_repeats=3, random_state=123, secondary_metric='precision'):\n        self.cleaned_df = cleaned_df\n        self.target_column = target_column\n        self.balance = balance\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        self.secondary_metric = secondary_metric\n        \n        # Initialize model classes\n        self.models = {\n            'DecisionTree': DT(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'RandomForest': RandomForest(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'XGBoost': XGBoost(cleaned_df, target_column, balance, n_splits, n_repeats, random_state),\n            'LightGBM': LightGBM(cleaned_df, target_column, balance, n_splits, n_repeats, random_state)\n        }\n\n    def evaluate_models(self):\n        results = {}\n        for name, model in self.models.items():\n            print(f'\\nEvaluating {name}...')\n            best_model, y_pred = model.run_bayesian_search()\n            \n            # Compute metrics\n            accuracy = accuracy_score(model.y_test, y_pred)\n            precision = precision_score(model.y_test, y_pred, average='weighted')\n            recall = recall_score(model.y_test, y_pred, average='weighted')\n            f1 = 2 * (precision * recall) / (precision + recall)\n            \n            results[name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'classification_report': classification_report(model.y_test, y_pred),\n                'confusion_matrix': model.show_matrix(confusion_matrix(model.y_test, y_pred))\n            }\n            \n            # Print Classification Report and Confusion Matrix\n            print(f'Classification Report for {name}:\\n{results[name][\"classification_report\"]}')\n            \n            # ROC and AUC\n            y_test_binarized = label_binarize(model.y_test, classes=np.unique(model.y_test))\n            y_pred_binarized = label_binarize(y_pred, classes=np.unique(model.y_test))\n            if y_test_binarized.shape[1] &gt; 1:\n                for i in range(y_test_binarized.shape[1]):\n                    model.plot_roc_auc(y_test_binarized[:, i], y_pred_binarized[:, i], f'{name} ROC Curve for Class {i}')\n            \n        # Determine the best model\n        sorted_results = sorted(results.items(), key=lambda x: (x[1]['accuracy'], x[1][self.secondary_metric]), reverse=True)\n        best_model_name, best_model_metrics = sorted_results[0]\n        print(f'\\nBest Model: {best_model_name}')\n        print(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\n        print(f'{self.secondary_metric.capitalize()}: {best_model_metrics[self.secondary_metric]}')\n        \n        return best_model_name, best_model_metrics\n\n# Example usage\n# Assuming `transformed_df` is your DataFrame and 'thirtydaymortality' is your target column\nauto_ml = AutoML(reduced_df, target_column='thirtydaymortality', balance='imbalanced', secondary_metric='recall')\nbest_model_name, best_model_metrics = auto_ml.evaluate_models()\n\nprint(f'\\nBest Model: {best_model_name}')\nprint(f'Accuracy: {best_model_metrics[\"accuracy\"]}')\nprint(f'Precision: {best_model_metrics[\"precision\"]}')\nprint(f'Recall: {best_model_metrics[\"recall\"]}')\nprint(f'F1 Score: {best_model_metrics[\"f1\"]}')\nprint(f'Confusion Matrix:\\n{best_model_metrics[\"confusion_matrix\"]}')\n\nEvaluating DecisionTree…\nTime taken = 38.33 sec\nBest score = 0.882 using params: OrderedDict([(‘ccp_alpha’, 0.001131086677421503), (‘criterion’, ‘entropy’), (‘max_depth’, 24), (‘max_features’, ‘log2’), (‘min_samples_leaf’, 3), (‘min_samples_split’, 3)])\nTest accuracy = 0.85\n\n\nEvaluating RandomForest…\nTime taken = 891.38 sec Best score = 0.97 using params: OrderedDict([(‘criterion’, ‘entropy’), (‘max_depth’, 27), (‘max_features’, ‘sqrt’), (‘min_samples_leaf’, 2), (‘min_samples_split’, 3), (‘n_estimators’, 82)]) Test accuracy = 0.956\n\n\nEvaluating XGBoost"
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#introduction",
    "href": "Python/AutoML_Binary_Classification.html#introduction",
    "title": "AutoML for Binary Classification",
    "section": "Introduction",
    "text": "Introduction\nHello everyone, this post is intended to demonstrate how it is possible to automate some of the evaluation process in deciding which machine learning models to utilise to achieve the best desired results, which is commonly know as AutoML.\nI was intrigued that such a thing was possible and couldn’t wait to try it, here’s an attempt on my version of AutoML for a Binary Classification Model. Going forward, i would like to automate more of the common decision-making processes like pre-processing that is in tune with domain knowledge.\nChangelog:\n\\[1.0.0\\] - 2024-08-02 - Initial deployment after consolidating course work and initial refinement.\n\\[1.0.1\\] - 2024-09-01 - Added Context, Logistic Regression, Fine-tuned Models, Evaluation of Results for Multivariate Analysis, Logistic Regression, Machine Learning Predictions"
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#context",
    "href": "Python/AutoML_Binary_Classification.html#context",
    "title": "AutoML for Binary Classification",
    "section": "Context",
    "text": "Context\nPredicting surgical risks is crucial for enhancing patient safety and improving surgical outcomes by allowing healthcare providers to tailor care and allocate resources effectively.\nBy using real-world EMR data from obtained from Chan et al. (2018) [1], we aim to evaluate risk factors and develop predictive models to predict the following outcome: mortality within 30 days."
  },
  {
    "objectID": "Python/AutoML_Binary_Classification.html#methodology",
    "href": "Python/AutoML_Binary_Classification.html#methodology",
    "title": "AutoML for Binary Classification",
    "section": "Methodology",
    "text": "Methodology\nThe obtained data set includes 90,785 surgery patients (excluded Cardiac and Neurosurgery patients) from Singapore General Hospital from 1 January 2012 to 31 October 2016. For each outcome, we developed a respective multivariate model to evaluate the risks factors and test out different machine learning model to see which model best predict the outcome."
  }
]